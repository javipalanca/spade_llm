{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SPADE-LLM Documentation - Multi-Agent LLM Framework for Python","text":"SPADE-LLM <p>     Build distributed based-XMPP multi-agent systems powered by Large Language Models. Extends SPADE multi-agent platform with many LLM providers for distributed AI applications, intelligent chatbots, and collaborative agent systems.   </p>        \ud83d\ude80 Quick Start             \ud83d\udce6 Installation             \ud83d\udcc2 GitHub"},{"location":"#key-features","title":"Key Features","text":"8+ LLM Providers 100% Python Native 0 External Dependencies \u221e Agent Scalability \ud83d\udd27 Built-in XMPP Server <p>No external server setup required with SPADE 4.0+. Get started instantly with zero configuration.</p> \ud83e\udde0 Multi-Provider Support <p>OpenAI GPT, Ollama, LM Studio, vLLM, Anthropic Claude and more. Switch providers seamlessly.</p> \u26a1 Advanced Tool System <p>Function calling with async execution, human-in-the-loop workflows, and LangChain integration.</p> \ud83d\udcbe Dual Memory Architecture <p>Agent learning and conversation continuity with SQLite persistence and contextual retrieval.</p> \ud83c\udfaf Context Management <p>Multi-conversation support with automatic cleanup and intelligent context window management.</p> \ud83d\udee1\ufe0f Guardrails System <p>Content filtering and safety controls for input/output with customizable rules and policies.</p> \ud83d\udd17 Message Routing <p>Conditional routing based on LLM responses with flexible workflows and decision trees.</p> \ud83c\udf10 MCP Integration <p>Model Context Protocol server support for external tool integration and service connectivity.</p> \ud83e\udde9 Coordinator Agents <p>LLM-driven coordinators orchestrate SPADE subagents with shared context, sequential planning, and inter-organization routing.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[LLMAgent] --&gt; C[ContextManager]\n    A --&gt; D[LLMProvider]\n    A --&gt; E[LLMTool]\n    A --&gt; G[Guardrails]\n    A --&gt; M[Memory]\n    D --&gt; F[OpenAI/Ollama/etc]\n    G --&gt; H[Input/Output Filtering]\n    E --&gt; I[Human-in-the-Loop]\n    E --&gt; J[MCP]\n    E --&gt; P[CustomTool/LangchainTool]\n    J --&gt; K[STDIO]\n    J --&gt; L[HTTP Streaming]\n    M --&gt; N[Agent-based]\n    M --&gt; O[Agent-thread]</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"\ud83d\udc0d Basic Agent Setup    <pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # First, start SPADE's built-in server:\n    # spade run\n\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    agent = LLMAgent(\n        jid=\"assistant@localhost\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    await agent.start()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre> Python 3.10+ MIT License Beta Release"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Setup and requirements</li> <li>Quick Start - Basic usage examples</li> </ul>"},{"location":"#core-guides","title":"Core Guides","text":"<ul> <li>Architecture - SPADE_LLM general structure</li> <li>Providers - LLM provider configuration</li> <li>Tools System - Function calling capabilities</li> <li>Memory System - Agent learning and conversation continuity</li> <li>Context Management - Context control and message management</li> <li>Conversations - Conversation lifecycle and management</li> <li>Guardrails - Content filtering and safety controls</li> <li>Message Routing - Conditional message routing</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Examples - Working code examples</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Explore the examples directory for complete working examples:</p> <ul> <li><code>multi_provider_chat_example.py</code> - Chat with different LLM providers</li> <li><code>ollama_with_tools_example.py</code> - Local models with tool calling</li> <li><code>guardrails_example.py</code> - Content filtering and safety controls</li> <li><code>langchain_tools_example.py</code> - LangChain tool integration</li> <li><code>valencia_multiagent_trip_planner.py</code> - Multi-agent workflow</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to SPADE_LLM! This guide will help you get started.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Create a branch for your changes</li> <li>Make your changes and test them</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Git</li> <li>Virtual environment tool (venv, conda, etc.)</li> <li>A XMPP server connection </li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"<pre><code># Clone your fork\ngit clone https://github.com/your-username/spade_llm.git\ncd spade_llm\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=spade_llm\n\n# Run specific test file\npytest tests/test_agent/test_llm_agent.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ul> <li>Clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Expected vs actual behavior</li> <li>Environment details (Python version, OS, etc.)</li> <li>Minimal code example that reproduces the bug</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For new features, please:</p> <ul> <li>Check existing issues to avoid duplicates</li> <li>Describe the use case and motivation</li> <li>Propose implementation approach if possible</li> <li>Consider backward compatibility</li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li>Write code</li> <li>Add tests</li> <li> <p>Update documentation</p> </li> <li> <p>Test your changes <pre><code>pytest\nflake8 spade_llm tests\nmypy spade_llm\n</code></pre></p> </li> <li> <p>Commit with clear messages <pre><code>git add .\ngit commit -m \"Add feature: clear description of changes\"\n</code></pre></p> </li> <li> <p>Push to your fork <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create pull request on GitHub</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Clear title describing the change</li> <li>Detailed description of what and why</li> <li>Link related issues using keywords (fixes #123)</li> <li>Include screenshots for UI changes</li> <li>Check that CI passes</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>Extend, don't replace SPADE functionality</li> <li>Maintain compatibility with existing SPADE agents</li> <li>Use async/await throughout</li> <li>Keep interfaces simple and consistent</li> <li>Favor composition over inheritance</li> </ul>"},{"location":"contributing/#code-organization","title":"Code Organization","text":"<pre><code>spade_llm/\n\u251c\u2500\u2500 agent/          # Agent classes\n\u251c\u2500\u2500 behaviour/      # Behaviour implementations\n\u251c\u2500\u2500 context/        # Context management\n\u251c\u2500\u2500 providers/      # LLM provider interfaces\n\u251c\u2500\u2500 tools/          # Tool system\n\u251c\u2500\u2500 routing/        # Message routing\n\u251c\u2500\u2500 mcp/           # MCP integration\n\u2514\u2500\u2500 utils/         # Utility functions\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>We use semantic versioning (semver):</p> <ul> <li>Major (X.0.0) - Breaking changes</li> <li>Minor (0.X.0) - New features, backward compatible</li> <li>Patch (0.0.X) - Bug fixes, backward compatible</li> </ul>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ul> <li>[ ] All tests pass</li> <li>[ ] Documentation updated</li> <li>[ ] Changelog updated</li> <li>[ ] Version bumped</li> <li>[ ] Release notes prepared</li> <li>[ ] Tagged release created</li> </ul>"},{"location":"contributing/#community","title":"Community","text":""},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues - Bug reports and feature requests</li> <li>Discussions - General questions and ideas</li> <li>Documentation - Comprehensive guides and API reference</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Help others learn and contribute</li> <li>Focus on constructive feedback</li> <li>Follow project guidelines and standards</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in:</p> <ul> <li>CONTRIBUTORS.md file</li> <li>Release notes for significant contributions</li> <li>Documentation acknowledgments</li> </ul> <p>Thank you for contributing to SPADE_LLM! \ud83d\ude80</p>"},{"location":"contributing/development/","title":"Development Guide","text":"<p>Detailed guide for SPADE_LLM development and testing.</p>"},{"location":"contributing/development/#development-environment","title":"Development Environment","text":""},{"location":"contributing/development/#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/javipalanca/spade_llm.git\ncd spade_llm\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install development dependencies\npip install -e \".[dev,docs]\"\n\n# Setup pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/development/#project-structure","title":"Project Structure","text":"<pre><code>spade_llm/\n\u251c\u2500\u2500 spade_llm/          # Main package\n\u2502   \u251c\u2500\u2500 agent/          # Agent implementations\n\u2502   \u251c\u2500\u2500 behaviour/      # Behaviour classes\n\u2502   \u251c\u2500\u2500 context/        # Context management\n\u2502   \u251c\u2500\u2500 providers/      # LLM providers\n\u2502   \u251c\u2500\u2500 tools/          # Tool system\n\u2502   \u251c\u2500\u2500 routing/        # Message routing\n\u2502   \u251c\u2500\u2500 mcp/           # MCP integration\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u251c\u2500\u2500 examples/          # Usage examples\n\u251c\u2500\u2500 docs/             # Documentation\n\u2514\u2500\u2500 requirements*.txt  # Dependencies\n</code></pre>"},{"location":"contributing/development/#testing","title":"Testing","text":""},{"location":"contributing/development/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_agent/        # Agent tests\n\u251c\u2500\u2500 test_behaviour/    # Behaviour tests\n\u251c\u2500\u2500 test_context/      # Context tests\n\u251c\u2500\u2500 test_providers/    # Provider tests\n\u251c\u2500\u2500 test_tools/        # Tool tests\n\u251c\u2500\u2500 test_routing/      # Routing tests\n\u251c\u2500\u2500 conftest.py        # Test configuration\n\u2514\u2500\u2500 factories.py       # Test factories\n</code></pre>"},{"location":"contributing/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=spade_llm --cov-report=html\n\n# Run specific test module\npytest tests/test_agent/\n\n# Run specific test\npytest tests/test_agent/test_llm_agent.py::test_agent_creation\n\n# Run with verbose output\npytest -v -s\n</code></pre>"},{"location":"contributing/development/#documentation-standards","title":"Documentation Standards","text":""},{"location":"contributing/development/#docstring-format","title":"Docstring Format","text":"<pre><code>def example_function(param1: str, param2: int = 0) -&gt; str:\n    \"\"\"Brief description of the function.\n\n    Longer description if needed. Explain the purpose,\n    behavior, and any important details.\n\n    Args:\n        param1: Description of first parameter\n        param2: Description of second parameter with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When invalid input is provided\n        ConnectionError: When service is unavailable\n\n    Example:\n        ```python\n        result = example_function(\"hello\", 42)\n        print(result)  # Output: processed result\n        ```\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"contributing/development/#class-documentation","title":"Class Documentation","text":"<pre><code>class ExampleClass:\n    \"\"\"Brief description of the class.\n\n    Longer description explaining the class purpose,\n    usage patterns, and important behavior.\n\n    Attributes:\n        attribute1: Description of attribute\n        attribute2: Description of another attribute\n\n    Example:\n        ```python\n        instance = ExampleClass(param=\"value\")\n        result = instance.method()\n        ```\n    \"\"\"\n\n    def __init__(self, param: str):\n        \"\"\"Initialize the class.\n\n        Args:\n            param: Configuration parameter\n        \"\"\"\n        self.attribute1 = param\n</code></pre> <p>This development guide should help you contribute effectively to SPADE_LLM. For specific questions, check the existing issues or create a new one.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Learn how to install and use SPADE_LLM to create LLM-powered agents.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Basic knowledge of Python async/await</li> <li>XMPP server</li> <li>Access to an LLM provider (OpenAI API key or local model)</li> </ul>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":"<ol> <li>Installation - Install SPADE_LLM and dependencies</li> <li>Quick Start - Create your first agent</li> <li>First Agent - Complete tutorial</li> </ol>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Check the API Reference for detailed documentation</li> <li>Browse Examples for working code</li> <li>Open an issue on GitHub</li> </ul>"},{"location":"getting-started/advanced-agent/","title":"Advanced Multi-Feature Agent Tutorial","text":"<p>In this tutorial, you'll learn how to create sophisticated multi-agent systems with advanced features like MCP integration, human-in-the-loop workflows, custom guardrails, and complex routing. This tutorial is based on the <code>github_issues_monitor_example.py</code> and demonstrates production-ready patterns.</p>"},{"location":"getting-started/advanced-agent/#what-youll-build","title":"What You'll Build","text":"<p>We'll create a comprehensive GitHub monitoring system with four specialized agents:</p> <ol> <li>Chat Agent - User interface with custom guardrails</li> <li>Analysis Agent - GitHub data collection and analysis via MCP</li> <li>Storage Agent - Notion integration for data persistence</li> <li>Notification Agent - Human-in-the-loop email confirmations</li> </ol>"},{"location":"getting-started/advanced-agent/#prerequisites","title":"Prerequisites","text":"<ul> <li>Complete all previous tutorials (First Agent, Guardrails, Tools)</li> <li>Advanced Python knowledge (classes, async programming, error handling)</li> <li>Understanding of multi-agent systems concepts</li> <li>Access to external services (GitHub, Notion, Gmail - for demonstration)</li> </ul>"},{"location":"getting-started/advanced-agent/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    %% Agents (rectangles)\n    A[User] --&gt; B[Chat Agent]\n    B --&gt;|XMPP Message| C[Analysis Agent]\n    C --&gt;|XMPP Message| D[Storage Agent]\n    D --&gt;|XMPP Message| E[Notification Agent]\n    E &lt;--&gt;|HITL Tool| F[Human Expert]\n\n    %% MCP Servers (rounded rectangles)\n    C -.-&gt;|MCP Call| G((GitHub MCP))\n    D -.-&gt;|MCP Call| H((Notion MCP))\n    E -.-&gt;|MCP Call| I((Gmail MCP))\n\n    %% Styling for both light and dark themes\n    classDef agent fill:#1976d2,stroke:#0d47a1,stroke-width:2px,color:#ffffff\n    classDef mcp fill:#7b1fa2,stroke:#4a148c,stroke-width:2px,color:#ffffff\n    classDef human fill:#388e3c,stroke:#1b5e20,stroke-width:2px,color:#ffffff\n\n    class A,B,C,D,E agent\n    class G,H,I mcp\n    class F human</code></pre>"},{"location":"getting-started/advanced-agent/#communication-flow-explanation","title":"Communication Flow Explanation","text":"<p>XMPP Messages (solid arrows): Direct agent-to-agent communication - Each agent sends structured messages to the next agent in the pipeline - Messages contain conversation context and processing results - Uses SPADE's built-in XMPP messaging system</p> <p>MCP Calls (dotted arrows): External service integration - Agents call MCP servers to interact with external services - MCP servers provide tools for GitHub, Notion, Gmail operations - Responses are integrated back into the agent's processing</p> <p>Human-in-the-Loop (HITL): Interactive human consultation - Agent asks human expert a question (e.g., \"Send this email?\") - Human expert responds with decision and additional data (e.g., \"Yes, send to user@example.com\") - Agent continues processing based on human response - Bidirectional communication through web interface</p>"},{"location":"getting-started/advanced-agent/#agent-to-agent-communication","title":"Agent-to-Agent Communication","text":""},{"location":"getting-started/advanced-agent/#understanding-agent-connections","title":"Understanding Agent Connections","text":"<p>In SPADE-LLM, agents communicate through XMPP messages. Here's how to establish connections:</p>"},{"location":"getting-started/advanced-agent/#1-direct-reply-configuration","title":"1. Direct Reply Configuration","text":"<p>The simplest way to connect agents is using the <code>reply_to</code> parameter:</p> <pre><code># Agent A sends messages to Agent B\nagent_a = LLMAgent(\n    jid=\"agent_a@server.com\",\n    password=\"password_a\",\n    provider=provider,\n    reply_to=\"agent_b@server.com\"  # All responses go to Agent B\n)\n\n# Agent B receives from Agent A and sends to Agent C\nagent_b = LLMAgent(\n    jid=\"agent_b@server.com\", \n    password=\"password_b\",\n    provider=provider,\n    reply_to=\"agent_c@server.com\"  # Forward to Agent C\n)\n</code></pre>"},{"location":"getting-started/advanced-agent/#2-chatagent-target-configuration","title":"2. ChatAgent Target Configuration","text":"<p>For user interfaces, use <code>target_agent_jid</code>:</p> <pre><code># User interface that sends to LLM Agent\nchat_agent = ChatAgent(\n    jid=\"user@server.com\",\n    password=\"password\",\n    target_agent_jid=\"assistant@server.com\"  # Where user messages go\n)\n</code></pre>"},{"location":"getting-started/advanced-agent/#3-message-flow-pipeline","title":"3. Message Flow Pipeline","text":"<p>Create a processing pipeline by chaining agents:</p> <pre><code>async def create_agent_pipeline():\n    # Entry point: User \u2192 Chat Agent\n    chat_agent = ChatAgent(\n        jid=\"user@server.com\",\n        password=\"pwd1\",\n        target_agent_jid=\"analyzer@server.com\"\n    )\n\n    # Step 1: Chat Agent \u2192 Analyzer Agent\n    analyzer_agent = LLMAgent(\n        jid=\"analyzer@server.com\",\n        password=\"pwd2\",\n        provider=provider,\n        reply_to=\"storage@server.com\"  # Forward to storage\n    )\n\n    # Step 2: Analyzer \u2192 Storage Agent\n    storage_agent = LLMAgent(\n        jid=\"storage@server.com\",\n        password=\"pwd3\",\n        provider=provider,\n        reply_to=\"notification@server.com\"  # Forward to notification\n    )\n\n    # Step 3: Storage \u2192 Notification Agent (end of pipeline)\n    notification_agent = LLMAgent(\n        jid=\"notification@server.com\",\n        password=\"pwd4\",\n        provider=provider,\n        # No reply_to = end of pipeline\n    )\n\n    return chat_agent, analyzer_agent, storage_agent, notification_agent\n</code></pre>"},{"location":"getting-started/advanced-agent/#4-conditional-routing","title":"4. Conditional Routing","text":"<p>Use routing functions for dynamic agent selection:</p> <pre><code>def smart_router(msg, response, context):\n    \"\"\"Route messages based on content analysis.\"\"\"\n    if \"urgent\" in response.lower():\n        return \"urgent_handler@server.com\"\n    elif \"storage\" in response.lower():\n        return \"storage_agent@server.com\"\n    else:\n        return \"default_handler@server.com\"\n\n# Agent with conditional routing\nrouter_agent = LLMAgent(\n    jid=\"router@server.com\",\n    password=\"password\",\n    provider=provider,\n    routing_function=smart_router  # Dynamic routing\n)\n</code></pre>"},{"location":"getting-started/advanced-agent/#5-message-context-preservation","title":"5. Message Context Preservation","text":"<p>Agents automatically maintain conversation context:</p> <pre><code># Context flows through the pipeline\n# User: \"Analyze repository issues\"\n# \u2192 Chat Agent \u2192 Analyzer Agent (receives full context)\n# \u2192 Storage Agent (receives context + analysis)\n# \u2192 Notification Agent (receives context + analysis + storage confirmation)\n</code></pre>"},{"location":"getting-started/advanced-agent/#step-1-custom-guardrails","title":"Step 1: Custom Guardrails","text":"<p>First, let's create a custom guardrail that only allows GitHub-related requests:</p> <pre><code>from spade_llm.guardrails.base import Guardrail, GuardrailResult, GuardrailAction\nfrom typing import Dict, Any\n\nclass GitHubOnlyGuardrail(Guardrail):\n    \"\"\"Custom guardrail that only allows GitHub-related requests.\"\"\"\n\n    def __init__(self, name: str = \"github_only_filter\", enabled: bool = True):\n        super().__init__(name, enabled, \"I only help with GitHub-related requests. Please ask about issues, pull requests, or repository monitoring.\")\n        self.github_keywords = [\n            \"github\", \"issue\", \"issues\", \"pull request\", \"pr\", \"prs\", \n            \"repository\", \"repo\", \"commit\", \"branch\", \"merge\", \"review\",\n            \"bug\", \"feature\", \"enhancement\", \"milestone\", \"project\",\n            \"analyze\", \"monitor\", \"check\", \"status\", \"activity\"\n        ]\n\n    async def check(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult:\n        \"\"\"Check if content is GitHub-related.\"\"\"\n        content_lower = content.lower()\n\n        # Check if any GitHub keyword is present\n        if any(keyword in content_lower for keyword in self.github_keywords):\n            return GuardrailResult(\n                action=GuardrailAction.PASS,\n                content=content,\n                reason=\"GitHub-related request detected\"\n            )\n        else:\n            return GuardrailResult(\n                action=GuardrailAction.BLOCK,\n                content=self.blocked_message,\n                reason=\"Non-GitHub request blocked\"\n            )\n\ndef create_github_guardrails():\n    \"\"\"Create input guardrails for GitHub-only requests.\"\"\"\n    return [GitHubOnlyGuardrail()]\n</code></pre>"},{"location":"getting-started/advanced-agent/#step-2-mcp-server-configuration","title":"Step 2: MCP Server Configuration","text":"<p>Configure external MCP servers for GitHub, Notion, and Gmail integration:</p> <pre><code>from spade_llm.mcp import StreamableHttpServerConfig\n\ndef create_mcp_servers():\n    \"\"\"Create MCP server configurations.\"\"\"\n\n    # GitHub MCP server\n    github_mcp = StreamableHttpServerConfig(\n        name=\"GitHubMCP\",\n        url=\"https://your-mcp-server.com/github/endpoint\",  # Replace with your MCP server\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"SPADE_LLM/1.0\"\n        },\n        timeout=30.0,\n        sse_read_timeout=300.0,\n        terminate_on_close=True,\n        cache_tools=True\n    )\n\n    # Notion MCP server\n    notion_mcp = StreamableHttpServerConfig(\n        name=\"NotionMCP\",\n        url=\"https://your-mcp-server.com/notion/endpoint\",  # Replace with your MCP server\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"SPADE_LLM/1.0\"\n        },\n        timeout=30.0,\n        sse_read_timeout=300.0,\n        terminate_on_close=True,\n        cache_tools=True\n    )\n\n    # Gmail MCP server\n    gmail_mcp = StreamableHttpServerConfig(\n        name=\"GmailMCP\",\n        url=\"https://your-mcp-server.com/gmail/endpoint\",  # Replace with your MCP server\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"SPADE_LLM/1.0\"\n        },\n        timeout=30.0,\n        sse_read_timeout=300.0,\n        terminate_on_close=True,\n        cache_tools=True\n    )\n\n    return github_mcp, notion_mcp, gmail_mcp\n</code></pre>"},{"location":"getting-started/advanced-agent/#step-3-specialized-agent-system-prompts","title":"Step 3: Specialized Agent System Prompts","text":"<p>Create detailed system prompts for each agent:</p> <pre><code>GITHUB_ANALYZER_PROMPT = \"\"\"You are a GitHub analysis specialist. You receive GitHub monitoring requests and provide comprehensive repository analysis.\n\nYour workflow:\n1. Identify the repository to analyze (ask user if not specified)\n2. Use GitHub MCP tools to gather data:\n   - List recent issues (open and closed, last 30 days)\n   - List recent pull requests (all states, last 30 days)\n   - Get repository details if available\n3. Analyze the collected data for patterns, priorities, and insights\n4. Generate a structured summary with actionable information\n\nIMPORTANT: \n- Always specify which repository you're analyzing\n- Include actual numbers and real data from the GitHub API\n- Focus on actionable insights and trends\n- Identify urgent items that need attention\n\nResponse format:\n=== GITHUB REPOSITORY ANALYSIS ===\n\n\ud83c\udfea Repository: [owner/repo-name]\n\ud83d\udcc5 Analysis Date: [current date and time]\n\ud83d\udd0d Analysis Period: Last 30 days\n\n\ud83d\udcca SUMMARY METRICS\n- Issues: [X] total ([X] open, [X] closed)\n- Pull Requests: [X] total ([X] open, [X] merged, [X] draft, [X] closed)\n- Recent Activity Level: [High/Medium/Low]\n- Last Updated: [when]\n\n\ud83d\udea8 URGENT ITEMS (High Priority)\n[List critical issues/PRs that need immediate attention, include #numbers and titles]\n\n\ud83d\udcc8 RECENT TRENDS (Last 30 days)\n- New Issues Created: [X]\n- Issues Closed: [X] \n- PRs Merged: [X]\n- Most Active Contributors: [list top 3]\n- Common Labels/Categories: [list most frequent]\n\n\ud83d\udd0d KEY INSIGHTS\n[Notable patterns, recurring issues, areas needing attention]\n\n\ud83d\udca1 RECOMMENDATIONS\n[Actionable suggestions based on the analysis]\n\n&lt;GITHUB_SUMMARY&gt;\n{\n  \"repository\": \"[owner/repo]\",\n  \"analysis_date\": \"[ISO date]\",\n  \"period_days\": 30,\n  \"summary\": {\n    \"total_issues\": X,\n    \"open_issues\": X,\n    \"closed_issues\": X,\n    \"total_prs\": X,\n    \"open_prs\": X,\n    \"merged_prs\": X,\n    \"draft_prs\": X\n  },\n  \"urgent_items\": [\n    {\"type\": \"issue/pr\", \"number\": X, \"title\": \"...\", \"priority\": \"high\", \"url\": \"...\"}\n  ],\n  \"trends\": {\n    \"new_issues\": X,\n    \"closed_issues\": X,\n    \"merged_prs\": X,\n    \"top_contributors\": [\"...\", \"...\"],\n    \"common_labels\": [\"...\", \"...\"]\n  },\n  \"insights\": [\"...\", \"...\"],\n  \"recommendations\": [\"...\", \"...\"]\n}\n&lt;/GITHUB_SUMMARY&gt;\n\nThis analysis will now be stored in Notion and potentially sent via email.\"\"\"\n\nNOTION_MANAGER_PROMPT = \"\"\"You are a Notion storage specialist. You receive GitHub analysis summaries and store them systematically.\n\nYour workflow:\n1. Receive complete GitHub analysis from GitHubAnalyzer agent\n2. Use Notion MCP tools to:\n   - Search for \"GitHub monitoring\" page\n   - Add a new entry with all analysis data\n   - Structure the data for easy reading\n3. After successful storage, prepare the summary for email notification\n\nIMPORTANT:\n- Create consistent Notion entries for easy tracking over time\n- Include the full analysis text for complete context\n- Verify the data was stored before forwarding\n\nResponse format:\n=== NOTION STORAGE COMPLETED ===\n\n\ud83d\udcda **Notion Database Updated**\n\u2705 Entry created: \"[Repository] Analysis - [Date]\"\n\ud83d\uddc2\ufe0f  Database: GitHub Repository Monitoring\n\ud83d\udcca Data stored:\n   - Repository: [owner/repo]\n   - Analysis Date: [date]\n   - Issues: [X] total ([X] open)\n   - PRs: [X] total ([X] open)\n\n\ud83d\udd17 Notion URL: [if available]\n\n\ud83d\udce7 **Forwarding to Email Manager**\nThe complete analysis is now ready for potential email notification.\n\n[Include the FULL original analysis text here for the Email Manager]\"\"\"\n\nEMAIL_MANAGER_PROMPT = \"\"\"You are an email notification specialist with human-in-the-loop confirmation. You receive GitHub analysis summaries and handle email notifications with human oversight.\n\nYour workflow:\n1. Receive complete GitHub analysis from Notion Manager\n2. Extract key information and prepare a concise summary for human review\n3. Use ask_human_expert tool to get human confirmation and email details\n4. If approved, format and send professional email via Gmail MCP\n5. ALWAYS end with termination marker after completing the process\n\nHUMAN INTERACTION PROCESS:\n1. Present a concise executive summary to the human\n2. Ask: \"Would you like to send this GitHub analysis via email?\"\n3. If YES: Ask \"Please provide the recipient's email address(es)\"\n4. If NO: Acknowledge and end with termination marker\n5. If email provided: Send formatted email, confirm delivery, and end with termination marker\n\nEMAIL FORMAT (when sending):\nSubject: \"GitHub Repository Analysis - [Repository Name] - [Date]\"\n\nEmail Content:\n---\n# GitHub Repository Analysis Report\n\n**Repository:** [owner/repo-name]  \n**Analysis Date:** [date]  \n**Period Analyzed:** Last 30 days\n\n## Executive Summary\n- **Issues:** [X] total ([X] open, [X] closed)\n- **Pull Requests:** [X] total ([X] open, [X] merged)\n- **Activity Level:** [High/Medium/Low]\n- **Urgent Items:** [X] items need attention\n\n## Key Insights\n[3-4 most important insights from analysis]\n\n## Urgent Items Requiring Attention\n[List critical issues/PRs with numbers and titles]\n\n## Recommendations\n[Top 3 actionable recommendations]\n\n## Full Analysis\n[Include complete detailed analysis from GitHubAnalyzer]\n\n---\n*This report was generated automatically and stored in Notion for tracking.*\n\nTERMINATION:\n- After sending email successfully: \"Email sent successfully to [recipient]. &lt;EMAIL_PROCESS_COMPLETE&gt;\"\n- After human declines email: \"GitHub analysis completed and stored in Notion. No email sent. &lt;EMAIL_PROCESS_COMPLETE&gt;\"\n\nIMPORTANT:\n- Always summarize key points for human decision-making\n- Wait for explicit human approval before sending emails\n- Include repository name and key metrics in human interaction\n- Use professional email formatting\n- Confirm successful email delivery\n- ALWAYS end with &lt;EMAIL_PROCESS_COMPLETE&gt; termination marker\"\"\"\n</code></pre>"},{"location":"getting-started/advanced-agent/#step-4-human-in-the-loop-tool","title":"Step 4: Human-in-the-Loop Tool","text":"<p>Create a tool for human expert consultation:</p> <pre><code>from spade_llm.tools import HumanInTheLoopTool\n\ndef create_human_tool(human_expert_jid: str):\n    \"\"\"Create human-in-the-loop tool for email confirmations.\"\"\"\n    return HumanInTheLoopTool(\n        human_expert_jid=human_expert_jid,\n        timeout=300.0,  # 5 minutes timeout\n        name=\"ask_human_expert\",\n        description=\"Ask human expert for email sending confirmation and recipient details\"\n    )\n</code></pre>"},{"location":"getting-started/advanced-agent/#step-5-complete-multi-agent-system","title":"Step 5: Complete Multi-Agent System","text":"<p>Here's the complete advanced agent system:</p> <pre><code>import asyncio\nimport getpass\nimport os\nimport spade\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nfrom spade_llm.agent import LLMAgent, ChatAgent\nfrom spade_llm.providers import LLMProvider\nfrom spade_llm.mcp import StreamableHttpServerConfig\nfrom spade_llm.guardrails.base import Guardrail, GuardrailResult, GuardrailAction\nfrom spade_llm.tools import HumanInTheLoopTool\nfrom spade_llm.utils import load_env_vars\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass GitHubOnlyGuardrail(Guardrail):\n    \"\"\"Custom guardrail that only allows GitHub-related requests.\"\"\"\n\n    def __init__(self, name: str = \"github_only_filter\", enabled: bool = True):\n        super().__init__(name, enabled, \"I only help with GitHub-related requests. Please ask about issues, pull requests, or repository monitoring.\")\n        self.github_keywords = [\n            \"github\", \"issue\", \"issues\", \"pull request\", \"pr\", \"prs\", \n            \"repository\", \"repo\", \"commit\", \"branch\", \"merge\", \"review\",\n            \"bug\", \"feature\", \"enhancement\", \"milestone\", \"project\",\n            \"analyze\", \"monitor\", \"check\", \"status\", \"activity\"\n        ]\n\n    async def check(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult:\n        \"\"\"Check if content is GitHub-related.\"\"\"\n        content_lower = content.lower()\n\n        if any(keyword in content_lower for keyword in self.github_keywords):\n            return GuardrailResult(\n                action=GuardrailAction.PASS,\n                content=content,\n                reason=\"GitHub-related request detected\"\n            )\n        else:\n            return GuardrailResult(\n                action=GuardrailAction.BLOCK,\n                content=self.blocked_message,\n                reason=\"Non-GitHub request blocked\"\n            )\n\ndef create_github_guardrails():\n    \"\"\"Create input guardrails for GitHub-only requests.\"\"\"\n    return [GitHubOnlyGuardrail()]\n\ndef create_mcp_servers():\n    \"\"\"Create MCP server configurations.\"\"\"\n\n    # GitHub MCP server\n    github_mcp = StreamableHttpServerConfig(\n        name=\"GitHubMCP\",\n        url=\"https://your-mcp-server.com/github/endpoint\",  # Replace with actual MCP server\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"SPADE_LLM/1.0\"\n        },\n        timeout=30.0,\n        sse_read_timeout=300.0,\n        terminate_on_close=True,\n        cache_tools=True\n    )\n\n    # Notion MCP server\n    notion_mcp = StreamableHttpServerConfig(\n        name=\"NotionMCP\",\n        url=\"https://your-mcp-server.com/notion/endpoint\",  # Replace with actual MCP server\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"SPADE_LLM/1.0\"\n        },\n        timeout=30.0,\n        sse_read_timeout=300.0,\n        terminate_on_close=True,\n        cache_tools=True\n    )\n\n    # Gmail MCP server\n    gmail_mcp = StreamableHttpServerConfig(\n        name=\"GmailMCP\",\n        url=\"https://your-mcp-server.com/gmail/endpoint\",  # Replace with actual MCP server\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"SPADE_LLM/1.0\"\n        },\n        timeout=30.0,\n        sse_read_timeout=300.0,\n        terminate_on_close=True,\n        cache_tools=True\n    )\n\n    return github_mcp, notion_mcp, gmail_mcp\n\nasync def main():\n    print(\"\ud83d\ude80 Advanced Multi-Agent GitHub Monitor System\")\n    print(\"=\" * 60)\n\n    # Load environment variables\n    load_env_vars()\n\n    # Get API keys\n    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not openai_key:\n        openai_key = getpass.getpass(\"Enter your OpenAI API key: \")\n\n    # Using SPADE's built-in server (recommended)\n    print(\"\ud83d\ude80 Using SPADE's built-in server\")\n    print(\"Make sure you started it with: spade run\")\n    input(\"Press Enter when the server is running...\")\n\n    spade_server = \"localhost\"\n\n    # Agent credentials\n    agents_config = {\n        \"chat\": (f\"github_chat@{spade_server}\", \"GitHub Chat Interface\"),\n        \"analyzer\": (f\"github_analyzer@{spade_server}\", \"GitHub Analyzer Agent\"),\n        \"notion\": (f\"notion_manager@{spade_server}\", \"Notion Storage Agent\"),\n        \"email\": (f\"email_manager@{spade_server}\", \"Email Manager Agent\"),\n        \"human\": (f\"human_expert@{spade_server}\", \"Human Expert\")\n    }\n\n    # Get passwords\n    passwords = {}\n    for role, (jid, label) in agents_config.items():\n        passwords[role] = getpass.getpass(f\"{label} password: \")\n\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=openai_key,\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Create MCP servers\n    print(\"\\\\n\ud83d\udd27 Configuring MCP servers...\")\n    github_mcp, notion_mcp, gmail_mcp = create_mcp_servers()\n\n    # Create human-in-the-loop tool\n    human_tool = HumanInTheLoopTool(\n        human_expert_jid=agents_config[\"human\"][0],\n        timeout=300.0,  # 5 minutes\n        name=\"ask_human_expert\",\n        description=\"Ask human expert for email sending confirmation and recipient details\"\n    )\n\n    # Create guardrails\n    input_guardrails = create_github_guardrails()\n\n    # Create agents with structured message flow\n    print(\"\\\\n\ud83e\udd16 Creating specialized agents...\")\n\n    # AGENT CONNECTION SETUP:\n    # User \u2192 Chat Agent \u2192 Analyzer Agent \u2192 Notion Agent \u2192 Email Agent \u2192 Human Expert\n\n    # 1. Chat Agent with Guardrails (Entry Point)\n    # Receives user input and forwards to analyzer\n    def display_response(message: str, sender: str):\n        print(f\"\\\\n\ud83e\udd16 GitHub Monitor: {message}\")\n        print(\"-\" * 50)\n\n    user_chat = ChatAgent(\n        jid=agents_config[\"chat\"][0],\n        password=passwords[\"chat\"],\n        target_agent_jid=agents_config[\"analyzer\"][0],  # \u2192 Sends to Analyzer Agent\n        display_callback=display_response\n    )\n\n    # 2. GitHub Analyzer Agent (Data Collection &amp; Analysis)\n    # Receives from Chat Agent, uses GitHub MCP, forwards to Notion Agent\n    analyzer_agent = LLMAgent(\n        jid=agents_config[\"analyzer\"][0],\n        password=passwords[\"analyzer\"],\n        provider=provider,\n        system_prompt=GITHUB_ANALYZER_PROMPT,\n        input_guardrails=input_guardrails,\n        mcp_servers=[github_mcp],  # Uses GitHub MCP for data collection\n        reply_to=agents_config[\"notion\"][0]  # \u2192 Forwards to Notion Agent\n    )\n\n    # 3. Notion Manager Agent (Storage &amp; Forwarding)\n    # Receives from Analyzer Agent, uses Notion MCP, forwards to Email Agent\n    notion_agent = LLMAgent(\n        jid=agents_config[\"notion\"][0],\n        password=passwords[\"notion\"],\n        provider=provider,\n        system_prompt=NOTION_MANAGER_PROMPT,\n        mcp_servers=[notion_mcp],  # Uses Notion MCP for data storage\n        reply_to=agents_config[\"email\"][0]  # \u2192 Forwards to Email Agent\n    )\n\n    # 4. Email Manager Agent (HITL &amp; Email Sending)\n    # Receives from Notion Agent, uses Human-in-the-Loop tool and Gmail MCP\n    # NO reply_to = End of pipeline\n    email_agent = LLMAgent(\n        jid=agents_config[\"email\"][0],\n        password=passwords[\"email\"],\n        provider=provider,\n        system_prompt=EMAIL_MANAGER_PROMPT,\n        tools=[human_tool],  # Human-in-the-Loop capability\n        mcp_servers=[gmail_mcp],  # Uses Gmail MCP for email sending\n        termination_markers=[\"&lt;EMAIL_PROCESS_COMPLETE&gt;\"]  # End conversation after email process\n        # No reply_to = This is the end of the pipeline\n    )\n\n    # Start all agents\n    print(\"\\\\n\ud83d\ude80 Starting multi-agent system...\")\n    agents = {\n        \"chat\": user_chat,\n        \"analyzer\": analyzer_agent, \n        \"notion\": notion_agent,\n        \"email\": email_agent,\n    }\n\n    for name, agent in agents.items():\n        await agent.start()\n        print(f\"\u2705 {name.capitalize()} agent started\")\n\n    # Wait for connections\n    await asyncio.sleep(3.0)\n\n    print(\"\\\\n\" + \"=\"*70)\n    print(\"\ud83d\udc19 ADVANCED GITHUB MONITOR SYSTEM READY\")\n    print(\"=\"*70)\n    print(\"\\\\n\ud83c\udfaf System capabilities:\")\n    print(\"1. \ud83d\udcca Analyzes GitHub issues and pull requests via MCP\")\n    print(\"2. \ud83d\udcda Stores analysis summaries in Notion database\")\n    print(\"3. \ud83e\udd14 Consults human expert for email decisions\")\n    print(\"4. \ud83d\udce7 Sends professional email reports via Gmail MCP\")\n    print(\"\\\\n\ud83d\udee1\ufe0f Security: Custom guardrails ensure GitHub-only requests\")\n    print(\"\ud83d\udd27 Architecture: 4-agent pipeline with specialized roles\")\n    print(\"\\\\n\ud83d\udca1 Example requests:\")\n    print(\"\u2022 'Analyze recent issues in the repository'\")\n    print(\"\u2022 'Check pull request activity this week'\")\n    print(\"\u2022 'Monitor GitHub activity and send summary'\")\n    print(\"\\\\n\u26a0\ufe0f  Note: MCP servers configured for external integrations\")\n    print(\"Ensure human expert is available for email confirmations.\")\n    print(\"\\\\nType 'exit' to quit\\\\n\")\n    print(\"-\" * 70)\n\n    # Instructions for human expert\n    print(f\"\\\\n\ud83d\udc64 Human Expert Instructions:\")\n    print(f\"\ud83c\udf10 Open web interface: http://localhost:8080\")\n    print(f\"\ud83d\udd11 Connect as: {agents_config['human'][0]}\")\n    print(\"\ud83d\udce7 You'll be asked about email sending decisions\")\n\n    try:\n        # Run interactive chat\n        await user_chat.run_interactive(\n            input_prompt=\"\ud83d\udc19 GitHub&gt; \",\n            exit_command=\"exit\",\n            response_timeout=120.0  # Longer timeout for multi-agent processing\n        )\n    except KeyboardInterrupt:\n        print(\"\\\\n\ud83d\udc4b Shutting down...\")\n    finally:\n        # Stop all agents\n        print(\"\\\\n\ud83d\udd04 Stopping agents...\")\n        await user_chat.stop()\n        for name, agent in agents.items():\n            await agent.stop()\n            print(f\"\u2705 {name.capitalize()} agent stopped\")\n\n    print(\"\\\\n\u2705 Advanced GitHub Monitor system shutdown complete!\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\ude80 Starting Advanced Multi-Agent System...\")\n    print(\"\\\\n\ud83d\udccb Prerequisites:\")\n    print(\"\u2022 OpenAI API key\")\n    print(\"\u2022 XMPP server running\")\n    print(\"\u2022 MCP servers configured for GitHub/Notion/Gmail\")\n    print(\"\u2022 Human expert web interface: python -m spade_llm.human_interface.web_server\")\n    print()\n\n    try:\n        spade.run(main())\n    except KeyboardInterrupt:\n        print(\"\\\\n\ud83d\udc4b System terminated by user\")\n    except Exception as e:\n        print(f\"\\\\n\u274c System failed: {e}\")\n        print(\"\ud83d\udca1 Check your configuration and try again\")\n</code></pre>"},{"location":"getting-started/advanced-agent/#key-advanced-features","title":"Key Advanced Features","text":""},{"location":"getting-started/advanced-agent/#1-custom-guardrails","title":"1. Custom Guardrails","text":"<ul> <li>Purpose-built filtering: Only GitHub-related requests allowed</li> <li>Contextual validation: Keyword-based content analysis</li> <li>Flexible architecture: Easy to extend with new rules</li> </ul>"},{"location":"getting-started/advanced-agent/#2-mcp-integration","title":"2. MCP Integration","text":"<ul> <li>External service access: GitHub, Notion, Gmail via MCP</li> <li>Streaming support: Real-time data processing</li> <li>Caching: Improved performance with tool caching</li> </ul>"},{"location":"getting-started/advanced-agent/#3-human-in-the-loop","title":"3. Human-in-the-Loop","text":"<ul> <li>Human oversight: Expert consultation for critical decisions</li> <li>Timeout handling: Graceful degradation if human unavailable</li> <li>Web interface: User-friendly expert dashboard</li> </ul>"},{"location":"getting-started/advanced-agent/#4-multi-agent-coordination","title":"4. Multi-Agent Coordination","text":"<ul> <li>Specialized roles: Each agent has specific responsibilities</li> <li>Message routing: Structured flow between agents</li> <li>Termination markers: Clean conversation endings</li> </ul>"},{"location":"getting-started/advanced-agent/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/advanced-agent/#common-issues","title":"Common Issues","text":"<ol> <li>Agent Communication Failures</li> <li>Check XMPP server connectivity</li> <li>Verify agent JIDs and passwords</li> <li> <p>Review message routing configuration</p> </li> <li> <p>MCP Server Issues</p> </li> <li>Verify MCP server URLs and authentication</li> <li>Check network connectivity</li> <li> <p>Review timeout settings</p> </li> <li> <p>Human-in-the-Loop Problems</p> </li> <li>Ensure web interface is running</li> <li>Check human expert JID configuration</li> <li> <p>Verify timeout settings</p> </li> <li> <p>Performance Issues</p> </li> <li>Monitor agent resource usage</li> <li>Implement caching where appropriate</li> <li>Use async operations for I/O</li> </ol>"},{"location":"getting-started/advanced-agent/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've built a sophisticated multi-agent system. To continue learning:</p> <ol> <li>Explore production deployment - containerization, orchestration</li> <li>Study advanced routing patterns - conditional logic, failover</li> <li>Implement monitoring - metrics, alerting, health checks</li> <li>Add more integrations - databases, APIs, external services</li> </ol>"},{"location":"getting-started/advanced-agent/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Architecture: Design for modularity and extensibility</li> <li>Security: Implement comprehensive safety measures</li> <li>Monitoring: Add logging and observability</li> <li>Testing: Write comprehensive tests</li> <li>Documentation: Maintain clear documentation</li> <li>Configuration: Use environment-based configuration</li> <li>Error Handling: Implement graceful failure handling</li> </ol> <p>This tutorial demonstrates how SPADE-LLM can be used to build production-ready, multi-agent systems with advanced features and real-world integrations.</p>"},{"location":"getting-started/first-agent/","title":"Your First LLM Agent","text":"<p>In this tutorial, you'll learn how to create your first SPADE-LLM agent step by step. We'll start with a basic setup and gradually add features to understand the core concepts.</p>"},{"location":"getting-started/first-agent/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have: - Python 3.10 or higher installed - SPADE-LLM installed (<code>pip install spade_llm</code>) - SPADE's built-in server running (recommended - no external setup needed!) - Access to at least one LLM provider (OpenAI API key or local Ollama installation)</p>"},{"location":"getting-started/first-agent/#start-spade-server","title":"Start SPADE Server","text":"<p>New in SPADE 4.0+ - Built-in XMPP server included!</p> <pre><code># Terminal 1: Start SPADE's built-in server\nspade run\n</code></pre> <p>This eliminates the need for external XMPP servers like Prosody. Keep this running in a separate terminal while you work through the tutorial.</p>"},{"location":"getting-started/first-agent/#step-1-basic-agent-setup","title":"Step 1: Basic Agent Setup","text":"<p>Let's start with the simplest possible SPADE-LLM agent. Create <code>my_first_agent.py</code>: This agent does not have the capability to interact with us except through XMPP messages. If we want to chat with it we need to use  ChatAgent (next step) <pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Create an LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key-here\",\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Create the LLM agent (using SPADE's built-in server)\n    agent = LLMAgent(\n        jid=\"assistant@localhost\",\n        password=\"password123\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant.\",\n    )\n\n    # Start the agent\n    await agent.start()\n    print(\"\u2705 Agent started successfully!\")\n\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre></p>"},{"location":"getting-started/first-agent/#understanding-the-components","title":"Understanding the Components","text":"<p>LLMProvider: Your interface to different LLM services. SPADE-LLM supports multiple providers:</p> <pre><code># OpenAI\nprovider = LLMProvider.create_openai(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\"\n)\n\n# Ollama (local)\nprovider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    base_url=\"http://localhost:11434/v1\"\n)\n\n# LM Studio (local)\nprovider = LLMProvider.create_lm_studio(\n    model=\"local-model\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre> <p>LLMAgent: The core component that connects to XMPP, receives messages, processes them through the LLM, and sends responses back.</p> <p>Run with: <code>python my_first_agent.py</code></p>"},{"location":"getting-started/first-agent/#step-2-creating-an-interactive-chat","title":"Step 2: Creating an Interactive Chat","text":"<p>To make your agent interactive, you'll need a <code>ChatAgent</code> to handle user input. Create <code>interactive_agent.py</code>:</p> <pre><code>import spade\nimport getpass\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\nasync def main():\n    # Using SPADE's built-in server (make sure it's running!)\n    spade_server = \"localhost\"\n\n    print(\"\ud83d\ude80 Using SPADE's built-in server\")\n    print(\"Make sure you started it with: spade run\")\n    input(\"Press Enter when the server is running...\")\n\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create the LLM agent\n    llm_agent = LLMAgent(\n        jid=f\"assistant@{spade_server}\",\n        password=\"assistant_pass\",  # Simple password for built-in server\n        provider=provider,\n        system_prompt=\"You are a helpful assistant. Keep responses concise and friendly.\",\n    )\n\n    # Create the chat agent for user interaction\n    chat_agent = ChatAgent(\n        jid=f\"user@{spade_server}\",\n        password=\"user_pass\",  # Simple password for built-in server\n        target_agent_jid=f\"assistant@{spade_server}\",\n    )\n\n    try:\n        # Start both agents\n        await llm_agent.start()\n        await chat_agent.start()\n\n        print(\"\u2705 Agents started successfully!\")\n        print(\"\ud83d\udcac You can now chat with your AI assistant\")\n        print(\"Type 'exit' to quit\\n\")\n\n        # Run interactive chat\n        await chat_agent.run_interactive()\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Shutting down...\")\n    finally:\n        # Clean up\n        await chat_agent.stop()\n        await llm_agent.stop()\n        print(\"\u2705 Agents stopped successfully!\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#adding-custom-display","title":"Adding Custom Display","text":"<p>You can customize how responses are displayed:</p> <pre><code>def display_response(message: str, sender: str):\n    print(f\"\\n\ud83e\udd16 Assistant: {message}\")\n    print(\"-\" * 50)\n\ndef on_message_sent(message: str, recipient: str):\n    print(f\"\ud83d\udc64 You: {message}\")\n\nchat_agent = ChatAgent(\n    jid=f\"user@localhost\",\n    password=\"user_pass\",\n    target_agent_jid=f\"assistant@localhost\",\n    display_callback=display_response,\n    on_message_sent=on_message_sent\n)\n</code></pre>"},{"location":"getting-started/first-agent/#step-3-adding-error-handling","title":"Step 3: Adding Error Handling","text":"<p>Always include proper error handling in production code:</p> <pre><code>import spade\nimport logging\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def main():\n    try:\n        # Create provider with error handling\n        provider = LLMProvider.create_openai(\n            api_key=\"your-api-key\",\n            model=\"gpt-4o-mini\",\n            timeout=30.0  # Set timeout\n        )\n\n        # Create agents with error handling\n        llm_agent = LLMAgent(\n            jid=\"assistant@localhost\",\n            password=\"password123\",\n            provider=provider,\n            system_prompt=\"You are a helpful assistant.\"\n        )\n\n        await llm_agent.start()\n        logger.info(\"\u2705 LLM Agent started successfully\")\n\n        # Your chat logic here...\n\n    except Exception as e:\n        logger.error(f\"\u274c Error: {e}\")\n        print(\"\ud83d\udca1 Check your configuration and try again\")\n    finally:\n        # Always clean up\n        if 'llm_agent' in locals():\n            await llm_agent.stop()\n        if 'chat_agent' in locals():\n            await chat_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#complete-example-with-best-practices","title":"Complete Example with Best Practices","text":"<p>Here's a complete, production-ready example that demonstrates best practices:</p> <pre><code>import spade\nimport getpass\nimport logging\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def main():\n    print(\"\ud83d\ude80 Starting your first SPADE-LLM agent!\")\n    print(\"\ud83d\udccb Make sure SPADE server is running: spade run\")\n    input(\"Press Enter when the server is running...\")\n\n    # Configuration - using built-in SPADE server\n    spade_server = \"localhost\"\n\n    # Create provider (choose one)\n    provider_type = input(\"Provider (openai/ollama): \").lower()\n\n    if provider_type == \"openai\":\n        api_key = getpass.getpass(\"OpenAI API key: \")\n        provider = LLMProvider.create_openai(\n            api_key=api_key,\n            model=\"gpt-4o-mini\",\n            timeout=30.0\n        )\n    else:  # ollama\n        model = input(\"Ollama model (default: llama3.1:8b): \") or \"llama3.1:8b\"\n        provider = LLMProvider.create_ollama(\n            model=model,\n            base_url=\"http://localhost:11434/v1\",\n            timeout=60.0\n        )\n\n    # Simple passwords for built-in server (no need for getpass)\n    llm_password = \"assistant_pass\"\n    chat_password = \"user_pass\"\n\n    # Create agents\n    llm_agent = LLMAgent(\n        jid=f\"assistant@{spade_server}\",\n        password=llm_password,\n        provider=provider,\n        system_prompt=\"You are a helpful and friendly AI assistant. Keep responses concise but informative.\"\n    )\n\n    def display_response(message: str, sender: str):\n        print(f\"\\n\ud83e\udd16 Assistant: {message}\")\n        print(\"-\" * 50)\n\n    def on_message_sent(message: str, recipient: str):\n        print(f\"\ud83d\udc64 You: {message}\")\n\n    chat_agent = ChatAgent(\n        jid=f\"user@{spade_server}\",\n        password=chat_password,\n        target_agent_jid=f\"assistant@{spade_server}\",\n        display_callback=display_response,\n        on_message_sent=on_message_sent\n    )\n\n    try:\n        # Start agents\n        await llm_agent.start()\n        await chat_agent.start()\n\n        logger.info(\"\u2705 Agents started successfully!\")\n        print(\"\ud83d\udcac Start chatting with your AI assistant\")\n        print(\"Type 'exit' to quit\\n\")\n\n        # Run interactive chat\n        await chat_agent.run_interactive()\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Shutting down...\")\n    except Exception as e:\n        logger.error(f\"\u274c Error: {e}\")\n        print(\"\ud83d\udca1 Check your configuration and try again\")\n    finally:\n        # Clean up\n        try:\n            await chat_agent.stop()\n            await llm_agent.stop()\n            logger.info(\"\u2705 Agents stopped successfully!\")\n        except Exception as e:\n            logger.error(f\"Error during cleanup: {e}\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#testing-your-agent","title":"Testing Your Agent","text":"<ol> <li>Start SPADE server: <code>spade run</code></li> <li>Run the agent: <code>python my_first_agent.py</code></li> <li>Choose your provider: OpenAI or Ollama</li> <li>Start chatting: Type messages and get responses</li> </ol>"},{"location":"getting-started/first-agent/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"getting-started/first-agent/#spade-server-issues","title":"SPADE Server Issues","text":"<ul> <li>Server won't start: Check if port 5222 is already in use (<code>netstat -an | grep 5222</code>)</li> <li>Port conflicts: Try different ports: <code>spade run --client_port 6222 --server_port 6269</code></li> <li>Agent connection fails: Ensure server is running before starting agents</li> </ul>"},{"location":"getting-started/first-agent/#connection-problems","title":"Connection Problems","text":"<ul> <li>Agent won't start: Ensure SPADE built-in server is running first</li> <li>Authentication failed: Built-in server auto-registers agents, but verify JID format</li> <li>Network issues: Built-in server runs locally, check firewall settings</li> </ul>"},{"location":"getting-started/first-agent/#llm-provider-issues","title":"LLM Provider Issues","text":"<ul> <li>OpenAI errors: Verify API key and account credits</li> <li>Ollama not responding: Check if <code>ollama serve</code> is running</li> <li>Timeout errors: Increase timeout values for slow models</li> </ul>"},{"location":"getting-started/first-agent/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use appropriate model sizes for your hardware</li> <li>Set reasonable timeouts based on your provider</li> <li>Monitor token usage for cost optimization</li> <li>Use local models (Ollama) for development</li> </ul>"},{"location":"getting-started/first-agent/#next-steps","title":"Next Steps","text":"<p>Now that you have a working agent, explore these advanced features:</p> <ol> <li>Custom Tools Tutorial - Add function calling capabilities</li> <li>Guardrails Tutorial - Implement safety and content filtering  </li> <li>Advanced Agent Tutorial - Multi-agent workflows and integrations</li> </ol> <p>Each tutorial builds on the concepts you've learned here, gradually adding more sophisticated capabilities to your agents.</p>"},{"location":"getting-started/guardrails-tutorial/","title":"Guardrails System Tutorial","text":"<p>In this tutorial, you'll learn how to implement safety and content filtering in your SPADE-LLM agents using the guardrails system. We'll build on the concepts from the First Agent Tutorial and add layers of protection.</p>"},{"location":"getting-started/guardrails-tutorial/#what-are-guardrails","title":"What are Guardrails?","text":"<p>Guardrails are safety mechanisms that filter and validate content before it reaches your LLM or before responses are sent to users. They provide essential protection against:</p> <ul> <li>Harmful or malicious content</li> <li>Inappropriate language</li> <li>Sensitive information leakage</li> <li>Policy violations</li> <li>Unsafe AI responses</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#types-of-guardrails","title":"Types of Guardrails","text":"<p>SPADE-LLM supports two types of guardrails:</p> <ol> <li>Input Guardrails: Filter incoming messages before they reach the LLM</li> <li>Output Guardrails: Validate LLM responses before sending them to users</li> </ol>"},{"location":"getting-started/guardrails-tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Complete the First Agent Tutorial</li> <li>SPADE-LLM installed with all dependencies</li> <li>Access to an LLM provider (OpenAI or Ollama)</li> <li>XMPP server running</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#step-1-basic-keyword-filtering","title":"Step 1: Basic Keyword Filtering","text":"<p>Let's start with simple keyword-based filtering. This example blocks harmful content:</p> <pre><code>import spade\nimport getpass\nimport logging\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\nfrom spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def main():\n    print(\"\ud83d\udee1\ufe0f Guardrails Tutorial: Basic Keyword Filtering\")\n\n    # Configuration\n    xmpp_server = input(\"XMPP server domain (default: localhost): \") or \"localhost\"\n\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=getpass.getpass(\"OpenAI API key: \"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create keyword guardrail that BLOCKS harmful content\n    safety_guardrail = KeywordGuardrail(\n        name=\"harmful_content_filter\",\n        blocked_keywords=[\"hack\", \"exploit\", \"malware\", \"virus\", \"illegal\", \"bomb\"],\n        action=GuardrailAction.BLOCK,\n        case_sensitive=False,\n        blocked_message=\"I cannot help with potentially harmful activities.\"\n    )\n\n    # Create LLM agent with input guardrail\n    llm_agent = LLMAgent(\n        jid=f\"safe_assistant@{xmpp_server}\",\n        password=getpass.getpass(\"LLM agent password: \"),\n        provider=provider,\n        system_prompt=\"You are a helpful and safe AI assistant.\",\n        input_guardrails=[safety_guardrail]  # Apply to incoming messages\n    )\n\n    # Create chat agent\n    def display_response(message: str, sender: str):\n        print(f\"\\n\ud83e\udd16 Safe Assistant: {message}\")\n        print(\"-\" * 50)\n\n    chat_agent = ChatAgent(\n        jid=f\"user@{xmpp_server}\",\n        password=getpass.getpass(\"Chat agent password: \"),\n        target_agent_jid=f\"safe_assistant@{xmpp_server}\",\n        display_callback=display_response\n    )\n\n    try:\n        # Start agents\n        await llm_agent.start()\n        await chat_agent.start()\n\n        print(\"\u2705 Safe assistant started!\")\n        print(\"\ud83e\uddea Test with: 'How to hack a system?' (should be blocked)\")\n        print(\"\ud83d\udcac Or try: 'How to protect my computer?' (should pass)\")\n        print(\"Type 'exit' to quit\\n\")\n\n        # Run interactive chat\n        await chat_agent.run_interactive()\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Shutting down...\")\n    finally:\n        await chat_agent.stop()\n        await llm_agent.stop()\n        print(\"\u2705 Agents stopped successfully!\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#step-2-content-modification","title":"Step 2: Content Modification","text":"<p>Instead of blocking content, you can modify it. This example replaces profanity:</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Create profanity filter that MODIFIES content\nprofanity_guardrail = KeywordGuardrail(\n    name=\"profanity_filter\",\n    blocked_keywords=[\"damn\", \"hell\", \"stupid\", \"idiot\", \"crap\"],\n    action=GuardrailAction.MODIFY,\n    replacement=\"[FILTERED]\",\n    case_sensitive=False\n)\n\n# Add to agent\nllm_agent = LLMAgent(\n    jid=f\"polite_assistant@{xmpp_server}\",\n    password=llm_password,\n    provider=provider,\n    system_prompt=\"You are a polite and helpful assistant.\",\n    input_guardrails=[profanity_guardrail]\n)\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#step-3-multiple-guardrails-pipeline","title":"Step 3: Multiple Guardrails Pipeline","text":"<p>You can chain multiple guardrails together. They execute in order:</p> <pre><code>def create_input_guardrails():\n    \"\"\"Create a pipeline of input guardrails.\"\"\"\n\n    # 1. Block harmful content\n    safety_filter = KeywordGuardrail(\n        name=\"harmful_content_filter\",\n        blocked_keywords=[\"hack\", \"exploit\", \"malware\", \"virus\", \"illegal\", \"bomb\"],\n        action=GuardrailAction.BLOCK,\n        case_sensitive=False,\n        blocked_message=\"I cannot help with potentially harmful activities.\"\n    )\n\n    # 2. Filter profanity\n    profanity_filter = KeywordGuardrail(\n        name=\"profanity_filter\",\n        blocked_keywords=[\"damn\", \"hell\", \"stupid\", \"idiot\", \"crap\"],\n        action=GuardrailAction.MODIFY,\n        replacement=\"[FILTERED]\",\n        case_sensitive=False\n    )\n\n    return [safety_filter, profanity_filter]\n\n# Use in agent\ninput_guardrails = create_input_guardrails()\nllm_agent = LLMAgent(\n    jid=f\"protected_assistant@{xmpp_server}\",\n    password=llm_password,\n    provider=provider,\n    system_prompt=\"You are a safe and polite assistant.\",\n    input_guardrails=input_guardrails\n)\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#step-4-monitoring-guardrail-actions","title":"Step 4: Monitoring Guardrail Actions","text":"<p>Track when guardrails are triggered:</p> <pre><code>def on_guardrail_trigger(result):\n    \"\"\"Callback function for when guardrails are triggered.\"\"\"\n    if result.action == GuardrailAction.BLOCK:\n        print(f\"\ud83d\udeab GUARDRAIL BLOCKED: {result.reason}\")\n        logger.warning(f\"Blocked content: {result.reason}\")\n    elif result.action == GuardrailAction.MODIFY:\n        print(f\"\u270f\ufe0f GUARDRAIL MODIFIED: {result.reason}\")\n        logger.info(f\"Modified content: {result.reason}\")\n    elif result.action == GuardrailAction.WARNING:\n        print(f\"\u26a0\ufe0f GUARDRAIL WARNING: {result.reason}\")\n        logger.warning(f\"Warning: {result.reason}\")\n\n# Add monitoring to agent\nllm_agent = LLMAgent(\n    jid=f\"monitored_assistant@{xmpp_server}\",\n    password=llm_password,\n    provider=provider,\n    system_prompt=\"You are a monitored assistant.\",\n    input_guardrails=input_guardrails,\n    on_guardrail_trigger=on_guardrail_trigger  # Monitor all guardrail actions\n)\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#step-5-output-guardrails-with-llm-validation","title":"Step 5: Output Guardrails with LLM Validation","text":"<p>For more sophisticated safety, use LLM-based output validation:</p> <pre><code>from spade_llm.guardrails import LLMGuardrail\n\ndef create_output_guardrails(safety_provider):\n    \"\"\"Create output guardrails using LLM validation.\"\"\"\n\n    # LLM-based safety checker\n    safety_guardrail = LLMGuardrail(\n        name=\"llm_safety_checker\",\n        provider=safety_provider,\n        safety_prompt=\"\"\"\n        Analyze this AI response for safety. Check for:\n        - Harmful instructions or advice\n        - Personal attacks or harassment\n        - Inappropriate content\n        - Anything that could cause harm\n\n        Respond with JSON: {\"safe\": true/false, \"reason\": \"explanation if unsafe\"}\n\n        AI Response: {content}\n        \"\"\",\n        blocked_message=\"I apologize, but I cannot provide that response due to safety concerns.\"\n    )\n\n    return [safety_guardrail]\n\n# Create separate provider for safety validation\nsafety_provider = LLMProvider.create_openai(\n    api_key=api_key,\n    model=\"gpt-4o-mini\",\n    temperature=0.3  # Lower temperature for safety validation\n)\n\noutput_guardrails = create_output_guardrails(safety_provider)\n\n# Add to agent\nllm_agent = LLMAgent(\n    jid=f\"llm_protected_assistant@{xmpp_server}\",\n    password=llm_password,\n    provider=provider,\n    system_prompt=\"You are a helpful assistant with LLM safety validation.\",\n    input_guardrails=input_guardrails,\n    output_guardrails=output_guardrails,  # Validate responses\n    on_guardrail_trigger=on_guardrail_trigger\n)\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#step-6-custom-guardrails","title":"Step 6: Custom Guardrails","text":"<p>Create custom guardrails for specific use cases:</p> <pre><code>from spade_llm.guardrails.base import Guardrail, GuardrailResult\nfrom typing import Dict, Any\nimport re\n\nclass EmailRedactionGuardrail(Guardrail):\n    \"\"\"Custom guardrail that redacts email addresses.\"\"\"\n\n    def __init__(self, name: str = \"email_redaction\", enabled: bool = True):\n        super().__init__(name, enabled, \"Email addresses are automatically redacted for privacy.\")\n        self.email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n\n    async def check(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult:\n        \"\"\"Check and redact email addresses.\"\"\"\n        if self.email_pattern.search(content):\n            # Redact email addresses\n            redacted_content = self.email_pattern.sub('[EMAIL_REDACTED]', content)\n            return GuardrailResult(\n                action=GuardrailAction.MODIFY,\n                content=redacted_content,\n                reason=\"Email addresses redacted for privacy\"\n            )\n        else:\n            return GuardrailResult(\n                action=GuardrailAction.PASS,\n                content=content,\n                reason=\"No email addresses found\"\n            )\n\n# Use custom guardrail\nemail_redaction = EmailRedactionGuardrail()\ncustom_guardrails = [email_redaction]\n\nllm_agent = LLMAgent(\n    jid=f\"privacy_assistant@{xmpp_server}\",\n    password=llm_password,\n    provider=provider,\n    system_prompt=\"You are a privacy-conscious assistant.\",\n    input_guardrails=custom_guardrails,\n    on_guardrail_trigger=on_guardrail_trigger\n)\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#complete-example-multi-layer-protection","title":"Complete Example: Multi-Layer Protection","text":"<p>Here's a complete example demonstrating all guardrail types:</p> <pre><code>import spade\nimport getpass\nimport logging\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\nfrom spade_llm.guardrails import (\n    KeywordGuardrail, LLMGuardrail,\n    GuardrailAction, InputGuardrail, OutputGuardrail\n)\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef create_input_guardrails():\n    \"\"\"Create comprehensive input guardrail pipeline.\"\"\"\n\n    # 1. Block harmful content\n    harmful_filter = KeywordGuardrail(\n        name=\"harmful_content_filter\",\n        blocked_keywords=[\"hack\", \"exploit\", \"malware\", \"virus\", \"illegal\", \"bomb\"],\n        action=GuardrailAction.BLOCK,\n        case_sensitive=False,\n        blocked_message=\"I cannot help with potentially harmful activities.\"\n    )\n\n    # 2. Filter profanity\n    profanity_filter = KeywordGuardrail(\n        name=\"profanity_filter\", \n        blocked_keywords=[\"damn\", \"hell\", \"stupid\", \"idiot\", \"crap\"],\n        action=GuardrailAction.MODIFY,\n        replacement=\"[FILTERED]\",\n        case_sensitive=False\n    )\n\n    return [harmful_filter, profanity_filter]\n\ndef create_output_guardrails(safety_provider):\n    \"\"\"Create output guardrail pipeline with LLM validation.\"\"\"\n\n    # LLM-based safety checker\n    safety_guardrail = LLMGuardrail(\n        name=\"llm_safety_checker\",\n        provider=safety_provider,\n        safety_prompt=\"\"\"\n        Analyze this AI response for safety. Check for harmful instructions, \n        personal attacks, inappropriate content, or anything that could cause harm.\n\n        Respond with JSON: {\"safe\": true/false, \"reason\": \"explanation if unsafe\"}\n\n        AI Response: {content}\n        \"\"\",\n        blocked_message=\"I apologize, but I cannot provide that response due to safety concerns.\"\n    )\n\n    return [safety_guardrail]\n\ndef on_guardrail_trigger(result):\n    \"\"\"Monitor all guardrail actions.\"\"\"\n    timestamp = logging.Formatter().formatTime(logging.LogRecord(\n        name=\"guardrail\", level=logging.INFO, pathname=\"\", lineno=0,\n        msg=\"\", args=(), exc_info=None\n    ))\n\n    if result.action == GuardrailAction.BLOCK:\n        print(f\"\ud83d\udeab [{timestamp}] BLOCKED: {result.reason}\")\n    elif result.action == GuardrailAction.MODIFY:\n        print(f\"\u270f\ufe0f [{timestamp}] MODIFIED: {result.reason}\")\n    elif result.action == GuardrailAction.WARNING:\n        print(f\"\u26a0\ufe0f [{timestamp}] WARNING: {result.reason}\")\n\nasync def main():\n    print(\"\ud83d\udee1\ufe0f Multi-Layer Guardrails Example\")\n\n    # Configuration\n    xmpp_server = input(\"XMPP server domain (default: localhost): \") or \"localhost\"\n    api_key = getpass.getpass(\"OpenAI API key: \")\n\n    # Create providers\n    main_provider = LLMProvider.create_openai(\n        api_key=api_key,\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    safety_provider = LLMProvider.create_openai(\n        api_key=api_key,\n        model=\"gpt-4o-mini\",\n        temperature=0.3  # Lower temperature for safety validation\n    )\n\n    # Create guardrails\n    input_guardrails = create_input_guardrails()\n    output_guardrails = create_output_guardrails(safety_provider)\n\n    # Create protected agent\n    llm_agent = LLMAgent(\n        jid=f\"guardian_ai@{xmpp_server}\",\n        password=getpass.getpass(\"LLM agent password: \"),\n        provider=main_provider,\n        system_prompt=\"You are a helpful AI assistant with comprehensive safety guardrails.\",\n        input_guardrails=input_guardrails,\n        output_guardrails=output_guardrails,\n        on_guardrail_trigger=on_guardrail_trigger\n    )\n\n    # Create chat interface\n    def display_response(message: str, sender: str):\n        print(f\"\\n\ud83e\udd16 Guardian AI: {message}\")\n        print(\"-\" * 50)\n\n    chat_agent = ChatAgent(\n        jid=f\"user@{xmpp_server}\",\n        password=getpass.getpass(\"Chat agent password: \"),\n        target_agent_jid=f\"guardian_ai@{xmpp_server}\",\n        display_callback=display_response\n    )\n\n    try:\n        # Start agents\n        await llm_agent.start()\n        await chat_agent.start()\n\n        print(\"\u2705 Guardian AI started with multi-layer protection!\")\n        print(\"\ud83d\udee1\ufe0f Protection layers:\")\n        print(\"\u2022 Input: Harmful content blocker, profanity filter\")\n        print(\"\u2022 Output: LLM safety validator\")\n        print(\"\\n\ud83e\uddea Test the system:\")\n        print(\"\u2022 Normal questions (should pass)\")\n        print(\"\u2022 Harmful requests (will be blocked)\")\n        print(\"\u2022 Messages with profanity (will be filtered)\")\n        print(\"Type 'exit' to quit\\n\")\n\n        # Run interactive chat\n        await chat_agent.run_interactive()\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Shutting down...\")\n    finally:\n        await chat_agent.stop()\n        await llm_agent.stop()\n        print(\"\u2705 Guardian AI stopped successfully!\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/guardrails-tutorial/#testing-your-guardrails","title":"Testing Your Guardrails","text":""},{"location":"getting-started/guardrails-tutorial/#test-cases-to-try","title":"Test Cases to Try","text":"<ol> <li>Normal queries: \"What's the weather like?\"</li> <li>Harmful content: \"How to hack into a system?\"</li> <li>Profanity: \"This is damn difficult\"</li> <li>Mixed content: \"Help me with this stupid computer problem\"</li> </ol>"},{"location":"getting-started/guardrails-tutorial/#expected-behaviors","title":"Expected Behaviors","text":"<ul> <li>BLOCK: Harmful requests should be completely blocked</li> <li>MODIFY: Profanity should be replaced with [FILTERED]</li> <li>PASS: Normal content should go through unchanged</li> <li>WARNING: Borderline content should trigger warnings</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/guardrails-tutorial/#1-layer-your-defense","title":"1. Layer Your Defense","text":"<ul> <li>Use multiple guardrails for comprehensive protection</li> <li>Combine keyword filtering with LLM validation</li> <li>Apply both input and output guardrails</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#2-monitor-and-log","title":"2. Monitor and Log","text":"<ul> <li>Always use guardrail monitoring callbacks</li> <li>Log all guardrail actions for analysis</li> <li>Track patterns in blocked content</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#3-balance-safety-and-usability","title":"3. Balance Safety and Usability","text":"<ul> <li>Don't make guardrails too restrictive</li> <li>Provide clear messages when content is blocked</li> <li>Allow users to rephrase blocked requests</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#4-regular-updates","title":"4. Regular Updates","text":"<ul> <li>Update keyword lists based on monitoring</li> <li>Review and improve safety prompts</li> <li>Test guardrails with new content types</li> </ul>"},{"location":"getting-started/guardrails-tutorial/#next-steps","title":"Next Steps","text":"<p>Now that you understand guardrails, explore:</p> <ol> <li>Custom Tools Tutorial - Add function calling with safety</li> <li>Advanced Agent Tutorial - Complex workflows with protection</li> <li>API Reference - Complete guardrails documentation</li> </ol> <p>The guardrails system provides essential protection for your AI agents, ensuring they operate safely and responsibly in production environments.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>4GB+ RAM (8GB+ for local models)</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>pip install spade_llm\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import spade_llm\nfrom spade_llm import LLMAgent, LLMProvider\n\nprint(f\"SPADE_LLM version: {spade_llm.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#xmpp-server-setup","title":"XMPP Server Setup","text":""},{"location":"getting-started/installation/#built-in-spade-server-recommended","title":"Built-in SPADE Server (Recommended)","text":"<p>SPADE 4.0+ includes a built-in XMPP server - no external server setup required!</p> <p><pre><code># Start SPADE's built-in server (simplest setup)\nspade run\n</code></pre> The built-in server provides everything you need to run SPADE-LLM agents locally. Simply start the server in one terminal and run your agents in another.</p>"},{"location":"getting-started/installation/#advanced-xmpp-server-configuration","title":"Advanced XMPP Server Configuration","text":"<p>For custom setups, you can specify different ports and hosts:</p> <pre><code># Custom host and ports\nspade run --host localhost --client_port 6222 --server_port 6269\n\n# Use IP address instead of localhost\nspade run --host 127.0.0.1 --client_port 6222 --server_port 6269\n\n# Custom ports if defaults are in use\nspade run --client_port 6223 --server_port 6270\n</code></pre>"},{"location":"getting-started/installation/#alternative-external-xmpp-servers","title":"Alternative: External XMPP Servers","text":"<p>For production or if you prefer external servers:</p> <ul> <li>Public servers: <code>jabber.at</code>, <code>jabber.org</code> (require manual account creation)</li> <li>Local Prosody: Install Prosody for local hosting</li> <li>Other servers: Any XMPP-compliant server</li> </ul>"},{"location":"getting-started/installation/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>Choose one provider:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/installation/#ollama-local","title":"Ollama (Local)","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Download a model\nollama pull llama3.1:8b\nollama serve\n</code></pre>"},{"location":"getting-started/installation/#lm-studio-local","title":"LM Studio (Local)","text":"<ol> <li>Download LM Studio</li> <li>Download a model through the GUI</li> <li>Start the local server</li> </ol>"},{"location":"getting-started/installation/#development-install","title":"Development Install","text":"<pre><code>git clone https://github.com/javipalanca/spade_llm.git\ncd spade_llm\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>SPADE server not starting:  <pre><code># Check if default ports are already in use\nnetstat -an | grep 5222\n\n# Try different ports if needed\nspade run --client_port 6222 --server_port 6269\n</code></pre></p> <p>Agent connection issues: Ensure SPADE server is running first <pre><code># Terminal 1: Start server\nspade run\n\n# Terminal 2: Run your agent\npython your_agent.py\n</code></pre></p> <p>Import errors: Ensure you're in the correct Python environment <pre><code>python -m pip install spade_llm\n</code></pre></p> <p>SSL errors: For development with built-in server, disable SSL verification: <pre><code>agent = LLMAgent(..., verify_security=False)\n</code></pre></p> <p>Ollama connection: Check if Ollama is running: <pre><code>curl http://localhost:11434/v1/models\n</code></pre></p>"},{"location":"getting-started/quickstart/","title":"Quick Start Tutorials","text":"<p>Learn SPADE-LLM step by step with our comprehensive tutorial series. Each tutorial builds on the previous one, taking you from basic concepts to advanced multi-agent systems.</p>"},{"location":"getting-started/quickstart/#tutorial-series","title":"\ud83d\udcda Tutorial Series","text":"<p>Learning Path</p> <p>Each tutorial builds on the previous one. Start with Tutorial 1 if you're new to SPADE-LLM.</p>"},{"location":"getting-started/quickstart/#tutorial-1-your-first-agent","title":"\ud83d\ude80 Tutorial 1: Your First Agent","text":"<p>Start here if you're new to SPADE-LLM</p> <p>Learn the basics of creating and running your first LLM-powered agent.</p> <p>What you'll learn:</p> <ul> <li>Basic agent setup and configuration</li> <li>LLM provider integration (OpenAI, Ollama)</li> <li>Interactive chat interfaces</li> <li>Error handling and best practices</li> </ul> <p>Duration: 15-20 minutes</p> <p>Start Tutorial \u2192</p>"},{"location":"getting-started/quickstart/#tutorial-2-guardrails-and-safety","title":"\ud83d\udee1\ufe0f Tutorial 2: Guardrails and Safety","text":"<p>Add safety and content filtering to your agents</p> <p>Implement comprehensive protection systems for your AI agents.</p> <p>What you'll learn:</p> <ul> <li>Input and output content filtering</li> <li>Custom guardrail creation</li> <li>LLM-based safety validation</li> <li>Monitoring and logging guardrail actions</li> </ul> <p>Duration: 20-25 minutes</p> <p>Start Tutorial \u2192</p>"},{"location":"getting-started/quickstart/#tutorial-3-custom-tools","title":"\ud83d\udd27 Tutorial 3: Custom Tools","text":"<p>Extend your agents with function calling</p> <p>Give your agents the ability to perform actions beyond text generation.</p> <p>What you'll learn:</p> <ul> <li>Creating custom tools and functions</li> <li>Tool parameter schemas</li> <li>Async tool execution</li> <li>External API integration</li> <li>Tool composition and chaining</li> </ul> <p>Duration: 25-30 minutes</p> <p>Start Tutorial \u2192</p>"},{"location":"getting-started/quickstart/#tutorial-4-advanced-multi-agent-systems","title":"\ud83c\udfd7\ufe0f Tutorial 4: Advanced Multi-Agent Systems","text":"<p>Build production-ready multi-agent workflows</p> <p>Create sophisticated systems with multiple specialized agents working together.</p> <p>What you'll learn:</p> <ul> <li>Agent-to-agent communication</li> <li>MCP server integration</li> <li>Human-in-the-loop workflows</li> <li>Custom guardrails and routing</li> <li>Production deployment patterns</li> </ul> <p>Duration: 45-60 minutes</p> <p>Start Tutorial \u2192</p>"},{"location":"getting-started/quickstart/#choose-your-path","title":"\ud83c\udfaf Choose Your Path","text":""},{"location":"getting-started/quickstart/#complete-beginner","title":"\ud83d\udc76 Complete Beginner","text":"<p>Start with Tutorial 1 and work through all tutorials in order.</p>"},{"location":"getting-started/quickstart/#have-basic-knowledge","title":"\ud83d\udd27 Have Basic Knowledge","text":"<p>Already familiar with SPADE-LLM basics? Jump to Tutorial 2 or Tutorial 3.</p>"},{"location":"getting-started/quickstart/#advanced-user","title":"\ud83c\udfc6 Advanced User","text":"<p>Building production systems? Go directly to Tutorial 4 for advanced patterns.</p>"},{"location":"getting-started/quickstart/#quick-setup","title":"\ud83d\udee0\ufe0f Quick Setup","text":"<p>Before starting any tutorial, make sure you have:</p>"},{"location":"getting-started/quickstart/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Python 3.10+ installed</li> <li>SPADE-LLM installed: <code>pip install spade_llm</code></li> <li>SPADE built-in server running (recommended for beginners)</li> </ul>"},{"location":"getting-started/quickstart/#2-start-spade-server","title":"2. Start SPADE Server","text":"<p>New in SPADE 4.0 - No external server needed!</p> <pre><code># Terminal 1: Start the built-in SPADE server\nspade run\n</code></pre> <p>This provides everything you need - no complex XMPP setup required!</p>"},{"location":"getting-started/quickstart/#3-llm-provider-access","title":"3. LLM Provider Access","text":"<p>Choose one:</p> <p>OpenAI (easiest for beginners): <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre></p> <p>Ollama (free, local option): <pre><code>ollama pull llama3.1:8b\nollama serve\n</code></pre></p>"},{"location":"getting-started/quickstart/#4-ready-to-go","title":"4. Ready to Go!","text":"<p>With your SPADE server running and LLM provider configured, pick a tutorial above and start building! \ud83d\ude80</p>"},{"location":"getting-started/quickstart/#tips-for-success","title":"\ud83d\udca1 Tips for Success","text":"<ul> <li>Follow the order: Each tutorial builds on previous concepts</li> <li>Try the examples: Run all code examples as you go</li> <li>Experiment: Modify examples to understand how they work</li> <li>Ask questions: Use the examples as a foundation for your own projects</li> </ul>"},{"location":"getting-started/quickstart/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Installation Guide - Detailed setup instructions</li> <li>API Reference - Complete documentation</li> <li>Examples - Working code examples</li> <li>Guides - In-depth feature explanations</li> </ul> <p>Ready to build intelligent multi-agent systems? Start with Tutorial 1 and begin your journey!</p>"},{"location":"getting-started/tools-tutorial/","title":"Custom Tools Tutorial","text":"<p>In this tutorial, you'll learn how to create and use custom tools with your SPADE-LLM agents. Tools enable your agents to perform actions beyond text generation, such as retrieving data, performing calculations, or interacting with external services.</p>"},{"location":"getting-started/tools-tutorial/#what-are-tools","title":"What are Tools?","text":"<p>Tools are Python functions that your LLM agent can call to perform specific tasks. When the LLM needs to execute an action, it can:</p> <ol> <li>Identify which tool to use</li> <li>Extract the required parameters</li> <li>Execute the tool function</li> <li>Use the results to generate a response</li> </ol>"},{"location":"getting-started/tools-tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Complete the First Agent Tutorial</li> <li>Basic understanding of Python functions</li> <li>SPADE-LLM installed with all dependencies</li> <li>Access to an LLM provider that supports function calling</li> </ul>"},{"location":"getting-started/tools-tutorial/#step-1-basic-tool-creation","title":"Step 1: Basic Tool Creation","text":"<p>Let's start with simple tools. Based on the <code>ollama_with_tools_example.py</code>, here's how to create basic tools:</p> <pre><code>import spade\nimport getpass\nfrom datetime import datetime\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider, LLMTool\n\n# Simple tool functions\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef calculate_math(expression: str) -&gt; str:\n    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n    try:\n        # Only allow basic math operations for safety\n        allowed_names = {\n            k: v for k, v in __builtins__.items() \n            if k in ['abs', 'round', 'min', 'max', 'sum']\n        }\n        result = eval(expression, {\"__builtins__\": allowed_names})\n        return str(result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get simulated weather for a city.\"\"\"\n    weather_data = {\n        \"madrid\": \"22\u00b0C, sunny\",\n        \"london\": \"15\u00b0C, cloudy\",\n        \"new york\": \"18\u00b0C, rainy\",\n        \"tokyo\": \"25\u00b0C, clear\",\n        \"paris\": \"19\u00b0C, partly cloudy\",\n        \"barcelona\": \"24\u00b0C, sunny\"\n    }\n    return weather_data.get(city.lower(), f\"No weather data available for {city}\")\n\nasync def main():\n    print(\"\ud83d\udd27 Custom Tools Tutorial: Basic Tool Creation\")\n\n    # Configuration\n    xmpp_server = input(\"XMPP server domain (default: localhost): \") or \"localhost\"\n\n    # Create provider (using Ollama as in the example)\n    provider = LLMProvider.create_ollama(\n        model=\"qwen2.5:7b\",  # Or any model that supports function calling\n        base_url=\"http://localhost:11434/v1\",\n        temperature=0.7\n    )\n\n    # Create tools with proper schema definitions\n    tools = [\n        LLMTool(\n            name=\"get_current_time\",\n            description=\"Get the current date and time\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": []\n            },\n            func=get_current_time\n        ),\n        LLMTool(\n            name=\"calculate_math\",\n            description=\"Safely evaluate a mathematical expression\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\n                        \"type\": \"string\",\n                        \"description\": \"Mathematical expression to evaluate (e.g., '2 + 3 * 4')\"\n                    }\n                },\n                \"required\": [\"expression\"]\n            },\n            func=calculate_math\n        ),\n        LLMTool(\n            name=\"get_weather\",\n            description=\"Get weather information for a city\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"City name (e.g., 'Madrid', 'London')\"\n                    }\n                },\n                \"required\": [\"city\"]\n            },\n            func=get_weather\n        )\n    ]\n\n    # Create LLM agent with tools\n    llm_agent = LLMAgent(\n        jid=f\"tool_assistant@{xmpp_server}\",\n        password=getpass.getpass(\"LLM agent password: \"),\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with access to tools: get_current_time, calculate_math, and get_weather. Use these tools when appropriate to help users.\",\n        tools=tools  # Pass tools to the agent\n    )\n\n    # Create chat interface\n    def display_response(message: str, sender: str):\n        print(f\"\\n\ud83e\udd16 Tool Assistant: {message}\")\n        print(\"-\" * 50)\n\n    chat_agent = ChatAgent(\n        jid=f\"user@{xmpp_server}\",\n        password=getpass.getpass(\"Chat agent password: \"),\n        target_agent_jid=f\"tool_assistant@{xmpp_server}\",\n        display_callback=display_response\n    )\n\n    try:\n        # Start agents\n        await llm_agent.start()\n        await chat_agent.start()\n\n        print(\"\u2705 Tool assistant started!\")\n        print(\"\ud83d\udd27 Available tools:\")\n        print(\"\u2022 get_current_time - Get current date and time\")\n        print(\"\u2022 calculate_math - Perform mathematical calculations\")\n        print(\"\u2022 get_weather - Get weather for major cities\")\n        print(\"\\n\ud83d\udca1 Try these queries:\")\n        print(\"\u2022 'What time is it?'\")\n        print(\"\u2022 'Calculate 15 * 8 + 32'\")\n        print(\"\u2022 'What's the weather in Madrid?'\")\n        print(\"Type 'exit' to quit\\n\")\n\n        # Run interactive chat\n        await chat_agent.run_interactive()\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Shutting down...\")\n    finally:\n        await chat_agent.stop()\n        await llm_agent.stop()\n        print(\"\u2705 Agents stopped successfully!\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\ude80 Prerequisites:\")\n    print(\"\u2022 Ollama running: ollama serve\")\n    print(\"\u2022 Model available: ollama pull qwen2.5:7b\")\n    print(\"\u2022 XMPP server running\")\n    print()\n\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/tools-tutorial/#step-2-understanding-tool-schemas","title":"Step 2: Understanding Tool Schemas","text":"<p>Tool schemas define what parameters your tools accept. Here's the structure:</p> <pre><code># Tool schema follows JSON Schema format\ntool_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"parameter_name\": {\n            \"type\": \"string\",  # or \"number\", \"boolean\", \"array\", \"object\"\n            \"description\": \"Clear description of what this parameter does\"\n        }\n    },\n    \"required\": [\"parameter_name\"]  # List of required parameters\n}\n</code></pre>"},{"location":"getting-started/tools-tutorial/#examples-of-different-parameter-types","title":"Examples of Different Parameter Types:","text":"<pre><code># Simple string parameter\nsimple_tool = LLMTool(\n    name=\"greet_user\",\n    description=\"Greet a user by name\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"description\": \"User's name\"}\n        },\n        \"required\": [\"name\"]\n    },\n    func=lambda name: f\"Hello, {name}!\"\n)\n\n# Multiple parameters with different types\ncomplex_tool = LLMTool(\n    name=\"create_reminder\",\n    description=\"Create a reminder with specific details\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\"type\": \"string\", \"description\": \"Reminder title\"},\n            \"minutes\": {\"type\": \"number\", \"description\": \"Minutes from now\"},\n            \"priority\": {\n                \"type\": \"string\", \n                \"enum\": [\"low\", \"medium\", \"high\"],\n                \"description\": \"Priority level\"\n            },\n            \"urgent\": {\"type\": \"boolean\", \"description\": \"Is this urgent?\"}\n        },\n        \"required\": [\"title\", \"minutes\"]\n    },\n    func=create_reminder_func\n)\n</code></pre>"},{"location":"getting-started/tools-tutorial/#complete-example-multi-tool-agent","title":"Complete Example: Multi-Tool Agent","text":"<p>Here's a complete example combining all tool types:</p> <pre><code>import spade\nimport getpass\nimport logging\nfrom datetime import datetime\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider, LLMTool\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Tool memory for data persistence\ntool_memory = {}\n\n# Basic utility tools\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef calculate_math(expression: str) -&gt; str:\n    \"\"\"Safely evaluate mathematical expressions.\"\"\"\n    try:\n        # Safe evaluation with limited built-ins\n        safe_dict = {\"__builtins__\": {}}\n        safe_dict.update({name: getattr(__builtins__, name, None) \n                         for name in ['abs', 'round', 'min', 'max', 'sum', 'len']})\n        result = eval(expression, safe_dict)\n        return str(result)\n    except Exception as e:\n        return f\"Math error: {str(e)}\"\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather information for major cities.\"\"\"\n    weather_data = {\n        \"madrid\": \"22\u00b0C, sunny with light clouds\",\n        \"london\": \"15\u00b0C, cloudy with occasional rain\",\n        \"new york\": \"18\u00b0C, rainy with strong winds\",\n        \"tokyo\": \"25\u00b0C, clear skies\",\n        \"paris\": \"19\u00b0C, partly cloudy\",\n        \"barcelona\": \"24\u00b0C, sunny and warm\",\n        \"berlin\": \"16\u00b0C, overcast\",\n        \"rome\": \"26\u00b0C, sunny\"\n    }\n    return weather_data.get(city.lower(), f\"Weather data not available for {city}\")\n\n# Memory tools for data persistence\ndef store_note(title: str, content: str) -&gt; str:\n    \"\"\"Store a note in memory.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    tool_memory[title] = {\n        \"content\": content,\n        \"timestamp\": timestamp\n    }\n    return f\"Note '{title}' stored successfully\"\n\ndef retrieve_note(title: str) -&gt; str:\n    \"\"\"Retrieve a previously stored note.\"\"\"\n    if title in tool_memory:\n        note = tool_memory[title]\n        return f\"Note '{title}' (stored: {note['timestamp']}):\\n{note['content']}\"\n    else:\n        return f\"No note found with title '{title}'\"\n\ndef list_notes() -&gt; str:\n    \"\"\"List all stored notes.\"\"\"\n    if tool_memory:\n        notes = [f\"'{title}' (stored: {note['timestamp']})\" \n                for title, note in tool_memory.items()]\n        return f\"Stored notes:\\n\" + \"\\n\".join(notes)\n    else:\n        return \"No notes stored\"\n\n# Create comprehensive tool set\ndef create_tools():\n    \"\"\"Create a comprehensive set of tools.\"\"\"\n    return [\n        # Utility tools\n        LLMTool(\n            name=\"get_current_time\",\n            description=\"Get the current date and time\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": []\n            },\n            func=get_current_time\n        ),\n        LLMTool(\n            name=\"calculate_math\",\n            description=\"Perform mathematical calculations safely\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\n                        \"type\": \"string\",\n                        \"description\": \"Mathematical expression (e.g., '2 + 3 * 4', 'round(15.7)')\"\n                    }\n                },\n                \"required\": [\"expression\"]\n            },\n            func=calculate_math\n        ),\n        LLMTool(\n            name=\"get_weather\",\n            description=\"Get weather information for major cities\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"City name (Madrid, London, New York, Tokyo, Paris, Barcelona, Berlin, Rome)\"\n                    }\n                },\n                \"required\": [\"city\"]\n            },\n            func=get_weather\n        ),\n\n        # Memory tools\n        LLMTool(\n            name=\"store_note\",\n            description=\"Store a note in memory for later retrieval\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"title\": {\n                        \"type\": \"string\",\n                        \"description\": \"Title of the note\"\n                    },\n                    \"content\": {\n                        \"type\": \"string\",\n                        \"description\": \"Content of the note\"\n                    }\n                },\n                \"required\": [\"title\", \"content\"]\n            },\n            func=store_note\n        ),\n        LLMTool(\n            name=\"retrieve_note\",\n            description=\"Retrieve a previously stored note\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"title\": {\n                        \"type\": \"string\",\n                        \"description\": \"Title of the note to retrieve\"\n                    }\n                },\n                \"required\": [\"title\"]\n            },\n            func=retrieve_note\n        ),\n        LLMTool(\n            name=\"list_notes\",\n            description=\"List all stored notes\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": []\n            },\n            func=list_notes\n        )\n    ]\n\nasync def main():\n    print(\"\ud83d\udd27 Complete Multi-Tool Agent Example\")\n\n    # Configuration\n    xmpp_server = input(\"XMPP server domain (default: localhost): \") or \"localhost\"\n\n    # Choose provider\n    provider_type = input(\"Provider (openai/ollama): \").lower()\n\n    if provider_type == \"openai\":\n        provider = LLMProvider.create_openai(\n            api_key=getpass.getpass(\"OpenAI API key: \"),\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n    else:  # ollama\n        model = input(\"Ollama model (default: qwen2.5:7b): \") or \"qwen2.5:7b\"\n        provider = LLMProvider.create_ollama(\n            model=model,\n            base_url=\"http://localhost:11434/v1\",\n            temperature=0.7\n        )\n\n    # Create tools\n    tools = create_tools()\n\n    # Create multi-tool agent\n    llm_agent = LLMAgent(\n        jid=f\"multi_tool_agent@{xmpp_server}\",\n        password=getpass.getpass(\"LLM agent password: \"),\n        provider=provider,\n        system_prompt=\"\"\"You are a helpful assistant with access to multiple tools:\n\nUTILITY TOOLS:\n- get_current_time: Get current date and time\n- calculate_math: Perform mathematical calculations  \n- get_weather: Get weather for major cities\n\nMEMORY TOOLS:\n- store_note: Store information for later retrieval\n- retrieve_note: Get previously stored notes\n- list_notes: See all stored notes\n\nUse these tools appropriately to help users. When using tools, explain what you're doing and why.\"\"\",\n        tools=tools\n    )\n\n    # Create chat interface\n    def display_response(message: str, sender: str):\n        print(f\"\\n\ud83e\udd16 Multi-Tool Agent: {message}\")\n        print(\"-\" * 60)\n\n    chat_agent = ChatAgent(\n        jid=f\"user@{xmpp_server}\",\n        password=getpass.getpass(\"Chat agent password: \"),\n        target_agent_jid=f\"multi_tool_agent@{xmpp_server}\",\n        display_callback=display_response\n    )\n\n    try:\n        # Start agents\n        await llm_agent.start()\n        await chat_agent.start()\n\n        print(\"\u2705 Multi-Tool Agent started!\")\n        print(\"\ud83d\udd27 Available tools:\")\n        print(\"  Utility: time, math, weather\")\n        print(\"  Memory: store/retrieve/list notes\")\n        print(\"\\n\ud83d\udca1 Example queries:\")\n        print(\"\u2022 'What time is it and what's the weather in Madrid?'\")\n        print(\"\u2022 'Calculate 15 * 8 + 32 and store the result as a note'\")\n        print(\"\u2022 'Store a note about my meeting tomorrow'\")\n        print(\"\u2022 'Show me all my notes'\")\n        print(\"Type 'exit' to quit\\n\")\n\n        # Run interactive chat\n        await chat_agent.run_interactive()\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Shutting down...\")\n    finally:\n        await chat_agent.stop()\n        await llm_agent.stop()\n        print(\"\u2705 Multi-Tool Agent stopped!\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/tools-tutorial/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/tools-tutorial/#1-tool-design","title":"1. Tool Design","text":"<ul> <li>Keep tools focused on single responsibilities</li> <li>Use clear, descriptive names and documentation</li> <li>Implement comprehensive error handling</li> <li>Validate inputs and sanitize outputs</li> </ul>"},{"location":"getting-started/tools-tutorial/#2-schema-definition","title":"2. Schema Definition","text":"<ul> <li>Provide detailed parameter descriptions</li> <li>Use appropriate data types</li> <li>Mark required parameters clearly</li> <li>Include examples in descriptions</li> </ul>"},{"location":"getting-started/tools-tutorial/#next-steps","title":"Next Steps","text":"<p>Now that you understand custom tools, explore:</p> <ol> <li>Advanced Agent Tutorial - Multi-agent workflows with tools</li> <li>MCP Integration Guide - External service integration</li> <li>API Reference - Complete tools documentation</li> </ol> <p>Custom tools are powerful building blocks that extend your agents' capabilities beyond text generation, enabling them to interact with the world and perform real tasks.</p>"},{"location":"guides/","title":"Guides","text":"<p>Comprehensive guides for SPADE_LLM features and concepts.</p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li>Architecture - System components and design</li> <li>Providers - LLM provider configuration and usage</li> <li>Tools System - Function calling and tool integration</li> <li>Context Management - Advanced context control and message management</li> <li>Memory System - Dual memory architecture for agent learning and conversation continuity</li> <li>Memory Architecture - Detailed memory system architecture and diagrams</li> <li>Coordinator Agent - Organizational orchestration with shared context and routing</li> <li>Conversations - Conversation lifecycle and management</li> <li>MCP - Model context protocol integration</li> <li>Human-in-the-Loop - Human expert consultation and workflows</li> <li>Guardrails - Content filtering and safety controls</li> <li>Routing - Message routing and multi-agent workflows</li> </ul>"},{"location":"guides/#usage-patterns","title":"Usage Patterns","text":"<p>Each guide covers: - Core concepts and configuration - Common usage patterns - Best practices and troubleshooting - Complete code examples</p>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Detailed API documentation</li> <li>Examples - Working code examples</li> </ul>"},{"location":"guides/architecture/","title":"Architecture","text":"<p>SPADE_LLM extends SPADE's multi-agent framework with LLM capabilities while maintaining full compatibility.</p>"},{"location":"guides/architecture/#component-overview","title":"Component Overview","text":"<pre><code>graph TB\n    A[LLMAgent] --&gt; B[LLMBehaviour]\n    B --&gt; C[ContextManager]\n    B --&gt; D[LLMProvider]\n    B --&gt; E[LLMTool]\n    B --&gt; I[Guardrails System]\n\n    D --&gt; F[OpenAI/Ollama/etc]\n    E --&gt; G[Python Functions]\n    E --&gt; H[MCP Servers]\n    I --&gt; J[Input Filters]\n    I --&gt; K[Output Filters]</code></pre>"},{"location":"guides/architecture/#core-components","title":"\ud83c\udfd7\ufe0f Core Components","text":""},{"location":"guides/architecture/#llmagent","title":"\ud83e\udd16 LLMAgent","text":"<p>The main agent class that extends SPADE's <code>Agent</code> with LLM capabilities:</p> <ul> <li>Manages LLM provider connection and configuration</li> <li>Registers tools and handles their lifecycle</li> <li>Controls conversation limits and termination conditions</li> <li>Provides the bridge between SPADE's XMPP messaging and LLM processing</li> </ul>"},{"location":"guides/architecture/#llmbehaviour","title":"\u26a1 LLMBehaviour","text":"<p>The core processing engine that orchestrates the entire LLM workflow:</p> <ol> <li>Receives XMPP messages from other agents</li> <li>Updates conversation context with new information</li> <li>Calls LLM provider for responses</li> <li>Executes tools when requested by the LLM</li> <li>Routes responses to appropriate recipients</li> </ol> <p>This is where the main processing occurs - transforming simple messages into interactions.</p>"},{"location":"guides/architecture/#contextmanager","title":"\ud83e\udde0 ContextManager","text":"<p>Manages conversation state across multiple concurrent discussions:</p> <ul> <li>Tracks multiple conversations simultaneously by thread ID</li> <li>Formats messages appropriately for different LLM providers</li> <li>Handles context windowing to manage token limits efficiently</li> <li>Ensures each conversation maintains its own context and history</li> </ul>"},{"location":"guides/architecture/#llmprovider","title":"\ud83d\udd0c LLMProvider","text":"<p>Unified interface for different LLM services, providing consistency:</p> <ul> <li>Abstracts provider-specific APIs (OpenAI, Ollama, Anthropic, etc.)</li> <li>Handles tool calling formats across different providers</li> <li>Provides consistent error handling and retry mechanisms</li> <li>Makes it easy to switch between different LLM services</li> </ul>"},{"location":"guides/architecture/#llmtool","title":"\ud83d\udee0\ufe0f LLMTool","text":"<p>Framework for executable functions that extend LLM capabilities:</p> <ul> <li>Async execution support for non-blocking operations</li> <li>JSON Schema parameter validation for type safety</li> <li>Integration with LangChain and MCP for ecosystem compatibility</li> <li>Enables LLMs to perform real actions beyond conversation</li> </ul>"},{"location":"guides/architecture/#message-flow","title":"\ud83d\udce8 Message Flow","text":"<pre><code>sequenceDiagram\n    participant A as External Agent\n    participant B as LLMBehaviour\n    participant C as LLMProvider\n    participant D as LLM Service\n    participant E as LLMTool\n\n    A-&gt;&gt;B: XMPP Message\n    B-&gt;&gt;C: Get Response\n    C-&gt;&gt;D: API Call\n    D-&gt;&gt;C: Tool Calls\n    C-&gt;&gt;B: Tool Requests\n    loop For Each Tool\n        B-&gt;&gt;E: Execute\n        E-&gt;&gt;B: Result\n    end\n    B-&gt;&gt;C: Get Final Response\n    C-&gt;&gt;D: API Call\n    D-&gt;&gt;C: Final Response\n    B-&gt;&gt;A: Response Message</code></pre>"},{"location":"guides/architecture/#conversation-lifecycle","title":"\ud83d\udd04 Conversation Lifecycle","text":"<p>The conversation lifecycle follows a well-defined process:</p> <ol> <li>Initialization: New conversation created from message thread</li> <li>Processing: Messages processed through LLM with tool execution</li> <li>Termination: Ends via markers, limits, or manual control</li> <li>Cleanup: Resources freed and callbacks executed</li> </ol> <p>Each stage ensures conversations can handle complex, multi-turn interactions while maintaining system stability.</p>"},{"location":"guides/architecture/#integration-points","title":"\ud83d\udd27 Integration Points","text":"<p>The architecture provides multiple integration points for customization:</p> <ul> <li>Custom Providers: Add new LLM services</li> <li>Tool Extensions: Create domain-specific tools</li> <li>Routing Logic: Implement custom message routing</li> <li>Context Management: Customize conversation handling</li> <li>MCP Integration: Connect to external servers</li> </ul> <p>This flexible design ensures SPADE_LLM can adapt to various use cases while maintaining its core multi-agent capabilities.</p>"},{"location":"guides/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Providers - Configure LLM providers</li> <li>Tools System - Add tool capabilities</li> <li>Routing - Implement message routing</li> <li>MCP - Connect to external services</li> </ul>"},{"location":"guides/context-management/","title":"Context Management","text":"<p>SPADE_LLM provides context management to control conversation memory and optimize LLM performance across multi-turn interactions.</p>"},{"location":"guides/context-management/#overview","title":"Overview","text":"<p>The context management system handles how conversation history is maintained and filtered:</p> <ul> <li>Memory Control: Prevents token overflow while preserving important information</li> <li>Multi-Strategy Support: Choose from basic, windowed, or extended strategies</li> <li>Tool-Aware: Maintains critical tool execution context</li> <li>Conversation Isolation: Each conversation maintains separate context</li> </ul>"},{"location":"guides/context-management/#context-strategies","title":"Context Strategies","text":""},{"location":"guides/context-management/#nocontextmanagement-default","title":"NoContextManagement (Default)","text":"<p>Behavior: Preserves all messages without any filtering or limitations.</p> <pre><code>from spade_llm.context import NoContextManagement\n\n# Keep all conversation history\ncontext = NoContextManagement()\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=context\n)\n</code></pre> <p>Characteristics: - \u2705 Preserves complete conversation history - \u2705 No context loss - \u274c Unlimited memory growth - \u274c Potential LLM context overflow</p> <p>Use Cases: - Short conversations (&lt; 10 exchanges) - Debugging sessions requiring complete history - Post-conversation analysis</p>"},{"location":"guides/context-management/#windowsizecontext-basic","title":"WindowSizeContext (Basic)","text":"<p>Behavior: Implements a sliding window maintaining only the last N messages.</p> <pre><code>from spade_llm.context import WindowSizeContext\n\n# Keep last 20 messages\ncontext = WindowSizeContext(max_messages=20)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=context\n)\n</code></pre> <p>Visual Example:</p> <p></p> <p>Basic sliding window keeps only the most recent N messages, dropping older ones as new messages arrive.</p> <p>Characteristics: - \u2705 Predictable memory control - \u2705 Prevents context overflow - \u274c Loses important initial context - \u274c No message type differentiation</p> <p>Use Cases: - Long conversations with memory constraints - Resource-limited environments - Continuous monitoring sessions</p>"},{"location":"guides/context-management/#smartwindowsizecontext-advanced","title":"SmartWindowSizeContext (Advanced) \ud83c\udd95","text":"<p>Behavior: Management combining sliding window with selective retention of critical messages.</p>"},{"location":"guides/context-management/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from spade_llm.context import SmartWindowSizeContext\n\n# Standard behavior (equivalent to WindowSizeContext)\nbasic_context = SmartWindowSizeContext(max_messages=20)\n\n# With initial message preservation\ninitial_preserve = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3\n)\n\n# With tool prioritization\ntool_priority = SmartWindowSizeContext(\n    max_messages=20,\n    prioritize_tools=True\n)\n\n# Full configuration\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n</code></pre>"},{"location":"guides/context-management/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>max_messages</code> <code>int</code> 20 Maximum messages in context <code>preserve_initial</code> <code>int</code> 0 Number of initial messages to always preserve <code>prioritize_tools</code> <code>bool</code> False Whether to prioritize tool results"},{"location":"guides/context-management/#retention-algorithm","title":"Retention Algorithm","text":"<p>The SmartWindowSizeContext uses an algorithm:</p> <ol> <li>If <code>total_messages \u2264 max_messages</code> \u2192 Return all messages</li> <li>If <code>preserve_initial = 0</code> and <code>prioritize_tools = False</code> \u2192 Basic sliding window</li> <li>If <code>preserve_initial &gt; 0</code> \u2192 Preserve initial messages + fill with recent ones</li> <li>If <code>prioritize_tools = True</code> \u2192 Prioritize tool results + fill remaining space</li> <li>If both enabled \u2192 Combine preservation + prioritization strategies</li> </ol>"},{"location":"guides/context-management/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/context-management/#initial-message-preservation","title":"Initial Message Preservation","text":"<pre><code># Example: 30 total messages, window=10, preserve_initial=3\n# Result: [msg1, msg2, msg3] + [msg24, msg25, ..., msg30]\n\ncontext = SmartWindowSizeContext(\n    max_messages=10, \n    preserve_initial=3\n)\n</code></pre> <p>Visual Example:</p> <p></p> <p>Smart window preserves the first N messages (objectives, instructions) while filling remaining space with recent messages.</p> <p>Benefits: Preserves conversation objectives and fundamental context.</p>"},{"location":"guides/context-management/#tool-result-prioritization","title":"Tool Result Prioritization","text":"<pre><code># Prioritizes all messages with role=\"tool\"\ncontext = SmartWindowSizeContext(\n    max_messages=15, \n    prioritize_tools=True\n)\n</code></pre> <p>Algorithm: 1. Extract all tool result messages 2. If tool messages \u2265 max_messages \u2192 Keep recent tool messages 3. Otherwise \u2192 tool messages + recent messages to fill window 4. Reorder chronologically</p>"},{"location":"guides/context-management/#tool-callresult-pair-preservation","title":"Tool Call/Result Pair Preservation","text":"<p>The system automatically detects and preserves tool call/result pairs:</p> <pre><code># Automatically preserves:\n# Assistant: \"I'll check the weather\" [tool_calls: get_weather]\n# Tool: \"22\u00b0C, sunny\" [tool_call_id: matching_id]\n\ncontext = SmartWindowSizeContext(\n    max_messages=20,\n    prioritize_tools=True\n)\n</code></pre> <p>Visual Example:</p> <p></p> <p>Smart window with initial preservation + tool prioritization maintains both conversation objectives and critical tool execution context.</p> <p>Benefits: Maintains execution context for complex tool workflows.</p>"},{"location":"guides/context-management/#integration-with-llmagent","title":"Integration with LLMAgent","text":""},{"location":"guides/context-management/#constructor-configuration","title":"Constructor Configuration","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.context import SmartWindowSizeContext\n\n# Create context strategy\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Integrate with agent\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=smart_context,\n    system_prompt=\"You are an assistant with context management...\"\n)\n</code></pre>"},{"location":"guides/context-management/#context-statistics","title":"Context Statistics","text":""},{"location":"guides/context-management/#getting-statistics","title":"Getting Statistics","text":"<pre><code>context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Get stats for current conversation\nstats = context.get_stats(total_messages=50)\n</code></pre>"},{"location":"guides/context-management/#statistics-format","title":"Statistics Format","text":"<pre><code>{\n    \"strategy\": \"smart_window_size\",\n    \"max_messages\": 20,\n    \"preserve_initial\": 3,\n    \"prioritize_tools\": True,\n    \"total_messages\": 50,\n    \"messages_in_context\": 20,\n    \"messages_dropped\": 30\n}\n</code></pre>"},{"location":"guides/context-management/#strategy-comparison","title":"Strategy Comparison","text":"Feature NoContext WindowSize SmartWindowSize Memory Control \u274c \u2705 \u2705 Preserves Initial Context \u2705 \u274c \u2705 (optional) Tool Prioritization \u2705 \u274c \u2705 (optional) Configuration Complexity None Low Medium Performance O(1) O(1) O(n log n) Memory Usage Unlimited Limited Limited"},{"location":"guides/context-management/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom spade_llm.agent import LLMAgent\nfrom spade_llm.context import SmartWindowSizeContext\nfrom spade_llm.providers import LLMProvider\n\nasync def main():\n    # Configure context management\n    smart_context = SmartWindowSizeContext(\n        max_messages=20,\n        preserve_initial=3,\n        prioritize_tools=True\n    )\n\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Create agent with context management\n    agent = LLMAgent(\n        jid=\"smart_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        context_management=smart_context,\n        system_prompt=\"You are an assistant with context management.\"\n    )\n\n    await agent.start()\n\n    # Monitor context during conversation\n    while True:\n        # ... agent processes messages ...\n\n        # Check context stats periodically\n        stats = agent.get_context_stats()\n        if stats['messages_in_context'] &gt; 15:\n            print(\"Context approaching limit\")\n\n        await asyncio.sleep(1)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/context-management/#next-steps","title":"Next Steps","text":"<ul> <li>Conversations - Learn about conversation lifecycle</li> <li>Memory System - Explore agent memory capabilities</li> <li>Tools System - Add tool capabilities</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"guides/conversations/","title":"Conversation Management","text":"<p>SPADE_LLM automatically manages conversation context across multi-turn interactions, enabling stateful dialogues between agents.</p>"},{"location":"guides/conversations/#conversation-flow","title":"Conversation Flow","text":"<pre><code>graph TD\n    A[New Message Arrives] --&gt; B{Conversation Exists?}\n    B --&gt;|No| C[Create New Conversation]\n    B --&gt;|Yes| D[Load Existing Context]\n    C --&gt; E[Initialize Context with System Prompt]\n    D --&gt; F[Add Message to Context]\n    E --&gt; F\n    F --&gt; G[Check Interaction Count]\n    G --&gt; H{Max Interactions Reached?}\n    H --&gt;|Yes| I[Mark for Termination]\n    H --&gt;|No| J[Process with LLM]\n    J --&gt; K[Check Response for Termination Markers]\n    K --&gt; L{Termination Marker Found?}\n    L --&gt;|Yes| M[End Conversation]\n    L --&gt;|No| N[Send Response]\n    I --&gt; M\n    M --&gt; O[Execute Cleanup Callbacks]\n    N --&gt; P[Update Context]\n    P --&gt; Q[Continue Conversation]</code></pre>"},{"location":"guides/conversations/#overview","title":"Overview","text":"<p>The conversation system provides comprehensive management for multi-agent interactions:</p> <ul> <li>Multi-turn Context: Maintains complete conversation history across interactions</li> <li>Concurrent Conversations: Supports multiple simultaneous conversations per agent  </li> <li>Automatic Lifecycle: Manages memory and conversation cleanup efficiently</li> <li>Flexible Termination: Controls interaction limits and ending conditions</li> </ul>"},{"location":"guides/conversations/#basic-configuration","title":"Basic Configuration","text":"<p>Conversation management is automatic. Each message thread creates a separate conversation with configurable parameters:</p> <pre><code>from spade_llm import LLMAgent, LLMProvider\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10,\n    termination_markers=[\"&lt;DONE&gt;\", \"&lt;END&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#conversation-lifecycle","title":"Conversation Lifecycle","text":"<p>The conversation lifecycle follows a structured process:</p> <ol> <li>Initialization: New conversation created from message thread ID</li> <li>Processing: Messages added to context, LLM generates responses</li> <li>Monitoring: System tracks interactions and checks termination conditions</li> <li>Termination: Conversation ends through various mechanisms</li> <li>Cleanup: Resources freed and callbacks executed</li> </ol>"},{"location":"guides/conversations/#lifecycle-states","title":"Lifecycle States","text":"<ul> <li>Active: Conversation is processing messages normally</li> <li>Approaching Limit: Near maximum interaction count</li> <li>Terminated: Conversation has ended and is being cleaned up</li> <li>Cleanup Complete: All resources have been freed</li> </ul>"},{"location":"guides/conversations/#termination-markers","title":"Termination Markers","text":"<p>Termination markers provide a way for LLMs to explicitly signal when a conversation should end. This enables intelligent conversation closure based on content rather than just interaction counts.</p>"},{"location":"guides/conversations/#how-termination-markers-work","title":"How Termination Markers Work","text":"<p>When the LLM includes a termination marker in its response:</p> <ol> <li>Detection: The system scans the LLM response for configured markers</li> <li>Immediate Termination: Conversation is marked for closure regardless of interaction count</li> <li>Response Processing: The marker is typically removed from the final response sent to users</li> <li>Cleanup Execution: Standard termination procedures are triggered</li> </ol>"},{"location":"guides/conversations/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Single termination marker\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    termination_markers=[\"&lt;CONVERSATION_END&gt;\"]\n)\n\n# Multiple termination markers\nagent = LLMAgent(\n    jid=\"assistant@example.com\", \n    password=\"password\",\n    provider=provider,\n    termination_markers=[\"&lt;DONE&gt;\", \"&lt;END&gt;\", \"&lt;GOODBYE&gt;\", \"&lt;TERMINATE&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#llm-integration","title":"LLM Integration","text":"<p>Train your LLM to use termination markers appropriately:</p> <pre><code>system_prompt = \"\"\"\nYou are a helpful assistant. When a conversation naturally comes to an end \nor the user says goodbye, include &lt;DONE&gt; at the end of your response to \nproperly close the conversation.\n\nExample:\nUser: \"Thanks for your help, goodbye!\"\nAssistant: \"You're welcome! Have a great day! &lt;DONE&gt;\"\n\"\"\"\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=system_prompt,\n    termination_markers=[\"&lt;DONE&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#termination-marker-best-practices","title":"Termination Marker Best Practices","text":"<ul> <li>Choose Unique Markers: Use markers unlikely to appear in normal conversation</li> <li>Document for LLMs: Clearly instruct the LLM when and how to use markers</li> <li>Multiple Options: Provide several markers for different termination scenarios</li> <li>Consistent Format: Use consistent marker syntax across your system</li> </ul>"},{"location":"guides/conversations/#context-management","title":"Context Management","text":"<p>SPADE_LLM provides context management to control conversation memory and optimize performance. See the Context Management Guide for detailed information.</p>"},{"location":"guides/conversations/#accessing-conversation-state","title":"Accessing Conversation State","text":"<p>Programmatic access to conversation information:</p> <pre><code># Get conversation state\nconversation_id = \"user1_session\"\nstate = agent.get_conversation_state(conversation_id)\n\n# Check conversation status\nis_active = state.get(\"active\", False)\ninteraction_count = state.get(\"interaction_count\", 0)\nmax_interactions = state.get(\"max_interactions\", 10)\n\n# Reset conversation (removes limits)\nsuccess = agent.reset_conversation(conversation_id)\n</code></pre>"},{"location":"guides/conversations/#context-strategy-configuration","title":"Context Strategy Configuration","text":"<p>Configure different context management strategies:</p> <pre><code>from spade_llm.context import SmartWindowSizeContext\n\n# Context management\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"coder@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=smart_context\n)\n</code></pre>"},{"location":"guides/conversations/#conversation-control","title":"Conversation Control","text":""},{"location":"guides/conversations/#termination-callbacks","title":"Termination Callbacks","text":"<p>Handle conversation endings with custom logic:</p> <pre><code>def on_conversation_end(conversation_id: str, reason: str):\n    \"\"\"Called when conversation terminates.\"\"\"\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n\n    # Possible reasons:\n    # - \"max_interactions_reached\"\n    # - \"termination_marker_found\" \n    # - \"manual_termination\"\n\n    # Custom cleanup logic\n    save_conversation_log(conversation_id)\n    send_completion_notification(conversation_id)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    max_interactions_per_conversation=5,\n    on_conversation_end=on_conversation_end\n)\n</code></pre>"},{"location":"guides/conversations/#best-practices","title":"Best Practices","text":""},{"location":"guides/conversations/#context-design","title":"Context Design","text":"<ul> <li>Use clear, focused system prompts that explain the agent's role</li> <li>Set appropriate interaction limits based on expected conversation length</li> <li>Implement proper cleanup callbacks for resource management</li> <li>Handle termination gracefully with user-friendly messages</li> </ul>"},{"location":"guides/conversations/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Monitor memory usage for long-running conversations</li> <li>Implement context compression when approaching token limits</li> <li>Use conversation limits to prevent runaway interactions</li> <li>Clean up inactive conversations regularly to free resources</li> </ul>"},{"location":"guides/conversations/#termination-strategy","title":"Termination Strategy","text":"<ul> <li>Train LLMs to recognize natural conversation endings</li> <li>Use multiple termination markers for different scenarios</li> <li>Combine markers with interaction limits for reliable termination</li> <li>Test termination behavior thoroughly in different conversation contexts</li> </ul>"},{"location":"guides/conversations/#next-steps","title":"Next Steps","text":"<ul> <li>Context Management - Context control strategies</li> <li>Memory System - Agent memory and learning capabilities</li> <li>Tools System - Add capabilities to conversations</li> <li>Message Routing - Control conversation flow between agents</li> <li>Architecture - Understanding conversation management internals</li> <li>Providers - Configure LLM providers for conversations</li> </ul>"},{"location":"guides/coordinator-agent/","title":"Coordinator Agent","text":"<p>SPADE_LLM provides the <code>CoordinatorAgent</code> class to orchestrate multiple SPADE entities from a single LLM-driven agent while maintaining a consolidated view of organizational activity.</p>"},{"location":"guides/coordinator-agent/#overview","title":"Overview","text":"<p>The coordinator:</p> <ul> <li>Centralizes context for all subagents through a shared coordination session.</li> <li>Controls routing so internal replies return to the coordinator and external results reach the original requester.</li> <li>Adds coordination tools that allow the LLM to send sequential commands and inspect subagent status.</li> <li>Tracks execution with timeouts and termination markers (<code>&lt;TASK_COMPLETE&gt;</code>, <code>&lt;END&gt;</code>, <code>&lt;DONE&gt;</code>).</li> </ul>"},{"location":"guides/coordinator-agent/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    Coordinator[\"CoordinatorAgent\"]\n    CCM[\"CoordinationContextManager\"]\n    Routing[\"Custom routing logic\"]\n    Tools[\"Coordination tools\"]\n    SubAgents[\"Subagents (LLM or SPADE)\"]\n    Requesters[\"External requesters or other coordinators\"]\n    SendTool[\"send_to_agent\"]\n    ListTool[\"list_subagents\"]\n\n    Coordinator --&gt; CCM\n    Coordinator --&gt; Routing\n    Coordinator --&gt; Tools\n    CCM --&gt;|Shared context| SubAgents\n    Tools --&gt; SendTool\n    Tools --&gt; ListTool\n    Routing --&gt; SubAgents\n    Routing --&gt; Requesters</code></pre> <ul> <li>CoordinationContextManager forces every message involving subagents to use the same thread, giving the LLM full visibility of the organizational state.</li> <li>Custom routing keeps intermediate messages inside the organization and returns the final outcome to the requester.</li> <li>Coordination tools expose actions that the LLM can trigger (send commands, check registry) without leaving the conversation loop.</li> <li>Status tracking maintains the latest response state for each subagent so the LLM can decide the next action.</li> </ul>"},{"location":"guides/coordinator-agent/#shared-coordination-session","title":"Shared Coordination Session","text":"<pre><code>flowchart LR\n    Requester((Requester))\n    Coordinator[\"CoordinatorAgent\"]\n    Session[[\"Coordination session&lt;br/&gt;(shared context)\"]]\n    SubA[\"Subagent A\"]\n    SubB[\"Subagent B\"]\n    OtherCoord[\"Other coordinator\"]\n\n    Requester --&gt;|Task request| Coordinator\n    Coordinator --&gt;|Log command| Session\n    Session --&gt;|Context state| Coordinator\n    Coordinator --&gt;|Instruction| SubA\n    SubA --&gt;|Response| Session\n    Coordinator --&gt;|Follow-up| SubB\n    SubB --&gt;|Response| Session\n    Coordinator --&gt;|Final summary| Requester\n    Coordinator --&gt;|Inter-organization message| OtherCoord\n    Session --&gt;|Shared state| OtherCoord</code></pre> <p>This view highlights how every exchange is recorded in the shared session so the coordinator and any collaborating coordinators can access the complete organizational context.</p>"},{"location":"guides/coordinator-agent/#coordination-flow","title":"Coordination Flow","text":"<pre><code>sequenceDiagram\n    participant R as Requester\n    participant Coord as CoordinatorAgent\n    participant SubA as Subagent A\n    participant SubB as Subagent B\n\n    R-&gt;&gt;Coord: Task request (XMPP)\n    Coord-&gt;&gt;Coord: Append to coordination session\n    Coord-&gt;&gt;SubA: send_to_agent(command)\n    SubA-&gt;&gt;Coord: Response\n    Coord-&gt;&gt;Coord: Update context &amp; status\n    Coord-&gt;&gt;SubB: send_to_agent(follow-up)\n    SubB-&gt;&gt;Coord: Response\n    Coord-&gt;&gt;R: Final summary (routing with markers)</code></pre> <ul> <li>Messages between the coordinator and subagents share the same <code>coordination_session</code> thread.</li> <li>Each response is logged in the shared context before the next instruction is issued.</li> <li>When a termination marker appears, the routing function forwards the final message to the original requester.</li> </ul>"},{"location":"guides/coordinator-agent/#built-in-tools","title":"Built-in Tools","text":"Tool Description Notes <code>send_to_agent</code> Sends a command to a registered subagent and waits for the reply within the coordination timeout. Updates the shared context so subsequent steps can reuse the result. <code>list_subagents</code> Lists the registered subagents and their last known status. Useful for planning the next action or diagnosing timeouts. <p>Both tools are registered automatically during <code>setup()</code> and are available to the LLM without extra configuration.</p>"},{"location":"guides/coordinator-agent/#setup-example","title":"Setup Example","text":"<pre><code>from spade_llm.agent import CoordinatorAgent\nfrom spade_llm.providers import LLMProvider\n\nsubagents = [\n    \"traffic-analyzer@xmpp.local\",\n    \"notification-service@xmpp.local\",\n    \"logistics-planner@xmpp.local\",\n]\n\ncoordinator = CoordinatorAgent(\n    jid=\"city-coordinator@xmpp.local\",\n    password=\"secret\",\n    subagent_ids=subagents,\n    provider=LLMProvider.create_openai(\n        api_key=\"sk-...\",\n        model=\"gpt-4o-mini\",\n    ),\n    coordination_session=\"valencia_city_ops\"\n)\nawait coordinator.start()\n</code></pre>"},{"location":"guides/coordinator-agent/#best-practices","title":"Best Practices","text":"<ul> <li>Provide self-contained commands: Subagents only see their individual thread, so include all necessary context in each instruction.</li> <li>Monitor timeouts: <code>send_to_agent</code> returns an error when the configured timeout is reached; update <code>_response_timeout</code> if longer operations are expected.</li> <li>Combine with guardrails/HITL: The coordinator inherits all safety mechanisms from <code>LLMAgent</code>; use them to supervise sensitive exchanges.</li> <li>Persist organizational state: Pair the coordinator with interaction or base memories when long-running collaborations are required.</li> </ul>"},{"location":"guides/guardrails/","title":"Guardrails System","text":"<p>Protect your agents with configurable content filtering and safety controls.</p>"},{"location":"guides/guardrails/#content-flow","title":"Content Flow","text":"<pre><code>flowchart TD\n    A[Input Message] --&gt; B[Input Guardrails]\n    B --&gt;|Pass| C[LLM Processing]  \n    B --&gt;|Block| D[Send Block Response]\n    B --&gt;|Modify| E[Process Modified Content]\n    E --&gt; C\n    C --&gt; F[LLM Response]\n    F --&gt; G[Output Guardrails]\n    G --&gt;|Pass| H[Send Response]\n    G --&gt;|Block| I[Send Safe Response]\n    G --&gt;|Modify| J[Send Modified Response]</code></pre>"},{"location":"guides/guardrails/#overview","title":"Overview","text":"<p>The Guardrails System provides multi-layer content protection for your LLM agents. It enables you to:</p> <ul> <li>\ud83d\udee1\ufe0f Filter harmful content before it reaches the LLM</li> <li>\ud83d\udd0d Validate LLM responses before sending to users</li> <li>\u270f\ufe0f Automatically modify inappropriate content</li> <li>\ud83d\udcca Monitor and log security events</li> <li>\ud83d\udd17 Chain multiple filters in sequence</li> </ul>"},{"location":"guides/guardrails/#how-guardrails-work","title":"How Guardrails Work","text":"<p>Guardrails operate at two critical points in the agent workflow:</p> <ol> <li>Input Guardrails: Process incoming messages before LLM processing</li> <li>Output Guardrails: Validate LLM responses before sending to users</li> </ol> <p>Each guardrail can take one of four actions:</p> <ul> <li>PASS: Allow content without changes</li> <li>MODIFY: Transform content and continue processing</li> <li>BLOCK: Stop processing and send rejection message</li> <li>WARNING: Log concern but allow content</li> </ul>"},{"location":"guides/guardrails/#basic-usage","title":"Basic Usage","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\nfrom spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Create content filter\nsafety_filter = KeywordGuardrail(\n    name=\"safety_filter\",\n    blocked_keywords=[\"hack\", \"exploit\", \"malware\"],\n    action=GuardrailAction.BLOCK,\n    blocked_message=\"I cannot help with potentially harmful activities.\"\n)\n\n# Apply to agent\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=[safety_filter]\n)\n</code></pre>"},{"location":"guides/guardrails/#built-in-guardrails","title":"Built-in Guardrails","text":""},{"location":"guides/guardrails/#keywordguardrail","title":"\ud83d\udd24 KeywordGuardrail","text":"<p>Block or modify content containing specific keywords.</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Block harmful keywords\nblock_filter = KeywordGuardrail(\n    name=\"harmful_content\",\n    blocked_keywords=[\"bomb\", \"hack\", \"exploit\"],\n    action=GuardrailAction.BLOCK,\n    case_sensitive=False\n)\n\n# Replace profanity\nprofanity_filter = KeywordGuardrail(\n    name=\"profanity_filter\",\n    blocked_keywords=[\"damn\", \"hell\", \"stupid\"],\n    action=GuardrailAction.MODIFY,\n    replacement=\"[FILTERED]\",\n    case_sensitive=False\n)\n</code></pre>"},{"location":"guides/guardrails/#regexguardrail","title":"\ud83d\udd0d RegexGuardrail","text":"<p>Apply regex patterns for sophisticated content detection.</p> <pre><code>from spade_llm.guardrails import RegexGuardrail, GuardrailAction\n\n# Redact email addresses\nemail_filter = RegexGuardrail(\n    name=\"email_redactor\",\n    patterns={\n        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b': '[EMAIL]',\n        r'\\b\\d{3}-\\d{2}-\\d{4}\\b': '[SSN]'  # Social Security Numbers\n    }\n)\n\n# Block credit card patterns\ncc_filter = RegexGuardrail(\n    name=\"credit_card_blocker\",\n    patterns={\n        r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b': GuardrailAction.BLOCK\n    },\n    blocked_message=\"Credit card information is not allowed.\"\n)\n</code></pre>"},{"location":"guides/guardrails/#llmguardrail","title":"\ud83e\udd16 LLMGuardrail","text":"<p>Use a smaller LLM model to validate content safety.</p> <pre><code>from spade_llm.guardrails import LLMGuardrail\nfrom spade_llm.providers import LLMProvider\n\n# Create safety validation model\nsafety_provider = LLMProvider.create_openai(\n    api_key=\"your-key\",\n    model=\"gpt-3.5-turbo\",\n    temperature=0.1\n)\n\nsafety_checker = LLMGuardrail(\n    name=\"ai_safety_validator\",\n    provider=safety_provider,\n    safety_prompt=\"\"\"\n    Analyze this text for harmful content including violence, harassment, \n    illegal activities, or inappropriate requests.\n\n    Respond with JSON: {\"safe\": true/false, \"reason\": \"explanation if unsafe\"}\n\n    Text: {content}\n    \"\"\",\n    blocked_message=\"This content was flagged by our safety system.\"\n)\n</code></pre>"},{"location":"guides/guardrails/#customfunctionguardrail","title":"\u2699\ufe0f CustomFunctionGuardrail","text":"<p>Create custom validation logic with your own functions.</p> <pre><code>from spade_llm.guardrails import CustomFunctionGuardrail, GuardrailResult, GuardrailAction\n\ndef business_hours_check(content: str, context: dict) -&gt; GuardrailResult:\n    \"\"\"Only allow certain requests during business hours.\"\"\"\n    from datetime import datetime\n\n    current_hour = datetime.now().hour\n\n    if \"urgent\" in content.lower() and not (9 &lt;= current_hour &lt;= 17):\n        return GuardrailResult(\n            action=GuardrailAction.MODIFY,\n            content=content + \" [Note: Non-business hours - response may be delayed]\",\n            reason=\"Added business hours notice\"\n        )\n\n    return GuardrailResult(action=GuardrailAction.PASS, content=content)\n\nhours_filter = CustomFunctionGuardrail(\n    name=\"business_hours\",\n    check_function=business_hours_check\n)\n</code></pre>"},{"location":"guides/guardrails/#input-vs-output-guardrails","title":"Input vs Output Guardrails","text":""},{"location":"guides/guardrails/#input-guardrails","title":"Input Guardrails","text":"<p>Applied to incoming messages before LLM processing.</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, RegexGuardrail\n\ninput_filters = [\n    KeywordGuardrail(\"safety\", [\"hack\", \"exploit\"], GuardrailAction.BLOCK),\n    RegexGuardrail(\"pii\", {r'\\b\\d{3}-\\d{2}-\\d{4}\\b': '[SSN]'})\n]\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=input_filters  # Process incoming messages\n)\n</code></pre>"},{"location":"guides/guardrails/#output-guardrails","title":"Output Guardrails","text":"<p>Applied to LLM responses before sending to users.</p> <pre><code>output_filters = [\n    LLMGuardrail(\"safety_check\", safety_provider),\n    KeywordGuardrail(\"sensitive_info\", [\"password\", \"token\"], GuardrailAction.BLOCK)\n]\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\", \n    password=\"password\",\n    provider=provider,\n    output_guardrails=output_filters  # Validate LLM responses\n)\n</code></pre>"},{"location":"guides/guardrails/#composite-guardrails","title":"Composite Guardrails","text":"<p>Chain multiple guardrails together for sophisticated filtering pipelines.</p> <pre><code>from spade_llm.guardrails import CompositeGuardrail\n\n# Create filtering pipeline\ncontent_pipeline = CompositeGuardrail(\n    name=\"content_security_pipeline\",\n    guardrails=[\n        KeywordGuardrail(\"profanity\", [\"damn\", \"hell\"], GuardrailAction.MODIFY, \"[CENSORED]\"),\n        RegexGuardrail(\"emails\", {r'[\\w\\.-]+@[\\w\\.-]+': '[EMAIL]'}),\n        LLMGuardrail(\"safety\", safety_provider)\n    ],\n    stop_on_block=True  # Stop at first block\n)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    input_guardrails=[content_pipeline]\n)\n</code></pre>"},{"location":"guides/guardrails/#dynamic-control","title":"Dynamic Control","text":"<p>Control guardrails at runtime for different scenarios.</p> <pre><code># Create guardrail\nsafety_filter = KeywordGuardrail(\"safety\", [\"hack\"], GuardrailAction.BLOCK)\n\n# Add to agent\nagent.add_input_guardrail(safety_filter)\n\n# Control at runtime\nif debug_mode:\n    safety_filter.enabled = False  # Disable for testing\n\nif high_security_mode:\n    safety_filter.enabled = True   # Enable for production\n</code></pre>"},{"location":"guides/guardrails/#development-vs-production","title":"Development vs Production","text":"<pre><code># Development: Relaxed filtering\ndev_guardrails = [\n    KeywordGuardrail(\"basic\", [\"exploit\"], GuardrailAction.WARNING)\n]\n\n# Production: Strict filtering  \nprod_guardrails = [\n    KeywordGuardrail(\"security\", [\"hack\", \"exploit\", \"malware\"], GuardrailAction.BLOCK),\n    LLMGuardrail(\"ai_safety\", safety_provider),\n    RegexGuardrail(\"pii\", pii_patterns)\n]\n\nguardrails = prod_guardrails if ENVIRONMENT == \"production\" else dev_guardrails\n</code></pre>"},{"location":"guides/guardrails/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Tools System - Function calling capabilities  </li> <li>Architecture - Understanding system design</li> <li>Examples - Working code examples</li> </ul>"},{"location":"guides/human-in-the-loop/","title":"Human-in-the-Loop","text":"<p>Enable LLM agents to consult with human experts during their reasoning process.</p>"},{"location":"guides/human-in-the-loop/#overview","title":"Overview","text":"<p>The Human-in-the-Loop system allows LLM agents to ask questions to human experts when they need:</p> <ul> <li>\ud83e\udde0 Human judgment or subjective opinions</li> <li>\ud83d\udcca Real-time information not in their training data  </li> <li>\ud83c\udfe2 Company-specific knowledge or proprietary information</li> <li>\u2753 Clarification on ambiguous requests</li> <li>\u2705 Verification of important decisions</li> </ul> <p>The system maintains the conversational flow while seamlessly integrating human expertise.</p>"},{"location":"guides/human-in-the-loop/#how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant A as LLM Agent  \n    participant H as Human Expert\n    participant W as Web Interface\n\n    U-&gt;&gt;A: Ask complex question\n    A-&gt;&gt;A: Analyze - needs human input\n    A-&gt;&gt;H: Send question via XMPP\n    H-&gt;&gt;W: Receives notification\n    W-&gt;&gt;H: Shows question in browser\n    H-&gt;&gt;W: Types response\n    W-&gt;&gt;A: Sends response via XMPP\n    A-&gt;&gt;U: Provides informed answer</code></pre>"},{"location":"guides/human-in-the-loop/#quick-start","title":"Quick Start","text":""},{"location":"guides/human-in-the-loop/#1-create-the-tool","title":"1. Create the Tool","text":"<pre><code>from spade_llm.tools import HumanInTheLoopTool\n\n# Create human consultation tool\nhuman_tool = HumanInTheLoopTool(\n    human_expert_jid=\"expert@xmpp.server\",\n    timeout=300.0,  # 5 minutes\n    name=\"ask_human_expert\",\n    description=\"Ask human expert for current info or clarification\"\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#2-add-to-agent","title":"2. Add to Agent","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\n\n# Create LLM agent with human tool\n# Note: Start SPADE server first: spade run\nagent = LLMAgent(\n    jid=\"agent@localhost\", \n    password=\"password\",\n    provider=LLMProvider.create_openai(api_key=\"sk-...\"),\n    tools=[human_tool],\n    system_prompt=\"\"\"You are an AI assistant with access to a human expert.\n    When you need current information, human judgment, or clarification,\n    use the ask_human_expert tool.\"\"\"\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#3-start-web-interface","title":"3. Start Web Interface","text":"<pre><code># Start the human expert web interface\npython -m spade_llm.human_interface.web_server\n\n# Open browser to http://localhost:8080\n# Connect with expert credentials\n</code></pre>"},{"location":"guides/human-in-the-loop/#4-test-the-integration","title":"4. Test the Integration","text":"<pre><code># Chat agent for testing\nchat_agent = ChatAgent(\n    jid=\"user@localhost\",\n    password=\"password\", \n    target_agent_jid=\"agent@localhost\"\n)\n\nawait chat_agent.start()\nawait agent.start()\n\n# Test questions that trigger human consultation\nawait chat_agent.run_interactive()\n</code></pre>"},{"location":"guides/human-in-the-loop/#system-requirements","title":"System Requirements","text":""},{"location":"guides/human-in-the-loop/#xmpp-server-setup","title":"XMPP Server Setup","text":"<p>You need an XMPP server with WebSocket support:</p> OpenFireejabberd <pre><code># Install OpenFire with HTTP File Upload and WebSocket plugins\n# Configure WebSocket on port 7070\n# Create user accounts for agents and human experts\n</code></pre> <pre><code># ejabberd.yml configuration\nlisten:\n  - port: 7070\n    module: ejabberd_http\n    request_handlers:\n      \"/ws\": ejabberd_http_ws\n</code></pre>"},{"location":"guides/human-in-the-loop/#user-accounts","title":"User Accounts","text":"<p>Create XMPP accounts for:</p> <ul> <li>Agent accounts: <code>agent1@server</code>, <code>agent2@server</code>, etc.</li> <li>Human experts: <code>expert1@server</code>, <code>expert2@server</code>, etc.  </li> <li>Chat users: <code>user1@server</code>, <code>user2@server</code>, etc.</li> </ul>"},{"location":"guides/human-in-the-loop/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/human-in-the-loop/#tool-parameters","title":"Tool Parameters","text":"<pre><code>HumanInTheLoopTool(\n    human_expert_jid=\"expert@server\",  # Required: Expert's XMPP address\n    timeout=300.0,                     # Optional: Response timeout (seconds)\n    name=\"ask_human_expert\",           # Optional: Tool name for LLM\n    description=\"Custom description\"   # Optional: When to use this tool\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#system-prompt-guidelines","title":"System Prompt Guidelines","text":"<p>Help the LLM know when to consult humans:</p> <pre><code>system_prompt = \"\"\"You are an AI assistant with access to human experts.\n\nUse the ask_human_expert tool when you need:\n- Current information not in your training data (after April 2024)\n- Human opinions or subjective judgments\n- Company-specific policies or procedures  \n- Clarification on ambiguous requests\n- Verification of important decisions\n\nAlways explain when you're using human vs. AI knowledge.\"\"\"\n</code></pre>"},{"location":"guides/human-in-the-loop/#web-interface-usage","title":"Web Interface Usage","text":""},{"location":"guides/human-in-the-loop/#connecting","title":"Connecting","text":"<ol> <li>Open <code>http://localhost:8080</code> in your browser</li> <li>Enter XMPP credentials:</li> <li>Service: <code>ws://your-server:7070/ws/</code></li> <li>JID: <code>expert@your-server</code></li> <li>Password: <code>your-password</code></li> <li>Click \"Connect\"</li> </ol>"},{"location":"guides/human-in-the-loop/#handling-queries","title":"Handling Queries","text":"<p>When agents ask questions:</p> <ol> <li>Notification appears in browser</li> <li>Question details show:</li> <li>Query ID for tracking</li> <li>Agent asking the question</li> <li>Question text and context</li> <li>Type response in text area</li> <li>Click \"Send Response\"</li> </ol>"},{"location":"guides/human-in-the-loop/#expert-features","title":"Expert Features","text":"<ul> <li>Query filtering: Show/hide answered queries</li> <li>Real-time notifications: Browser notifications for new questions</li> <li>Response history: Track previous interactions</li> <li>Connection status: Visual connection indicator</li> </ul>"},{"location":"guides/human-in-the-loop/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/human-in-the-loop/#multiple-expert-routing","title":"Multiple Expert Routing","text":"<pre><code># Different experts for different domains\nsales_expert = HumanInTheLoopTool(\n    human_expert_jid=\"sales@company.com\",\n    name=\"ask_sales_expert\", \n    description=\"Ask sales team about pricing, deals, customers\"\n)\n\ntech_expert = HumanInTheLoopTool(\n    human_expert_jid=\"tech@company.com\",\n    name=\"ask_tech_expert\",\n    description=\"Ask technical team about systems, infrastructure\"\n)\n\nagent = LLMAgent(\n    jid=\"agent@server\",\n    password=\"password\",\n    provider=provider,\n    tools=[sales_expert, tech_expert],  # Multiple expert tools\n    system_prompt=\"Choose the right expert based on question domain...\"\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#conditional-human-consultation","title":"Conditional Human Consultation","text":"<pre><code>system_prompt = \"\"\"You have access to human experts via ask_human_expert.\n\nOnly consult humans when:\n1. The question involves information after April 2024\n2. You need subjective human judgment  \n3. The request is ambiguous and needs clarification\n4. The decision has significant business impact\n\nFor general knowledge questions, answer directly without consulting humans.\"\"\"\n</code></pre>"},{"location":"guides/human-in-the-loop/#error-handling","title":"Error Handling","text":"<p>The system handles common error scenarios:</p> <pre><code># Timeout handling\nif \"Timeout:\" in response:\n    # Human didn't respond in time\n    print(\"Expert unavailable, proceeding with AI-only response\")\n\n# Expert offline  \nif \"Error:\" in response:\n    # Connection or configuration issue\n    print(\"Expert consultation failed, using fallback approach\")\n</code></pre>"},{"location":"guides/human-in-the-loop/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/human-in-the-loop/#common-issues","title":"Common Issues","text":"<p>Agent Connection Errors</p> <p>Symptom: <code>Agent XMPP client not available</code></p> <p>Solution: Ensure agent is fully connected before tool execution: <pre><code>await agent.start()\nawait asyncio.sleep(2.0)  # Wait for full connection\n</code></pre></p> <p>Human Expert Not Responding</p> <p>Symptom: Timeouts on human consultation</p> <p>Solutions: - Check expert is connected to web interface - Verify XMPP server WebSocket configuration - Increase timeout in tool configuration</p> <p>Double Message Processing</p> <p>Symptom: Human responses processed as new user messages</p> <p>Solution: This is handled automatically by template-based message filtering</p>"},{"location":"guides/human-in-the-loop/#debugging","title":"Debugging","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger(\"spade_llm.tools.human_in_the_loop\").setLevel(logging.DEBUG)\nlogging.getLogger(\"spade_llm.behaviour.human_interaction\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"guides/human-in-the-loop/#network-configuration","title":"Network Configuration","text":"<p>For complex networks:</p> <pre><code># Custom WebSocket URL for web interface\nservice_url = \"wss://xmpp.company.com:7070/ws/\"\n\n# Update web interface connection settings\n# Edit spade_llm/human_interface/web_client/index.html\n</code></pre>"},{"location":"guides/human-in-the-loop/#best-practices","title":"Best Practices","text":""},{"location":"guides/human-in-the-loop/#expert-availability","title":"Expert Availability","text":"<ul> <li>Set expectations: Inform users about expert availability hours</li> <li>Fallback strategies: Plan for when experts are unavailable</li> <li>Response time SLAs: Set clear expectations for response times</li> </ul>"},{"location":"guides/human-in-the-loop/#question-quality","title":"Question Quality","text":"<ul> <li>Provide context: Include relevant background information</li> <li>Be specific: Ask focused questions rather than broad queries</li> <li>Include urgency: Indicate if immediate response is needed</li> </ul>"},{"location":"guides/human-in-the-loop/#system-reliability","title":"System Reliability","text":"<ul> <li>Monitor timeouts: Track expert response patterns</li> <li>Have backup experts: Multiple experts for critical domains</li> <li>Graceful degradation: System works even if human consultation fails</li> </ul>"},{"location":"guides/human-in-the-loop/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference: Detailed class documentation</li> <li>Examples: Working code examples</li> <li>Architecture: System design details</li> <li>Tools System: Complete tools documentation</li> </ul>"},{"location":"guides/mcp/","title":"MCP Integration","text":"<p>Model Context Protocol (MCP) enables SPADE_LLM agents to connect to external services and tools through standardized servers.</p>"},{"location":"guides/mcp/#overview","title":"Overview","text":"<p>MCP provides a standard way for AI applications to connect to data sources and tools. SPADE_LLM automatically discovers and adapts MCP tools for use with LLM agents.</p>"},{"location":"guides/mcp/#benefits","title":"Benefits","text":"<ul> <li>Standardized Interface: Consistent API across different services</li> <li>Dynamic Discovery: Automatic tool detection from MCP servers</li> <li>External Services: Connect to databases, APIs, and file systems</li> <li>Tool Caching: Improved performance through tool caching</li> </ul>"},{"location":"guides/mcp/#mcp-server-types","title":"MCP Server Types","text":""},{"location":"guides/mcp/#stdio-servers","title":"STDIO Servers","text":"<p>Communicate via standard input/output streams:</p> <pre><code>from spade_llm.mcp import StdioServerConfig\n\nserver_config = StdioServerConfig(\n    name=\"DatabaseServer\",\n    command=\"python\",\n    args=[\"path/to/database_server.py\"],\n    env={\"DB_URL\": \"sqlite:///data.db\"},\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#sse-servers","title":"SSE Servers","text":"<p>SSE Transport Deprecated</p> <p>The SSE transport is deprecated in favor of the new Streamable HTTP transport.  While SSE is still supported for backward compatibility, new implementations  should use <code>StreamableHttpServerConfig</code> instead of <code>SseServerConfig</code>.</p> <p>Communicate via Server-Sent Events over HTTP:</p> <pre><code>from spade_llm.mcp import SseServerConfig\n\nserver_config = SseServerConfig(\n    name=\"WebService\",\n    url=\"http://localhost:8080/mcp\",\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#streamable-http-servers","title":"Streamable HTTP Servers","text":"<p>Communicate via the modern Streamable HTTP protocol (recommended for new implementations):</p> <pre><code>from spade_llm.mcp import StreamableHttpServerConfig\n\nserver_config = StreamableHttpServerConfig(\n    name=\"ModernWebService\",\n    url=\"http://localhost:8080/mcp\",\n    headers={\"Authorization\": \"Bearer token\"},\n    timeout=30.0,  # Connection timeout in seconds\n    sse_read_timeout=300.0,  # Read timeout for SSE stream in seconds\n    terminate_on_close=True,  # Terminate connection on close\n    cache_tools=True\n)\n</code></pre> <p>The Streamable HTTP transport provides: - Improved session management and stability - Better handling of long-running connections - Enhanced error recovery mechanisms - Full compatibility with the MCP specification</p>"},{"location":"guides/mcp/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/mcp/#agent-with-mcp-tools","title":"Agent with MCP Tools","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\nfrom spade_llm.mcp import StdioServerConfig\n\nasync def main():\n    # Configure MCP server\n    mcp_server = StdioServerConfig(\n        name=\"FileManager\",\n        command=\"python\",\n        args=[\"-m\", \"file_manager_mcp\"],\n        cache_tools=True\n    )\n\n    # Create agent with MCP integration\n    agent = LLMAgent(\n        jid=\"assistant@example.com\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with file management capabilities\",\n        mcp_servers=[mcp_server]\n    )\n\n    await agent.start()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"guides/mcp/#multiple-mcp-servers","title":"Multiple MCP Servers","text":"<pre><code># Configure multiple servers\nmcp_servers = [\n    StdioServerConfig(\n        name=\"DatabaseService\",\n        command=\"python\",\n        args=[\"database_mcp_server.py\"],\n        env={\"DB_CONNECTION\": \"postgresql://localhost/mydb\"}\n    ),\n    StdioServerConfig(\n        name=\"WeatherService\", \n        command=\"node\",\n        args=[\"weather_mcp_server.js\"],\n        env={\"API_KEY\": \"your-weather-api-key\"}\n    ),\n    StreamableHttpServerConfig(\n        name=\"AnalyticsService\",\n        url=\"https://analytics.example.com/mcp\",\n        headers={\"X-API-Key\": \"your-api-key\"},\n        terminate_on_close=True,\n        cache_tools=True\n    )\n]\n\nagent = LLMAgent(\n    jid=\"multi-service@example.com\",\n    password=\"password\",\n    provider=provider,\n    mcp_servers=mcp_servers\n)\n</code></pre>"},{"location":"guides/mcp/#creating-mcp-servers","title":"Creating MCP Servers","text":""},{"location":"guides/mcp/#basic-stdio-server","title":"Basic STDIO Server","text":"<p>Create a simple MCP server (<code>math_server.py</code>):</p> <pre><code>#!/usr/bin/env python3\nimport json\nimport sys\nimport math\nfrom typing import Any, Dict\n\nclass MathMCPServer:\n    def __init__(self):\n        self.tools = [\n            {\n                \"name\": \"calculate\",\n                \"description\": \"Perform mathematical calculations\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"expression\": {\n                            \"type\": \"string\",\n                            \"description\": \"Mathematical expression to evaluate\"\n                        }\n                    },\n                    \"required\": [\"expression\"]\n                }\n            },\n            {\n                \"name\": \"sqrt\",\n                \"description\": \"Calculate square root\",\n                \"inputSchema\": {\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"number\": {\n                            \"type\": \"number\",\n                            \"description\": \"Number to calculate square root of\"\n                        }\n                    },\n                    \"required\": [\"number\"]\n                }\n            }\n        ]\n\n    def handle_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle MCP requests.\"\"\"\n        method = request.get(\"method\")\n\n        if method == \"tools/list\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request.get(\"id\"),\n                \"result\": {\"tools\": self.tools}\n            }\n\n        elif method == \"tools/call\":\n            params = request.get(\"params\", {})\n            tool_name = params.get(\"name\")\n            arguments = params.get(\"arguments\", {})\n\n            if tool_name == \"calculate\":\n                return self._calculate(request.get(\"id\"), arguments)\n            elif tool_name == \"sqrt\":\n                return self._sqrt(request.get(\"id\"), arguments)\n\n            return self._error(request.get(\"id\"), \"Unknown tool\")\n\n        elif method == \"initialize\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request.get(\"id\"),\n                \"result\": {\n                    \"protocolVersion\": \"2024-11-05\",\n                    \"capabilities\": {\n                        \"tools\": {}\n                    },\n                    \"serverInfo\": {\n                        \"name\": \"math-server\",\n                        \"version\": \"1.0.0\"\n                    }\n                }\n            }\n\n        return self._error(request.get(\"id\"), \"Unknown method\")\n\n    def _calculate(self, request_id: str, args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle calculate tool call.\"\"\"\n        try:\n            expression = args.get(\"expression\", \"\")\n            # Safe evaluation (restrict to math operations)\n            result = eval(expression, {\"__builtins__\": {}, \"math\": math})\n\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request_id,\n                \"result\": {\n                    \"content\": [\n                        {\n                            \"type\": \"text\", \n                            \"text\": f\"Result: {result}\"\n                        }\n                    ]\n                }\n            }\n        except Exception as e:\n            return self._error(request_id, f\"Calculation error: {str(e)}\")\n\n    def _sqrt(self, request_id: str, args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle sqrt tool call.\"\"\"\n        try:\n            number = float(args.get(\"number\", 0))\n            if number &lt; 0:\n                return self._error(request_id, \"Cannot calculate square root of negative number\")\n\n            result = math.sqrt(number)\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request_id,\n                \"result\": {\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": f\"\u221a{number} = {result}\"\n                        }\n                    ]\n                }\n            }\n        except Exception as e:\n            return self._error(request_id, f\"Square root error: {str(e)}\")\n\n    def _error(self, request_id: str, message: str) -&gt; Dict[str, Any]:\n        \"\"\"Return error response.\"\"\"\n        return {\n            \"jsonrpc\": \"2.0\",\n            \"id\": request_id,\n            \"error\": {\n                \"code\": -1,\n                \"message\": message\n            }\n        }\n\n    def run(self):\n        \"\"\"Run the MCP server.\"\"\"\n        for line in sys.stdin:\n            try:\n                request = json.loads(line.strip())\n                response = self.handle_request(request)\n                print(json.dumps(response), flush=True)\n            except Exception as e:\n                error_response = {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32700,\n                        \"message\": f\"Parse error: {str(e)}\"\n                    }\n                }\n                print(json.dumps(error_response), flush=True)\n\nif __name__ == \"__main__\":\n    server = MathMCPServer()\n    server.run()\n</code></pre>"},{"location":"guides/mcp/#using-the-math-server","title":"Using the Math Server","text":"<pre><code># Configure the math MCP server\nmath_server = StdioServerConfig(\n    name=\"MathService\",\n    command=\"python\",\n    args=[\"math_server.py\"],\n    cache_tools=True\n)\n\n# Create agent with math capabilities\nagent = LLMAgent(\n    jid=\"math-assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a math assistant. Use the calculate and sqrt tools for mathematical operations.\",\n    mcp_servers=[math_server]\n)\n\nawait agent.start()\n</code></pre>"},{"location":"guides/mcp/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/mcp/#server-configuration","title":"Server Configuration","text":"<pre><code># STDIO Server with environment variables\nstdio_config = StdioServerConfig(\n    name=\"MyService\",\n    command=\"python\",\n    args=[\"my_mcp_server.py\"],\n    env={\n        \"API_KEY\": \"your-api-key\",\n        \"DB_URL\": \"postgresql://localhost/mydb\",\n        \"LOG_LEVEL\": \"INFO\"\n    },\n    cache_tools=True,\n    working_directory=\"/path/to/server\"\n)\n\n# SSE Server with authentication (deprecated - use Streamable HTTP instead)\nsse_config = SseServerConfig(\n    name=\"WebService\",\n    url=\"https://api.example.com/mcp\",\n    headers={\n        \"Authorization\": \"Bearer your-token\",\n        \"X-API-Version\": \"v1\"\n    },\n    cache_tools=True\n)\n\n# Streamable HTTP Server with advanced options (recommended)\nstreamable_config = StreamableHttpServerConfig(\n    name=\"ModernService\",\n    url=\"https://api.example.com/mcp\",\n    headers={\n        \"Authorization\": \"Bearer your-token\",\n        \"X-API-Version\": \"v2\"\n    },\n    timeout=30.0,  # Connection timeout in seconds\n    sse_read_timeout=300.0,  # Read timeout for SSE stream (5 minutes)\n    terminate_on_close=True,  # Cleanly terminate connection on close\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#agent-configuration","title":"Agent Configuration","text":"<pre><code>agent = LLMAgent(\n    jid=\"mcp-agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You have access to external services via MCP tools. Use them when needed.\",\n    mcp_servers=[stdio_config, sse_config]\n)\n</code></pre>"},{"location":"guides/mcp/#best-practices","title":"Best Practices","text":""},{"location":"guides/mcp/#server-development","title":"Server Development","text":"<ul> <li>Error Handling: Always return proper error responses</li> <li>Input Validation: Validate all tool parameters</li> <li>Resource Cleanup: Properly close database connections</li> <li>Logging: Include detailed logging for debugging</li> <li>Security: Validate and sanitize all inputs</li> </ul>"},{"location":"guides/mcp/#agent-configuration_1","title":"Agent Configuration","text":"<ul> <li>Tool Caching: Enable caching for better performance</li> <li>Environment Variables: Use env vars for configuration</li> <li>Error Recovery: Handle MCP server failures gracefully</li> <li>Tool Selection: Configure agents with relevant tools only</li> </ul>"},{"location":"guides/mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/mcp/#common-issues","title":"Common Issues","text":"<p>Server not starting: - Check command and arguments - Verify working directory - Check environment variables</p> <p>Tool discovery fails: - Test server manually with JSON-RPC calls - Check server implements required methods - Verify JSON-RPC format</p> <p>Tool execution errors: - Check parameter schemas match - Validate input data types - Handle exceptions in tool functions</p> <p>MCP integration provides powerful capabilities for connecting SPADE_LLM agents to external services and data sources. Start with simple STDIO servers and gradually build more complex integrations as needed.</p>"},{"location":"guides/memory-architecture/","title":"Memory Architecture","text":"<p>This guide provides detailed architectural diagrams and explanations of SPADE_LLM's memory system components and their interactions.</p>"},{"location":"guides/memory-architecture/#system-overview","title":"System Overview","text":"<pre><code>graph TB\n    subgraph \"SPADE_LLM Agent\"\n        A[LLMAgent] --&gt; B[LLMBehaviour]\n        B --&gt; C[Memory Manager]\n    end\n\n    subgraph \"Dual Memory System\"\n        C --&gt; D[Interaction Memory]\n        C --&gt; E[Agent Base Memory]\n    end\n\n    subgraph \"Storage Backends\"\n        D --&gt; F[JSON Files]\n        E --&gt; G[SQLite Database]\n        G --&gt; G1[Persistent Mode]\n        G --&gt; G2[In-Memory Mode]\n    end\n\n    subgraph \"Memory Tools\"\n        H[remember_interaction_info] --&gt; D\n        I[store_memory] --&gt; E\n        J[search_memories] --&gt; E\n        K[list_memories] --&gt; E\n    end\n\n    subgraph \"Integration Points\"\n        L[Context Manager] --&gt; D\n        M[LLM Provider] --&gt; H\n        M --&gt; I\n        M --&gt; J\n        M --&gt; K\n    end\n\n    style A fill:#e1f5fe\n    style D fill:#f3e5f5\n    style E fill:#e8f5e8\n    style F fill:#fff3e0\n    style G fill:#e8f5e8</code></pre>"},{"location":"guides/memory-architecture/#memory-types-architecture","title":"Memory Types Architecture","text":""},{"location":"guides/memory-architecture/#interaction-memory-flow","title":"Interaction Memory Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant A as LLMAgent\n    participant IM as Interaction Memory\n    participant CM as Context Manager\n    participant LLM as LLM Provider\n\n    U-&gt;&gt;A: Send message\n    A-&gt;&gt;IM: Check existing memory\n    IM-&gt;&gt;CM: Inject memory context\n    CM-&gt;&gt;LLM: Send enriched context\n    LLM-&gt;&gt;A: Response + tool calls\n    A-&gt;&gt;IM: Store new information\n    IM-&gt;&gt;U: Enhanced response\n\n    Note over IM: JSON-based storage\n    Note over CM: Auto-injection</code></pre>"},{"location":"guides/memory-architecture/#agent-base-memory-flow","title":"Agent Base Memory Flow","text":""},{"location":"guides/memory-architecture/#persistent-mode","title":"Persistent Mode","text":"<pre><code>sequenceDiagram\n    participant LLM as LLM Provider\n    participant ABM as Agent Base Memory\n    participant SB as SQLite Backend\n    participant FS as File System\n\n    LLM-&gt;&gt;ABM: store_memory(content, category)\n    ABM-&gt;&gt;SB: Execute INSERT query\n    SB-&gt;&gt;FS: Write to database\n    FS-&gt;&gt;SB: Confirm write\n    SB-&gt;&gt;ABM: Return memory ID\n    ABM-&gt;&gt;LLM: Confirmation message\n\n    LLM-&gt;&gt;ABM: search_memories(query)\n    ABM-&gt;&gt;SB: Execute SELECT query\n    SB-&gt;&gt;FS: Read from database\n    FS-&gt;&gt;SB: Return results\n    SB-&gt;&gt;ABM: Memory entries\n    ABM-&gt;&gt;LLM: Formatted results</code></pre>"},{"location":"guides/memory-architecture/#in-memory-mode","title":"In-Memory Mode","text":"<pre><code>sequenceDiagram\n    participant LLM as LLM Provider\n    participant ABM as Agent Base Memory\n    participant SB as SQLite Backend\n    participant RAM as Memory (RAM)\n\n    LLM-&gt;&gt;ABM: store_memory(content, category)\n    ABM-&gt;&gt;SB: Execute INSERT query\n    SB-&gt;&gt;RAM: Write to in-memory database\n    RAM-&gt;&gt;SB: Confirm write\n    SB-&gt;&gt;ABM: Return memory ID\n    ABM-&gt;&gt;LLM: Confirmation message\n\n    LLM-&gt;&gt;ABM: search_memories(query)\n    ABM-&gt;&gt;SB: Execute SELECT query\n    SB-&gt;&gt;RAM: Read from memory\n    RAM-&gt;&gt;SB: Return results\n    SB-&gt;&gt;ABM: Memory entries\n    ABM-&gt;&gt;LLM: Formatted results\n\n    Note over RAM: Automatically deleted on agent stop</code></pre>"},{"location":"guides/memory-architecture/#data-storage-architecture","title":"Data Storage Architecture","text":""},{"location":"guides/memory-architecture/#file-system-organization","title":"File System Organization","text":""},{"location":"guides/memory-architecture/#persistent-storage","title":"Persistent Storage","text":"<pre><code>graph LR\n    subgraph \"Memory Root Path\"\n        A[/memory/path/]\n    end\n\n    subgraph \"Agent 1 Files\"\n        A --&gt; B[agent1_example_com_interactions.json]\n        A --&gt; C[agent1_example_com_base_memory.db]\n    end\n\n    subgraph \"Agent 2 Files\"\n        A --&gt; D[agent2_example_com_interactions.json]\n        A --&gt; E[agent2_example_com_base_memory.db]\n    end\n\n    subgraph \"Shared Resources\"\n        A --&gt; F[.memory_metadata.json]\n        A --&gt; G[backup/]\n    end\n\n    style A fill:#fff3e0\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#f3e5f5\n    style E fill:#e8f5e8</code></pre>"},{"location":"guides/memory-architecture/#in-memory-storage","title":"In-Memory Storage","text":"<pre><code>graph LR\n    subgraph \"RAM Memory Space\"\n        A[In-Memory Databases]\n    end\n\n    subgraph \"Agent 1 Memory\"\n        A --&gt; B[:memory: database]\n        B --&gt; C[agent_memories table]\n        C --&gt; D[Temporary data]\n    end\n\n    subgraph \"Agent 2 Memory\"\n        A --&gt; E[:memory: database]\n        E --&gt; F[agent_memories table]\n        F --&gt; G[Temporary data]\n    end\n\n    subgraph \"Lifecycle\"\n        H[Agent Start] --&gt; I[Create :memory: DB]\n        I --&gt; J[Store/Retrieve Data]\n        J --&gt; K[Agent Stop]\n        K --&gt; L[Automatic Cleanup]\n    end\n\n    style A fill:#e3f2fd\n    style B fill:#e8f5e8\n    style E fill:#e8f5e8\n    style L fill:#ffebee</code></pre>"},{"location":"guides/memory-architecture/#database-schema-architecture","title":"Database Schema Architecture","text":"<pre><code>erDiagram\n    AGENT_MEMORIES {\n        string id PK\n        string agent_id FK\n        string category\n        string content\n        string context\n        real confidence\n        timestamp created_at\n        timestamp last_accessed\n        integer access_count\n    }\n\n    MEMORY_CATEGORIES {\n        string category PK\n        string description\n        string usage_examples\n    }\n\n    MEMORY_STATS {\n        string agent_id PK\n        integer total_memories\n        real avg_confidence\n        timestamp last_update\n        json category_distribution\n    }\n\n    AGENT_MEMORIES ||--o{ MEMORY_CATEGORIES : belongs_to\n    AGENT_MEMORIES ||--o{ MEMORY_STATS : contributes_to</code></pre>"},{"location":"guides/memory-architecture/#memory-tool-integration","title":"Memory Tool Integration","text":""},{"location":"guides/memory-architecture/#tool-registration-flow","title":"Tool Registration Flow","text":"<pre><code>graph TB\n    subgraph \"Agent Initialization\"\n        A[LLMAgent.__init__] --&gt; B{Memory Config?}\n        B --&gt;|interaction_memory=True| C[Register interaction tools]\n        B --&gt;|agent_base_memory=True| D[Register base memory tools]\n        B --&gt;|Both enabled| E[Register all tools]\n    end\n\n    subgraph \"Tool Registration\"\n        C --&gt; F[remember_interaction_info]\n        D --&gt; G[store_memory]\n        D --&gt; H[search_memories]\n        D --&gt; I[list_memories]\n        E --&gt; F\n        E --&gt; G\n        E --&gt; H\n        E --&gt; I\n    end\n\n    subgraph \"Tool Execution\"\n        F --&gt; J[Interaction Memory API]\n        G --&gt; K[Base Memory API]\n        H --&gt; K\n        I --&gt; K\n    end\n\n    style A fill:#e1f5fe\n    style J fill:#f3e5f5\n    style K fill:#e8f5e8</code></pre>"},{"location":"guides/memory-architecture/#tool-call-processing","title":"Tool Call Processing","text":"<pre><code>sequenceDiagram\n    participant LLM as LLM Provider\n    participant TB as Tool Bridge\n    participant MT as Memory Tool\n    participant MS as Memory System\n\n    LLM-&gt;&gt;TB: Tool call request\n    TB-&gt;&gt;MT: Parse and validate\n    MT-&gt;&gt;MS: Execute memory operation\n    MS-&gt;&gt;MT: Return result\n    MT-&gt;&gt;TB: Formatted response\n    TB-&gt;&gt;LLM: Tool execution result\n\n    Note over MT: Handles validation\n    Note over MS: Performs storage/retrieval</code></pre>"},{"location":"guides/memory-architecture/#memory-categories-system","title":"Memory Categories System","text":""},{"location":"guides/memory-architecture/#category-organization","title":"Category Organization","text":"<pre><code>graph TB\n    subgraph \"Memory Categories\"\n        A[Agent Base Memory] --&gt; B[Facts]\n        A --&gt; C[Patterns]\n        A --&gt; D[Preferences]\n        A --&gt; E[Capabilities]\n    end\n\n    subgraph \"Fact Examples\"\n        B --&gt; F[\"API endpoints\"]\n        B --&gt; G[\"Configuration values\"]\n        B --&gt; H[\"Database schemas\"]\n    end\n\n    subgraph \"Pattern Examples\"\n        C --&gt; I[\"User behavior trends\"]\n        C --&gt; J[\"Error patterns\"]\n        C --&gt; K[\"Usage patterns\"]\n    end\n\n    subgraph \"Preference Examples\"\n        D --&gt; L[\"Response formats\"]\n        D --&gt; M[\"Communication styles\"]\n        D --&gt; N[\"Tool preferences\"]\n    end\n\n    subgraph \"Capability Examples\"\n        E --&gt; O[\"Skills and abilities\"]\n        E --&gt; P[\"Limitations\"]\n        E --&gt; Q[\"Specializations\"]\n    end\n\n    style A fill:#e8f5e8\n    style B fill:#e3f2fd\n    style C fill:#f3e5f5\n    style D fill:#fff3e0\n    style E fill:#e8f5e8</code></pre>"},{"location":"guides/memory-architecture/#context-integration","title":"Context Integration","text":""},{"location":"guides/memory-architecture/#agent-interaction-memory-context-injection","title":"Agent interaction Memory Context Injection","text":"<pre><code>graph TB\n    subgraph \"Conversation Processing\"\n        A[New Message] --&gt; B[Context Manager]\n        B --&gt; C{Memory Available?}\n    end\n\n    subgraph \"Memory Injection\"\n        C --&gt;|Yes| D[Retrieve Memories]\n        D --&gt; E[Format Context]\n        E --&gt; F[Inject into Prompt]\n        C --&gt;|No| G[Standard Context]\n    end\n\n    subgraph \"Context Types\"\n        F --&gt; H[System Message]\n        F --&gt; I[User Context]\n        F --&gt; J[Tool Context]\n        G --&gt; K[Basic Context]\n    end\n\n    subgraph \"LLM Processing\"\n        H --&gt; L[LLM Provider]\n        I --&gt; L\n        J --&gt; L\n        K --&gt; L\n    end\n\n    style A fill:#e1f5fe\n    style D fill:#e8f5e8\n    style L fill:#fff3e0</code></pre>"},{"location":"guides/memory-architecture/#multi-agent-memory-architecture","title":"Multi-Agent Memory Architecture","text":""},{"location":"guides/memory-architecture/#agent-memory-isolation","title":"Agent Memory Isolation","text":"<pre><code>graph TB\n    subgraph \"Agent 1\"\n        A1[LLMAgent 1] --&gt; B1[Memory Manager 1]\n        B1 --&gt; C1[Interaction Memory 1]\n        B1 --&gt; D1[Base Memory 1]\n    end\n\n    subgraph \"Agent 2\"\n        A2[LLMAgent 2] --&gt; B2[Memory Manager 2]\n        B2 --&gt; C2[Interaction Memory 2]\n        B2 --&gt; D2[Base Memory 2]\n    end\n\n    subgraph \"Shared Infrastructure\"\n        E[Memory Backend Pool] --&gt; F[SQLite Connections]\n        E --&gt; G[File System Access]\n        E --&gt; H[Configuration Manager]\n    end\n\n    subgraph \"Isolation Boundaries\"\n        C1 -.-&gt; I[Conversation Isolation]\n        C2 -.-&gt; I\n        D1 -.-&gt; J[Agent Data Isolation]\n        D2 -.-&gt; J\n    end\n\n    style I fill:#ffebee\n    style J fill:#ffebee\n    style E fill:#e8f5e8</code></pre>"},{"location":"guides/memory-architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Memory System Guide - Complete memory system documentation</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"guides/memory/","title":"Memory System","text":"<p>SPADE_LLM provides a dual memory architecture that enables agents to learn from interactions and maintain knowledge across conversations.</p>"},{"location":"guides/memory/#overview","title":"Overview","text":"<p>The memory system allows agents to:</p> <ul> <li>Store Information: Persist data across conversations and agent restarts</li> <li>Learn from Experience: Build knowledge from interactions over time</li> <li>Categorize Knowledge: Organize information into facts, patterns, preferences, and capabilities</li> <li>Search Memories: Find relevant information using search capabilities</li> <li>Tool Integration: LLMs can store and retrieve memories using built-in tools</li> </ul>"},{"location":"guides/memory/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    A[LLM Agent] --&gt; B[Dual Memory System]\n    B --&gt; C[Interaction Memory]\n    B --&gt; D[Agent Base Memory]\n\n    C --&gt; E[JSON Storage]\n    C --&gt; F[Conversation-Specific]\n    C --&gt; G[Auto-Injection]\n\n    D --&gt; H[SQLite Backend]\n    D --&gt; I[Agent-Wide Learning]\n    D --&gt; J[Categorized Storage]\n\n    K[Memory Tools] --&gt; C\n    K --&gt; D\n    L[LLM Integration] --&gt; K\n\n    M[Context Manager] --&gt; G\n    N[Search Engine] --&gt; J\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n    style D fill:#e8f5e8\n    style K fill:#fff8e1</code></pre>"},{"location":"guides/memory/#memory-types","title":"Memory Types","text":""},{"location":"guides/memory/#1-interaction-memory","title":"1. Interaction Memory","text":"<p>Purpose: Stores conversation-specific information for immediate context enhancement.</p> <p>Key Features: - Conversation-scoped: Memory isolated by conversation ID - JSON-based: Simple file-based storage - Auto-injection: Automatically added to conversation context - Temporary: Designed for conversation-specific details</p> <p>Use Cases: - User preferences within a conversation - API tokens or configuration details - Conversation-specific settings - Contextual information for current session</p>"},{"location":"guides/memory/#2-agent-base-memory","title":"2. Agent Base Memory","text":"<p>Purpose: Provides long-term learning capabilities across all conversations.</p> <p>Key Features: - Agent-scoped: Memory persists across all conversations - SQLite backend: Reliable database storage with search capabilities - Flexible Storage: Choose between persistent file-based or temporary in-memory storage - Categorized: Organized into four memory categories - Persistent: Survives agent restarts and system reboots (file-based mode) - Tool integration: Three auto-registered tools for LLM access</p> <p>Memory Categories: - <code>fact</code>: Concrete information (APIs, data formats, configurations) - <code>pattern</code>: Behavioral patterns or trends observed - <code>preference</code>: User preferences or system settings learned - <code>capability</code>: Agent's abilities or limitations discovered</p>"},{"location":"guides/memory/#configuration","title":"Configuration","text":""},{"location":"guides/memory/#basic-setup","title":"Basic Setup","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\n\n# Interaction memory only\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True\n)\n\n# Agent base memory only (persistent)\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    agent_base_memory=True\n)\n\n# Agent base memory (in-memory, temporary)\nagent = LLMAgent(\n    jid=\"test_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    agent_base_memory=(True, \":memory:\")\n)\n\n# Both memory types\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True,\n    agent_base_memory=True\n)\n</code></pre>"},{"location":"guides/memory/#custom-memory-paths","title":"Custom Memory Paths","text":"<pre><code># Environment variable (recommended)\nimport os\nos.environ['SPADE_LLM_MEMORY_PATH'] = \"/custom/memory/path\"\n\n# Constructor parameters\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=(True, \"/custom/interaction/path\"),\n    agent_base_memory=(True, \"/custom/base/memory/path\")\n)\n</code></pre>"},{"location":"guides/memory/#memory-path-resolution","title":"Memory Path Resolution","text":"<p>Memory paths are resolved in the following order: 1. Constructor parameter: <code>(True, \"/custom/path\")</code> 2. Environment variable: <code>SPADE_LLM_MEMORY_PATH</code> 3. Default path: <code>spade_llm/data/agent_memory/</code></p>"},{"location":"guides/memory/#memory-tools","title":"Memory Tools","text":""},{"location":"guides/memory/#interaction-memory-tools","title":"Interaction Memory Tools","text":""},{"location":"guides/memory/#remember_interaction_info","title":"remember_interaction_info","text":"<p>Store information for the current conversation.</p> <pre><code># LLM tool call\n{\n    \"name\": \"remember_interaction_info\",\n    \"parameters\": {\n        \"information\": \"User prefers JSON responses over XML\"\n    }\n}\n</code></pre>"},{"location":"guides/memory/#agent-base-memory-tools","title":"Agent Base Memory Tools","text":""},{"location":"guides/memory/#store_memory","title":"store_memory","text":"<p>Store information in long-term memory with category classification.</p> <pre><code># LLM tool call\n{\n    \"name\": \"store_memory\",\n    \"parameters\": {\n        \"content\": \"User authentication requires API key in header\",\n        \"category\": \"fact\",\n        \"context\": \"API integration discussion\"\n    }\n}\n</code></pre>"},{"location":"guides/memory/#search_memories","title":"search_memories","text":"<p>Search through stored memories for relevant information.</p> <pre><code># LLM tool call\n{\n    \"name\": \"search_memories\",\n    \"parameters\": {\n        \"query\": \"API authentication\",\n        \"limit\": 5\n    }\n}\n</code></pre>"},{"location":"guides/memory/#list_memories","title":"list_memories","text":"<p>List memories by category or view recent memories.</p> <pre><code># LLM tool call\n{\n    \"name\": \"list_memories\",\n    \"parameters\": {\n        \"category\": \"fact\",\n        \"limit\": 10\n    }\n}\n</code></pre>"},{"location":"guides/memory/#storage-architecture","title":"Storage Architecture","text":""},{"location":"guides/memory/#interaction-memory-storage","title":"Interaction Memory Storage","text":"<p>Location: <code>{memory_path}/{safe_agent_id}_interactions.json</code></p> <p>Format: <pre><code>{\n    \"agent_id\": \"agent@example.com\",\n    \"interactions\": {\n        \"conversation_id\": [\n            {\n                \"content\": \"User prefers JSON responses\",\n                \"timestamp\": \"2025-01-10T10:30:00.000Z\"\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"guides/memory/#agent-base-memory-storage","title":"Agent Base Memory Storage","text":""},{"location":"guides/memory/#persistent-mode-default","title":"Persistent Mode (Default)","text":"<p>Location: <code>{memory_path}/{safe_agent_id}_base_memory.db</code></p>"},{"location":"guides/memory/#in-memory-mode","title":"In-Memory Mode","text":"<p>Location: RAM-only (<code>:memory:</code> database)</p> <p>SQLite Schema (both modes): <pre><code>CREATE TABLE agent_memories (\n    id TEXT PRIMARY KEY,\n    agent_id TEXT NOT NULL,\n    category TEXT NOT NULL,\n    content TEXT NOT NULL,\n    context TEXT,\n    confidence REAL DEFAULT 1.0,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    access_count INTEGER DEFAULT 0,\n    INDEX idx_agent_category (agent_id, category),\n    INDEX idx_content_search (content)\n);\n</code></pre></p> <p>Storage Modes: - Persistent: Survives agent restarts, stored in database files - In-Memory: Temporary storage, automatically deleted when agent stops</p>"},{"location":"guides/memory/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/memory/#basic-memory-usage","title":"Basic Memory Usage","text":"<pre><code>import asyncio\nfrom spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\n\nasync def basic_memory_example():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    agent = LLMAgent(\n        jid=\"learning_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        agent_base_memory=True,\n        system_prompt=\"\"\"You are a learning assistant with memory capabilities.\n\n        Use these tools to learn and remember:\n        - store_memory: Save important information for future use\n        - search_memories: Find relevant information from past conversations\n        - list_memories: Browse your stored knowledge by category\n\n        Remember facts, patterns, preferences, and capabilities you discover.\"\"\"\n    )\n\n    await agent.start()\n    print(\"Agent started with memory capabilities\")\n\n    # Agent will automatically use memory tools during conversations\n    await asyncio.sleep(3600)  # Run for 1 hour\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(basic_memory_example())\n</code></pre>"},{"location":"guides/memory/#dual-memory-configuration","title":"Dual Memory Configuration","text":"<pre><code>async def dual_memory_example():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    agent = LLMAgent(\n        jid=\"advanced_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,  # Conversation-specific memory\n        agent_base_memory=True,   # Long-term learning memory\n        system_prompt=\"\"\"You are an advanced assistant with dual memory systems.\n\n        You have access to:\n        - Interaction memory: For conversation-specific details\n        - Base memory: For long-term learning across all conversations\n\n        Use remember_interaction_info for conversation-specific information.\n        Use store_memory, search_memories, and list_memories for long-term learning.\"\"\"\n    )\n\n    await agent.start()\n    print(\"Agent started with dual memory systems\")\n    await asyncio.sleep(3600)\n    await agent.stop()\n</code></pre>"},{"location":"guides/memory/#memory-enabled-multi-agent-system","title":"Memory-Enabled Multi-Agent System","text":"<pre><code>async def multi_agent_memory_example():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Database specialist with memory\n    db_agent = LLMAgent(\n        jid=\"db_specialist@example.com\",\n        password=\"password\",\n        provider=provider,\n        agent_base_memory=True,\n        system_prompt=\"\"\"You are a database specialist with memory capabilities.\n\n        Store database facts, patterns, and user preferences.\n        Search your memory for relevant database solutions.\"\"\"\n    )\n\n    # API specialist with memory\n    api_agent = LLMAgent(\n        jid=\"api_specialist@example.com\",\n        password=\"password\",\n        provider=provider,\n        agent_base_memory=True,\n        system_prompt=\"\"\"You are an API specialist with memory capabilities.\n\n        Remember API patterns, authentication methods, and integration details.\n        Use your memory to provide consistent API guidance.\"\"\"\n    )\n\n    await db_agent.start()\n    await api_agent.start()\n\n    print(\"Multi-agent system with individual memory systems started\")\n\n    # Each agent maintains its own memory\n    await asyncio.sleep(3600)\n\n    await db_agent.stop()\n    await api_agent.stop()\n</code></pre>"},{"location":"guides/memory/#memory-categories-guide","title":"Memory Categories Guide","text":""},{"location":"guides/memory/#fact-category","title":"Fact Category","text":"<p>Store concrete, verifiable information.</p> <p>Examples: - \"API endpoint: https://api.example.com/v1/users\" - \"Database connection requires SSL on port 5432\" - \"User authentication uses JWT tokens\"</p> <p>Usage: <pre><code>store_memory(\n    content=\"API rate limit is 1000 requests per hour\",\n    category=\"fact\",\n    context=\"API integration requirements\"\n)\n</code></pre></p>"},{"location":"guides/memory/#pattern-category","title":"Pattern Category","text":"<p>Store behavioral patterns or trends observed.</p> <p>Examples: - \"Users typically request data in JSON format\" - \"Database queries perform better with indexed columns\" - \"API failures increase during peak hours\"</p> <p>Usage: <pre><code>store_memory(\n    content=\"Users prefer batch operations over individual requests\",\n    category=\"pattern\",\n    context=\"API usage optimization\"\n)\n</code></pre></p>"},{"location":"guides/memory/#preference-category","title":"Preference Category","text":"<p>Store user preferences and system settings.</p> <p>Examples: - \"User prefers verbose error messages\" - \"System should use UTC timezone\" - \"Responses should include example code\"</p> <p>Usage: <pre><code>store_memory(\n    content=\"User prefers Python examples over JavaScript\",\n    category=\"preference\",\n    context=\"Code example preferences\"\n)\n</code></pre></p>"},{"location":"guides/memory/#capability-category","title":"Capability Category","text":"<p>Store agent abilities and limitations.</p> <p>Examples: - \"Can generate SQL queries for PostgreSQL\" - \"Cannot access external APIs without authentication\" - \"Specializes in REST API design patterns\"</p> <p>Usage: <pre><code>store_memory(\n    content=\"Can provide database schema optimization recommendations\",\n    category=\"capability\",\n    context=\"Agent specialization discovery\"\n)\n</code></pre></p>"},{"location":"guides/memory/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"guides/memory/#context-management-integration","title":"Context Management Integration","text":"<pre><code>from spade_llm.context import SmartWindowSizeContext\n\n# Memory works with context management\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True  # Prioritizes memory tool results\n)\n\nagent = LLMAgent(\n    jid=\"smart_memory_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    agent_base_memory=True,\n    context_management=smart_context\n)\n</code></pre>"},{"location":"guides/memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/memory/#interaction-memory","title":"Interaction Memory","text":"<p>Advantages: - Fast file-based operations - Simple JSON format - Minimal overhead</p> <p>Limitations: - File size grows with conversation length - No search capabilities - File locking issues with concurrent access</p>"},{"location":"guides/memory/#agent-base-memory","title":"Agent Base Memory","text":""},{"location":"guides/memory/#persistent-mode","title":"Persistent Mode","text":"<p>Advantages: - Fast SQLite database operations - Full-text search capabilities - Concurrent access support - Structured data storage - Survives agent restarts</p> <p>Limitations: - Database file size growth - Memory queries add latency - Requires database maintenance</p>"},{"location":"guides/memory/#in-memory-mode_1","title":"In-Memory Mode","text":"<p>Advantages: - Fastest possible operations (no disk I/O) - Automatic cleanup on agent stop - No file system pollution - Perfect for testing and development</p> <p>Limitations: - No persistence across agent restarts - Higher memory usage - Data lost when agent stops</p>"},{"location":"guides/memory/#next-steps","title":"Next Steps","text":"<ul> <li>Context Management - Context control strategies</li> <li>Conversations - Conversation lifecycle management</li> <li>Tools System - Tool integration and capabilities</li> <li>API Reference - Detailed memory API documentation</li> </ul>"},{"location":"guides/providers/","title":"LLM Providers","text":"<p>SPADE_LLM supports multiple LLM providers through a unified interface, enabling seamless switching between different AI services.</p>"},{"location":"guides/providers/#provider-architecture","title":"Provider Architecture","text":"<pre><code>graph TD\n    A[LLMProvider Interface] --&gt; B[OpenAI Provider]\n    A --&gt; C[Ollama Provider]\n    A --&gt; D[LM Studio Provider]\n    A --&gt; E[vLLM Provider]\n\n    B --&gt; F[GPT-4o]\n    B --&gt; G[GPT-4o-mini]\n    B --&gt; H[GPT-3.5-turbo]\n\n    C --&gt; I[Llama 3.1:8b]\n    C --&gt; J[Mistral:7b]\n    C --&gt; K[CodeLlama:7b]\n\n    D --&gt; L[Local Models]\n    E --&gt; M[High-Performance Inference]</code></pre>"},{"location":"guides/providers/#supported-providers","title":"Supported Providers","text":"<p>The unified LLMProvider interface supports:</p> <ul> <li>OpenAI - GPT models via API for production-ready solutions</li> <li>Ollama - Local open-source models for privacy-focused deployments  </li> <li>LM Studio - Local models with GUI for easy experimentation</li> <li>vLLM - High-performance inference server for scalable applications</li> </ul>"},{"location":"guides/providers/#openai-provider","title":"OpenAI Provider","text":"<p>Cloud-based LLM service with state-of-the-art models:</p> <pre><code>from spade_llm.providers import LLMProvider\n\nprovider = LLMProvider.create_openai(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7\n)\n</code></pre> <p>Popular models: <code>gpt-4o</code>, <code>gpt-4o-mini</code>, <code>gpt-3.5-turbo</code></p> <p>Key advantages: Excellent tool calling, consistent performance, extensive model options.</p>"},{"location":"guides/providers/#ollama-provider","title":"Ollama Provider","text":"<p>Local deployment for privacy and control:</p> <pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    base_url=\"http://localhost:11434/v1\"\n)\n</code></pre> <p>Popular models: <code>llama3.1:8b</code>, <code>mistral:7b</code>, <code>codellama:7b</code></p> <p>Tool support: Available with <code>llama3.1:8b</code>, <code>llama3.1:70b</code>, <code>mistral:7b</code></p> <p>Key advantages: Complete privacy, no internet required, cost-effective for high usage.</p>"},{"location":"guides/providers/#lm-studio-provider","title":"LM Studio Provider","text":"<p>Local models with GUI for easy management:</p> <pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"local-model\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre> <p>The model name should match exactly what's displayed in the LM Studio interface.</p> <p>Key advantages: User-friendly interface, easy model switching, good for experimentation.</p>"},{"location":"guides/providers/#vllm-provider","title":"vLLM Provider","text":"<p>High-performance inference for production deployments:</p> <pre><code>provider = LLMProvider.create_vllm(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Start vLLM server: <pre><code>python -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-2-7b-chat-hf \\\n    --port 8000\n</code></pre></p> <p>Key advantages: Optimized performance, batching support, scalable architecture.</p>"},{"location":"guides/providers/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/providers/#environment-variables","title":"Environment Variables","text":"<p>Centralized configuration using environment variables:</p> <pre><code># .env file\nOPENAI_API_KEY=your-key\nOLLAMA_BASE_URL=http://localhost:11434/v1\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\n</code></pre>"},{"location":"guides/providers/#dynamic-provider-selection","title":"Dynamic Provider Selection","text":"<p>Runtime provider switching based on configuration:</p> <pre><code>import os\n\ndef create_provider():\n    provider_type = os.getenv('LLM_PROVIDER', 'openai')\n\n    if provider_type == 'openai':\n        return LLMProvider.create_openai(\n            api_key=os.getenv('OPENAI_API_KEY'),\n            model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n        )\n    elif provider_type == 'ollama':\n        return LLMProvider.create_ollama(\n            model=os.getenv('OLLAMA_MODEL', 'llama3.1:8b')\n        )\n</code></pre> <p>This approach enables easy deployment across different environments without code changes.</p>"},{"location":"guides/providers/#error-handling","title":"Error Handling","text":"<p>Robust error handling for production reliability:</p> <pre><code>try:\n    response = await provider.get_llm_response(context)\nexcept Exception as e:\n    logger.error(f\"Provider error: {e}\")\n    # Handle fallback or retry logic\n</code></pre>"},{"location":"guides/providers/#provider-fallback-system","title":"Provider Fallback System","text":"<p>Automatic failover for high availability:</p> <pre><code>providers = [\n    LLMProvider.create_openai(api_key=\"key\"),\n    LLMProvider.create_ollama(model=\"llama3.1:8b\")\n]\n\nasync def get_response_with_fallback(context):\n    for provider in providers:\n        try:\n            return await provider.get_llm_response(context)\n        except Exception:\n            continue\n    raise Exception(\"All providers failed\")\n</code></pre> <p>This pattern ensures service continuity even when individual providers experience issues.</p>"},{"location":"guides/providers/#provider-selection-guide","title":"Provider Selection Guide","text":""},{"location":"guides/providers/#cloud-vs-local","title":"Cloud vs Local","text":"<p>Choose OpenAI when: - Need best-in-class performance - Want consistent reliability - Have internet connectivity - Budget allows for API costs</p> <p>Choose Local Providers when: - Privacy is paramount - Want complete control over infrastructure - Have computational resources - Need to minimize ongoing costs</p>"},{"location":"guides/providers/#performance-considerations","title":"Performance Considerations","text":"<p>OpenAI: Fastest response times, excellent reasoning capabilities Ollama: Good performance with smaller models, privacy benefits LM Studio: Easy setup, good for development and testing vLLM: Optimized inference, best for high-throughput applications</p>"},{"location":"guides/providers/#tool-calling-support","title":"Tool Calling Support","text":"<p>Full tool support: OpenAI (all models) Limited tool support: Ollama (specific models only) Experimental: LM Studio and vLLM (model dependent)</p>"},{"location":"guides/providers/#best-practices","title":"Best Practices","text":"<ul> <li>Test multiple providers during development to find the best fit</li> <li>Implement fallback systems for critical applications</li> <li>Use environment variables for easy configuration management</li> <li>Monitor provider performance and costs in production</li> <li>Choose models based on your specific use case requirements</li> </ul>"},{"location":"guides/providers/#next-steps","title":"Next Steps","text":"<ul> <li>Tools System - Add tool capabilities to your providers</li> <li>Architecture - Understanding the provider layer</li> <li>Routing - Route responses based on provider capabilities</li> </ul>"},{"location":"guides/routing/","title":"Message Routing","text":"<p>Route LLM responses to different recipients based on content, context, or custom logic.</p>"},{"location":"guides/routing/#routing-flow","title":"Routing Flow","text":"<pre><code>flowchart TD\n    A[User sends message] --&gt; B[LLM Agent receives message]\n    B --&gt; C[LLM generates response]\n    C --&gt; D[Routing function analyzes response]\n    D --&gt; E{Content analysis}\n    E --&gt;|Contains error| F[Route to Agent A&lt;br/&gt;Support Team]\n    E --&gt;|Contains price| G[Route to Agent B&lt;br/&gt;Sales Team]\n    E --&gt;|Multiple keywords| H[Route to multiple agents&lt;br/&gt;Agent A + Agent B]\n    E --&gt;|No specific keywords| I[Route to default&lt;br/&gt;General Support]\n    F --&gt; J[Message delivered]\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J</code></pre>"},{"location":"guides/routing/#overview","title":"Overview","text":"<p>Message routing enables you to automatically direct LLM responses to appropriate recipients:</p> <ul> <li> <p>Technical issues \u2192 Support team</p> </li> <li> <p>Sales inquiries \u2192 Sales team  </p> </li> <li> <p>General questions \u2192 General support</p> </li> <li> <p>Send to multiple recipients or transform messages before sending</p> </li> </ul>"},{"location":"guides/routing/#basic-routing","title":"Basic Routing","text":""},{"location":"guides/routing/#simple-routing-function","title":"Simple Routing Function","text":"<pre><code>from spade_llm import LLMAgent\n\ndef simple_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"error\" in response.lower():\n        return \"support@example.com\"\n    elif \"price\" in response.lower():\n        return \"sales@example.com\"\n    else:\n        return \"general@example.com\"\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"router@example.com\",\n    password=\"password\",\n    provider=provider,\n    routing_function=simple_router\n)\n</code></pre>"},{"location":"guides/routing/#function-signature","title":"Function Signature","text":"<pre><code>def routing_function(msg, response, context):\n    \"\"\"\n    Args:\n        msg: Original SPADE message\n        response: LLM response text  \n        context: Conversation context dict\n\n    Returns:\n        str: Single recipient JID\n        List[str]: Multiple recipients\n        RoutingResponse: Advanced routing\n        None: Send to original sender\n    \"\"\"\n    return \"recipient@example.com\"\n</code></pre>"},{"location":"guides/routing/#advanced-routing","title":"Advanced Routing","text":""},{"location":"guides/routing/#multiple-recipients","title":"Multiple Recipients","text":"<p>Send to several agents simultaneously:</p> <pre><code>def multi_router(msg, response, context):\n    \"\"\"Route to multiple recipients.\"\"\"\n    recipients = [\"primary@example.com\"]\n\n    # Copy errors to support\n    if \"error\" in response.lower():\n        recipients.append(\"support@example.com\")\n\n    # Copy sales inquiries to sales team\n    if \"price\" in response.lower():\n        recipients.append(\"sales@example.com\")\n\n    return recipients\n</code></pre>"},{"location":"guides/routing/#routingresponse","title":"RoutingResponse","text":"<p>For advanced routing with message transformation:</p> <pre><code>from spade_llm.routing import RoutingResponse\n\ndef advanced_router(msg, response, context):\n    \"\"\"Advanced routing with transformations.\"\"\"\n\n    def add_signature(text):\n        return f\"{text}\\n\\n--\\nProcessed by AI Assistant\"\n\n    return RoutingResponse(\n        recipients=\"customer@example.com\",\n        transform=add_signature,\n        metadata={\"processed_by\": \"my_agent\"}\n    )\n</code></pre>"},{"location":"guides/routing/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/routing/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>def content_router(msg, response, context):\n    \"\"\"Route based on keywords in response.\"\"\"\n    text = response.lower()\n\n    # Technical issues\n    if any(word in text for word in [\"error\", \"bug\", \"crash\", \"problem\"]):\n        return \"tech-support@example.com\"\n\n    # Sales inquiries\n    if any(word in text for word in [\"price\", \"cost\", \"buy\", \"purchase\"]):\n        return \"sales@example.com\"\n\n    # Billing questions\n    if any(word in text for word in [\"payment\", \"invoice\", \"billing\"]):\n        return \"billing@example.com\"\n\n    return \"general@example.com\"  # Default\n</code></pre>"},{"location":"guides/routing/#sender-based-routing","title":"Sender-Based Routing","text":"<pre><code>def sender_router(msg, response, context):\n    \"\"\"Route based on message sender.\"\"\"\n    sender = str(msg.sender)\n\n    # VIP users get priority support\n    vip_users = [\"ceo@company.com\", \"admin@company.com\"]\n    if sender in vip_users:\n        return \"vip-support@example.com\"\n\n    # Internal vs external users\n    if sender.endswith(\"@company.com\"):\n        return \"internal@example.com\"\n    else:\n        return \"external@example.com\"\n</code></pre>"},{"location":"guides/routing/#context-aware-routing","title":"Context-Aware Routing","text":"<pre><code>def context_router(msg, response, context):\n    \"\"\"Route based on conversation history.\"\"\"\n    state = context.get(\"state\", {})\n    interaction_count = state.get(\"interaction_count\", 0)\n\n    # Long conversations need escalation\n    if interaction_count &gt; 5:\n        return RoutingResponse(\n            recipients=\"escalation@example.com\",\n            metadata={\"reason\": \"long_conversation\"}\n        )\n\n    # New conversations to onboarding\n    if interaction_count &lt;= 1:\n        return \"onboarding@example.com\"\n\n    return \"standard@example.com\"\n</code></pre>"},{"location":"guides/routing/#workflow-routing","title":"Workflow Routing","text":""},{"location":"guides/routing/#sequential-processing","title":"Sequential Processing","text":"<pre><code>def workflow_router(msg, response, context):\n    \"\"\"Route through workflow steps.\"\"\"\n\n    if \"analysis complete\" in response.lower():\n        return \"review@example.com\"\n    elif \"review approved\" in response.lower():\n        return \"execution@example.com\"\n    elif \"execution finished\" in response.lower():\n        return \"completion@example.com\"\n    else:\n        return \"analysis@example.com\"  # Start workflow\n</code></pre>"},{"location":"guides/routing/#quick-tips","title":"Quick Tips","text":"<ul> <li>Keep logic simple: Complex routing is hard to debug</li> <li>Always have a default: Don't leave messages unrouted</li> <li>Return <code>None</code>: Sends back to original sender (useful for direct conversations)</li> <li>Use metadata: Add context for debugging and tracking</li> </ul> <pre><code>def router_with_fallback(msg, response, context):\n    # Your routing logic here...\n\n    # If no specific rule matches, return to sender\n    return None  # Sends back to original sender\n</code></pre>"},{"location":"guides/routing/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understanding message flow</li> <li>Providers - LLM provider configuration</li> <li>Tools System - Adding tool capabilities</li> </ul>"},{"location":"guides/tools-system/","title":"Tools System","text":"<p>Enable LLM agents to execute functions and interact with external services.</p>"},{"location":"guides/tools-system/#tool-calling-flow","title":"Tool Calling Flow","text":"<pre><code>flowchart TD\n    A[User sends message] --&gt; B{Does LLM need tools?}\n    B --&gt;|No| C[Generate direct response]\n    B --&gt;|Yes| D[Decide which tool to use]\n    D --&gt; E[Determine required arguments]\n    E --&gt; F[Send tool_calls in response]\n    F --&gt; G[System executes tool]\n    G --&gt; H[Add results to context]\n    H --&gt; I[Second LLM query with results]\n    I --&gt; J[Generate final response]\n    C --&gt; K[Send response to user]\n    J --&gt; K</code></pre>"},{"location":"guides/tools-system/#overview","title":"Overview","text":"<p>The Tools System empowers LLM agents to extend beyond conversation by executing real functions. This enables agents to:</p> <ul> <li>\ud83d\udd27 Execute Python functions with dynamic parameters</li> <li>\ud83c\udf10 Access external APIs and databases  </li> <li>\ud83d\udcc1 Process files and perform calculations</li> <li>\ud83d\udd17 Integrate with third-party services</li> </ul>"},{"location":"guides/tools-system/#how-tool-calling-works","title":"How Tool Calling Works","text":"<p>When an LLM agent receives a message, it can either respond directly or decide to use tools. The process involves:</p> <ol> <li>Intelligence Decision: The LLM analyzes if it needs external data or functionality</li> <li>Tool Selection: It chooses the appropriate tool from available options</li> <li>Parameter Generation: The LLM determines what arguments the tool needs</li> <li>Execution: The system runs the tool function asynchronously</li> <li>Context Integration: Results are added back to the conversation</li> <li>Final Response: The LLM processes results and provides a complete answer</li> </ol>"},{"location":"guides/tools-system/#basic-tool-definition","title":"Basic Tool Definition","text":"<pre><code>from spade_llm import LLMTool\n\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Weather in {city}: 22\u00b0C, sunny\"\n\nweather_tool = LLMTool(\n    name=\"get_weather\",\n    description=\"Get current weather for a city\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n        },\n        \"required\": [\"city\"]\n    },\n    func=get_weather\n)\n</code></pre>"},{"location":"guides/tools-system/#using-tools-with-agents","title":"Using Tools with Agents","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[weather_tool]  # Register tools\n)\n</code></pre> <p>When the LLM needs weather information, it will automatically detect the need and call the tool.</p>"},{"location":"guides/tools-system/#common-tool-categories","title":"Common Tool Categories","text":""},{"location":"guides/tools-system/#api-integration","title":"\ud83c\udf10 API Integration","text":"<p>Connect to external web services for real-time data.</p> <pre><code>import aiohttp\n\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(f\"https://api.duckduckgo.com/?q={query}&amp;format=json\") as response:\n            data = await response.json()\n            return str(data)\n\nsearch_tool = LLLTool(\n    name=\"web_search\",\n    description=\"Search the web for current information\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\"type\": \"string\"}\n        },\n        \"required\": [\"query\"]\n    },\n    func=web_search\n)\n</code></pre>"},{"location":"guides/tools-system/#file-operations","title":"\ud83d\udcc1 File Operations","text":"<p>Read, write, and process files on the system.</p> <pre><code>import aiofiles\n\nasync def read_file(filepath: str) -&gt; str:\n    \"\"\"Read a text file.\"\"\"\n    try:\n        async with aiofiles.open(filepath, 'r') as f:\n            content = await f.read()\n        return f\"File content:\\n{content}\"\n    except Exception as e:\n        return f\"Error reading file: {e}\"\n\nfile_tool = LLMTool(\n    name=\"read_file\",\n    description=\"Read contents of a text file\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"filepath\": {\"type\": \"string\"}\n        },\n        \"required\": [\"filepath\"]\n    },\n    func=read_file\n)\n</code></pre>"},{"location":"guides/tools-system/#data-processing","title":"\ud83d\udcca Data Processing","text":"<p>Perform calculations and data analysis.</p> <pre><code>import json\n\nasync def calculate_stats(numbers: list) -&gt; str:\n    \"\"\"Calculate statistics for a list of numbers.\"\"\"\n    if not numbers:\n        return \"Error: No numbers provided\"\n\n    stats = {\n        \"count\": len(numbers),\n        \"mean\": sum(numbers) / len(numbers),\n        \"min\": min(numbers),\n        \"max\": max(numbers)\n    }\n    return json.dumps(stats, indent=2)\n\nstats_tool = LLMTool(\n    name=\"calculate_stats\",\n    description=\"Calculate basic statistics\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"numbers\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"number\"}\n            }\n        },\n        \"required\": [\"numbers\"]\n    },\n    func=calculate_stats\n)\n</code></pre>"},{"location":"guides/tools-system/#human-expert-consultation","title":"\ud83e\udde0 Human Expert Consultation","text":"<p>Connect LLM agents with human experts for real-time guidance and decision support.</p> <pre><code>from spade_llm.tools import HumanInTheLoopTool\n\n# Create human expert consultation tool\nhuman_expert = HumanInTheLoopTool(\n    human_expert_jid=\"expert@company.com\",\n    timeout=300.0,  # 5 minutes\n    name=\"ask_human_expert\",\n    description=\"\"\"Ask a human expert for help when you need:\n    - Current information not in your training data\n    - Human judgment or subjective opinions\n    - Company-specific policies or procedures\n    - Clarification on ambiguous requests\"\"\"\n)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"assistant@company.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[human_expert],\n    system_prompt=\"\"\"You are an AI assistant with access to human experts.\n    When you encounter questions requiring human judgment, current information,\n    or company-specific knowledge, consult the human expert.\"\"\"\n)\n</code></pre> <p>Key Features:</p> <ul> <li>\u26a1 Real-time consultation via XMPP messaging</li> <li>\ud83c\udf10 Web interface for human experts to respond</li> <li>\ud83d\udd04 Message correlation using XMPP thread IDs</li> <li>\u23f1\ufe0f Configurable timeouts with graceful error handling</li> <li>\ud83d\udd12 Template-based filtering prevents message conflicts</li> </ul> <p>When the LLM uses this tool:</p> <ol> <li>Question sent to human expert via XMPP</li> <li>Expert receives notification in web interface  </li> <li>Human provides response through browser</li> <li>Response returns to LLM via XMPP</li> <li>Agent continues with human-informed answer</li> </ol> <p>Example consultation flow: <pre><code>User: \"What's our company policy on remote work?\"\nAgent: [Uses ask_human_expert tool]\n\u2192 Human Expert: \"We allow 3 days remote per week with manager approval\"\nAgent: \"According to our HR expert, our policy allows up to 3 days \n       remote work per week with manager approval.\"\n</code></pre></p> <p>Setup Required</p> <p>Human-in-the-loop requires XMPP server with WebSocket support and web interface. See the working example in <code>examples/human_in_the_loop_example.py</code> for complete setup instructions.</p>"},{"location":"guides/tools-system/#langchain-integration","title":"LangChain Integration","text":"<p>Seamlessly use existing LangChain tools with SPADE_LLM:</p> <pre><code>from langchain_community.tools import DuckDuckGoSearchRun\nfrom spade_llm.tools import LangChainToolAdapter\n\n# Create LangChain tool\nsearch_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM  \nsearch_tool = LangChainToolAdapter(search_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"guides/tools-system/#best-practices","title":"\u2705 Best Practices","text":"<ul> <li>Single Purpose: Each tool should do one thing well</li> <li>Clear Naming: Use descriptive tool names that explain functionality</li> <li>Rich Descriptions: Help the LLM understand when and how to use tools</li> <li>Input Validation: Always validate and sanitize inputs for security</li> <li>Meaningful Errors: Return clear error messages for troubleshooting</li> <li>Async Functions: Use async/await for non-blocking execution</li> </ul>"},{"location":"guides/tools-system/#next-steps","title":"Next Steps","text":"<ul> <li>MCP Integration - Connect to external MCP servers</li> <li>Architecture - Understanding system design</li> <li>Providers - LLM provider configuration</li> </ul>"},{"location":"includes/mkdocs/","title":"Mkdocs","text":"<p>Production Use</p> <p>This feature is still in beta. While functional, it may change in future versions. Use with caution in production environments.</p> <p>Async Required</p> <p>This method is asynchronous and must be awaited when called from async code.</p> <p>Performance Tip</p> <p>For better performance, consider implementing connection pooling or caching for frequently accessed resources.</p> <p>Basic Example</p> <p>Here's a simple example to get you started:</p> <p>Compatibility</p> <p>This feature requires Python 3.10+ and SPADE 3.3.0+.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Complete API documentation for SPADE_LLM components.</p>"},{"location":"reference/#core-components","title":"Core Components","text":"<ul> <li>Agent - LLMAgent and ChatAgent classes</li> <li>Behaviour - LLMBehaviour implementation  </li> <li>Providers - LLM provider interfaces</li> <li>Tools - Tool system and LLMTool class</li> <li>Memory - Memory system API for agent learning and persistence</li> <li>Human Interface - Human-in-the-loop API and integration</li> <li>Guardrails - Content filtering and safety controls</li> <li>Context - Context and conversation management</li> <li>Routing - Message routing system</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#creating-agents","title":"Creating Agents","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\nagent = LLMAgent(jid=\"agent@server.com\", password=\"pass\", provider=provider)\n</code></pre>"},{"location":"reference/#creating-tools","title":"Creating Tools","text":"<pre><code>from spade_llm import LLMTool\n\nasync def my_function(param: str) -&gt; str:\n    return f\"Result: {param}\"\n\ntool = LLMTool(\n    name=\"my_function\",\n    description=\"Description of function\",\n    parameters={\"type\": \"object\", \"properties\": {\"param\": {\"type\": \"string\"}}, \"required\": [\"param\"]},\n    func=my_function\n)\n</code></pre>"},{"location":"reference/#message-routing","title":"Message Routing","text":"<pre><code>def router(msg, response, context):\n    if \"technical\" in response.lower():\n        return \"tech@example.com\"\n    return str(msg.sender)\n\nagent = LLMAgent(..., routing_function=router)\n</code></pre>"},{"location":"reference/#examples","title":"Examples","text":"<p>See Examples for complete working code examples.</p>"},{"location":"reference/#type-definitions","title":"Type Definitions","text":""},{"location":"reference/#common-types","title":"Common Types","text":"<pre><code># Message context\nContextMessage = Union[SystemMessage, UserMessage, AssistantMessage, ToolResultMessage]\n\n# Routing result\nRoutingResult = Union[str, List[str], RoutingResponse, None]\n\n# Tool parameters\nToolParameters = Dict[str, Any]  # JSON Schema format\n</code></pre>"},{"location":"reference/#error-handling","title":"Error Handling","text":"<p>All SPADE_LLM components use standard Python exceptions:</p> <ul> <li><code>ValueError</code> - Invalid parameters or configuration</li> <li><code>ConnectionError</code> - Network or provider connection issues  </li> <li><code>TimeoutError</code> - Operations that exceed timeout limits</li> <li><code>RuntimeError</code> - General runtime errors</li> </ul>"},{"location":"reference/#configuration","title":"Configuration","text":""},{"location":"reference/#environment-variables","title":"Environment Variables","text":"<pre><code>OPENAI_API_KEY=your-api-key\nOLLAMA_BASE_URL=http://localhost:11434/v1\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\n</code></pre>"},{"location":"reference/#provider-configuration","title":"Provider Configuration","text":"<pre><code># OpenAI\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\n\n# Ollama  \nprovider = LLMProvider.create_ollama(model=\"llama3.1:8b\")\n\n# LM Studio\nprovider = LLMProvider.create_lm_studio(model=\"local-model\")\n</code></pre> <p>For detailed API documentation, see the individual component pages.</p>"},{"location":"reference/examples/","title":"Examples","text":"<p>Complete working examples for SPADE_LLM applications.</p>"},{"location":"reference/examples/#repository-examples","title":"Repository Examples","text":"<p>The examples directory contains complete working examples:</p> <ul> <li><code>multi_provider_chat_example.py</code> - Chat with different LLM providers</li> <li><code>ollama_with_tools_example.py</code> - Local models with tool calling</li> <li><code>langchain_tools_example.py</code> - LangChain tool integration</li> <li><code>valencia_multiagent_trip_planner.py</code> - Multi-agent workflow</li> <li><code>spanish_to_english_translator.py</code> - Translation agent</li> <li><code>human_in_the_loop_example.py</code> - LLM agent with human expert consultation</li> <li><code>simple_coordinator_example.py</code> - CoordinatorAgent directing multiple SPADE subagents</li> </ul>"},{"location":"reference/examples/#human-in-the-loop-example","title":"Human-in-the-Loop Example","text":"<p>Complete example demonstrating LLM agents consulting with human experts:</p> <pre><code>\"\"\"\nExample: LLM Agent with Human Expert Consultation\n\nThis example shows how to create an agent that can ask human experts\nfor help when it needs current information or human judgment.\n\nPrerequisites:\n1. XMPP server with WebSocket support (e.g., OpenFire)\n2. Human expert web interface running\n3. XMPP accounts for agent, expert, and chat user\n\"\"\"\n\nimport asyncio\nimport logging\nimport spade\nfrom spade_llm.agent import LLMAgent, ChatAgent\nfrom spade_llm.tools import HumanInTheLoopTool\nfrom spade_llm.providers import LLMProvider\nfrom spade_llm.utils import load_env_vars\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\nasync def main():\n    # Load environment variables\n    env_vars = load_env_vars()\n\n    # Configuration\n    XMPP_SERVER = \"localhost\"  # or your XMPP server\n    AGENT_JID = f\"agent@{XMPP_SERVER}\"\n    EXPERT_JID = f\"expert@{XMPP_SERVER}\"\n    USER_JID = f\"user@{XMPP_SERVER}\"\n\n    # Create OpenAI provider\n    provider = LLMProvider.create_openai(\n        api_key=env_vars[\"OPENAI_API_KEY\"],\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # System prompt encouraging human consultation\n    system_prompt = \"\"\"You are an AI assistant with access to human experts.\n\n    When you need:\n    - Current information not in your training data\n    - Human judgment or opinions\n    - Company-specific information\n    - Clarification on ambiguous requests\n\n    Use the ask_human_expert tool to consult with human experts.\"\"\"\n\n    # Create human consultation tool\n    human_tool = HumanInTheLoopTool(\n        human_expert_jid=EXPERT_JID,\n        timeout=300.0,  # 5 minutes\n        name=\"ask_human_expert\",\n        description=\"Ask human expert for current info or clarification\"\n    )\n\n    # Create LLM agent with human tool\n    agent = LLMAgent(\n        jid=AGENT_JID,\n        password=\"agent_password\",\n        provider=provider,\n        system_prompt=system_prompt,\n        tools=[human_tool],\n        verify_security=False\n    )\n\n    # Create chat interface for testing\n    chat_agent = ChatAgent(\n        jid=USER_JID,\n        password=\"user_password\",\n        target_agent_jid=AGENT_JID,\n        verify_security=False\n    )\n\n    # Start agents\n    await agent.start()\n    await chat_agent.start()\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Human-in-the-Loop Example Running\")\n    print(\"=\"*50)\n    print(f\"Agent: {AGENT_JID}\")\n    print(f\"Expert: {EXPERT_JID}\")\n    print(f\"User: {USER_JID}\")\n    print(\"\\n\ud83d\udccb Try these questions:\")\n    print(\"\u2022 'What's the current weather in Madrid?'\")\n    print(\"\u2022 'Should we proceed with the new project?'\")\n    print(\"\u2022 'What's our company WiFi password?'\")\n    print(\"\\n\ud83c\udf10 Make sure human expert is connected at:\")\n    print(\"   http://localhost:8080\")\n    print(\"\\n\ud83d\udcac Type 'exit' to quit\\n\")\n\n    # Run interactive chat\n    try:\n        await chat_agent.run_interactive(\n            input_prompt=\"You: \",\n            exit_command=\"exit\",\n            response_timeout=120.0\n        )\n    except KeyboardInterrupt:\n        pass\n    finally:\n        await chat_agent.stop()\n        await agent.stop()\n\nif __name__ == \"__main__\":\n    print(\"Starting Human-in-the-Loop example...\")\n    print(\"Make sure to start the human expert interface:\")\n    print(\"  python -m spade_llm.human_interface.web_server\")\n    print()\n    spade.run(main())\n</code></pre> <p>Key Features Demonstrated:</p> <ul> <li>\ud83e\udde0 Human Expert Tool: Seamless integration with <code>HumanInTheLoopTool</code></li> <li>\u26a1 Real-time Communication: XMPP messaging between agent and human</li> <li>\ud83c\udf10 Web Interface: Browser-based interface for human experts</li> <li>\ud83d\udd04 Message Correlation: Thread-based message routing</li> <li>\u23f1\ufe0f Timeout Handling: Graceful handling of delayed responses</li> </ul> <p>Setup Instructions:</p> <ol> <li>Start web interface: <code>python -m spade_llm.human_interface.web_server</code></li> <li>Open browser: Go to <code>http://localhost:8080</code></li> <li>Connect as expert: Use expert credentials to connect</li> <li>Run example: Execute the Python script</li> <li>Test consultation: Ask questions that require human input</li> </ol> <p>See the working example in <code>examples/human_in_the_loop_example.py</code> for complete setup instructions.</p>"},{"location":"reference/examples/#basic-examples","title":"Basic Examples","text":""},{"location":"reference/examples/#simple-chat-agent","title":"Simple Chat Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\nasync def main():\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create LLM agent\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    # Create chat interface\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\",\n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    # Start agents\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Type messages to chat. Enter 'exit' to quit.\")\n    await chat_agent.run_interactive()\n\n    # Cleanup\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#tool-enabled-agent","title":"Tool-Enabled Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider, LLMTool\nfrom datetime import datetime\nimport requests\n\n# Tool functions\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather information for a city.\"\"\"\n    # Simplified weather API call\n    try:\n        response = requests.get(f\"http://api.weatherapi.com/v1/current.json?key=YOUR_KEY&amp;q={city}\")\n        data = response.json()\n        return f\"Weather in {city}: {data['current']['temp_c']}\u00b0C, {data['current']['condition']['text']}\"\n    except:\n        return f\"Could not get weather for {city}\"\n\nasync def get_time() -&gt; str:\n    \"\"\"Get current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nasync def main():\n    # Create tools\n    weather_tool = LLMTool(\n        name=\"get_weather\",\n        description=\"Get current weather for a city\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"city\"]\n        },\n        func=get_weather\n    )\n\n    time_tool = LLMTool(\n        name=\"get_time\",\n        description=\"Get current date and time\",\n        parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n        func=get_time\n    )\n\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create agent with tools\n    agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with access to weather and time information\",\n        tools=[weather_tool, time_tool]\n    )\n\n    await agent.start()\n    print(\"Agent with tools started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#multi-agent-workflow","title":"Multi-Agent Workflow","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\n# Routing functions\ndef analyzer_router(msg, response, context):\n    \"\"\"Route analysis results to reviewer.\"\"\"\n    if \"analysis complete\" in response.lower():\n        return \"reviewer@jabber.at\"\n    return str(msg.sender)\n\ndef reviewer_router(msg, response, context):\n    \"\"\"Route review results to executor.\"\"\"\n    if \"approved\" in response.lower():\n        return \"executor@jabber.at\"\n    elif \"rejected\" in response.lower():\n        return \"analyzer@jabber.at\"  # Send back for revision\n    return str(msg.sender)\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Analyzer agent\n    analyzer = LLMAgent(\n        jid=\"analyzer@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You analyze requests and provide detailed analysis. End with 'Analysis complete.'\",\n        routing_function=analyzer_router\n    )\n\n    # Reviewer agent  \n    reviewer = LLMAgent(\n        jid=\"reviewer@jabber.at\",\n        password=\"password2\",\n        provider=provider,\n        system_prompt=\"You review analysis and either approve or reject. Say 'Approved' or 'Rejected'.\",\n        routing_function=reviewer_router\n    )\n\n    # Executor agent\n    executor = LLMAgent(\n        jid=\"executor@jabber.at\",\n        password=\"password3\",\n        provider=provider,\n        system_prompt=\"You execute approved plans and report completion.\"\n    )\n\n    # Start all agents\n    await analyzer.start()\n    await reviewer.start() \n    await executor.start()\n\n    print(\"Multi-agent workflow started!\")\n    print(\"Send a request to analyzer@jabber.at to start the workflow\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(120)\n\n    # Cleanup\n    await analyzer.stop()\n    await reviewer.stop()\n    await executor.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#local-model-with-ollama","title":"Local Model with Ollama","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Create Ollama provider\n    provider = LLMProvider.create_ollama(\n        model=\"llama3.1:8b\",\n        base_url=\"http://localhost:11434/v1\",\n        temperature=0.7,\n        timeout=120.0\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"local-agent@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant running on a local model\"\n    )\n\n    await agent.start()\n    print(\"Local Ollama agent started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#conversation-management","title":"Conversation Management","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\ndef conversation_ended(conversation_id: str, reason: str):\n    \"\"\"Handle conversation end.\"\"\"\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n    # Save conversation, send notifications, etc.\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    agent = LLMAgent(\n        jid=\"managed-agent@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant. Say 'DONE' when tasks are complete.\",\n        max_interactions_per_conversation=5,  # Limit conversation length\n        termination_markers=[\"DONE\", \"COMPLETE\", \"FINISHED\"],\n        on_conversation_end=conversation_ended\n    )\n\n    await agent.start()\n    print(\"Agent with conversation management started!\")\n\n    # Test conversation state\n    import asyncio\n    await asyncio.sleep(30)\n\n    # Check conversation states\n    # In a real application, you'd have actual conversation IDs\n    print(\"Active conversations:\", len(agent.context.get_active_conversations()))\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"reference/examples/#with-langchain-tools","title":"With LangChain Tools","text":"<pre><code>from langchain_community.tools import DuckDuckGoSearchRun\nfrom spade_llm.tools import LangChainToolAdapter\n\n# Create LangChain tool\nsearch_tool_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM\nsearch_tool = LangChainToolAdapter(search_tool_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"search-agent@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a research assistant with web search capabilities\",\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"reference/examples/#running-examples","title":"Running Examples","text":"<ol> <li>Install dependencies: <code>pip install spade_llm</code></li> <li>Set environment variables: <code>export OPENAI_API_KEY=\"your-key\"</code></li> <li>Run example: <code>python example.py</code></li> </ol>"},{"location":"reference/examples/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/examples/#environment-configuration","title":"Environment Configuration","text":"<pre><code>import os\nfrom spade_llm.utils import load_env_vars\n\n# Load .env file\nload_env_vars()\n\n# Use environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n</code></pre> <p>For more examples, check the examples directory in the SPADE_LLM repository.</p>"},{"location":"reference/api/agent/","title":"Agent API","text":"<p>API reference for SPADE_LLM agent classes.</p>"},{"location":"reference/api/agent/#llmagent","title":"LLMAgent","text":"<p>Main agent class that extends SPADE Agent with LLM capabilities.</p>"},{"location":"reference/api/agent/#constructor","title":"Constructor","text":"<pre><code>LLMAgent(\n    jid: str,\n    password: str,\n    provider: LLMProvider,\n    reply_to: Optional[str] = None,\n    routing_function: Optional[RoutingFunction] = None,\n    system_prompt: Optional[str] = None,\n    mcp_servers: Optional[List[MCPServerConfig]] = None,\n    tools: Optional[List[LLMTool]] = None,\n    termination_markers: Optional[List[str]] = None,\n    max_interactions_per_conversation: Optional[int] = None,\n    on_conversation_end: Optional[Callable[[str, str], None]] = None,\n    verify_security: bool = False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>jid</code> - Jabber ID for the agent</li> <li><code>password</code> - Agent password  </li> <li><code>provider</code> - LLM provider instance</li> <li><code>reply_to</code> - Optional fixed reply destination</li> <li><code>routing_function</code> - Custom routing function</li> <li><code>system_prompt</code> - System instructions for LLM</li> <li><code>tools</code> - List of available tools</li> <li><code>termination_markers</code> - Conversation end markers</li> <li><code>max_interactions_per_conversation</code> - Conversation length limit</li> <li><code>on_conversation_end</code> - Callback when conversation ends</li> <li><code>verify_security</code> - Enable SSL verification</li> </ul>"},{"location":"reference/api/agent/#methods","title":"Methods","text":""},{"location":"reference/api/agent/#add_tooltool-llmtool","title":"add_tool(tool: LLMTool)","text":"<p>Add a tool to the agent.</p> <pre><code>tool = LLMTool(name=\"function\", description=\"desc\", parameters={}, func=my_func)\nagent.add_tool(tool)\n</code></pre>"},{"location":"reference/api/agent/#get_tools-listllmtool","title":"get_tools() -&gt; List[LLMTool]","text":"<p>Get all registered tools.</p> <pre><code>tools = agent.get_tools()\nprint(f\"Agent has {len(tools)} tools\")\n</code></pre>"},{"location":"reference/api/agent/#reset_conversationconversation_id-str-bool","title":"reset_conversation(conversation_id: str) -&gt; bool","text":"<p>Reset conversation limits.</p> <pre><code>success = agent.reset_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/agent/#get_conversation_stateconversation_id-str-optionaldictstr-any","title":"get_conversation_state(conversation_id: str) -&gt; Optional[Dict[str, Any]]","text":"<p>Get conversation state information.</p> <pre><code>state = agent.get_conversation_state(\"user1_session\")\nif state:\n    print(f\"Interactions: {state['interaction_count']}\")\n</code></pre>"},{"location":"reference/api/agent/#example","title":"Example","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10\n)\n\nawait agent.start()\n</code></pre>"},{"location":"reference/api/agent/#chatagent","title":"ChatAgent","text":"<p>Interactive chat agent for human-computer communication.</p>"},{"location":"reference/api/agent/#constructor_1","title":"Constructor","text":"<pre><code>ChatAgent(\n    jid: str,\n    password: str,\n    target_agent_jid: str,\n    display_callback: Optional[Callable[[str, str], None]] = None,\n    on_message_sent: Optional[Callable[[str, str], None]] = None,\n    on_message_received: Optional[Callable[[str, str], None]] = None,\n    verbose: bool = False,\n    verify_security: bool = False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target_agent_jid</code> - JID of agent to communicate with</li> <li><code>display_callback</code> - Custom response display function  </li> <li><code>on_message_sent</code> - Callback after sending message</li> <li><code>on_message_received</code> - Callback after receiving response</li> <li><code>verbose</code> - Enable detailed logging</li> </ul>"},{"location":"reference/api/agent/#methods_1","title":"Methods","text":""},{"location":"reference/api/agent/#send_messagemessage-str","title":"send_message(message: str)","text":"<p>Send message to target agent.</p> <pre><code>chat_agent.send_message(\"Hello, how are you?\")\n</code></pre>"},{"location":"reference/api/agent/#send_message_asyncmessage-str","title":"send_message_async(message: str)","text":"<p>Send message asynchronously.</p> <pre><code>await chat_agent.send_message_async(\"Hello!\")\n</code></pre>"},{"location":"reference/api/agent/#wait_for_responsetimeout-float-100-bool","title":"wait_for_response(timeout: float = 10.0) -&gt; bool","text":"<p>Wait for response from target agent.</p> <pre><code>received = await chat_agent.wait_for_response(timeout=30.0)\n</code></pre>"},{"location":"reference/api/agent/#run_interactive","title":"run_interactive()","text":"<p>Start interactive chat session.</p> <pre><code>await chat_agent.run_interactive()  # Starts interactive chat\n</code></pre>"},{"location":"reference/api/agent/#example_1","title":"Example","text":"<pre><code>from spade_llm import ChatAgent\n\ndef display_response(message: str, sender: str):\n    print(f\"Response: {message}\")\n\nchat_agent = ChatAgent(\n    jid=\"human@example.com\",\n    password=\"password\",\n    target_agent_jid=\"assistant@example.com\",\n    display_callback=display_response\n)\n\nawait chat_agent.start()\nawait chat_agent.run_interactive()  # Interactive chat\nawait chat_agent.stop()\n</code></pre>"},{"location":"reference/api/agent/#coordinatoragent","title":"CoordinatorAgent","text":"<p>Specialized agent for orchestrating multiple SPADE subagents through a single LLM-driven coordinator.</p>"},{"location":"reference/api/agent/#constructor_2","title":"Constructor","text":"<pre><code>CoordinatorAgent(\n    jid: str,\n    password: str,\n    subagent_ids: List[str],\n    coordination_session: str = \"main_coordination\",\n    provider: LLMProvider,\n    routing_function: Optional[RoutingFunction] = None,\n    **kwargs\n)\n</code></pre> <p>Parameters (in addition to <code>LLMAgent</code>):</p> <ul> <li><code>subagent_ids</code> \u2014 JIDs of the subagents managed by the coordinator (required).</li> <li><code>coordination_session</code> \u2014 Thread identifier shared by all internal exchanges.</li> <li><code>_response_timeout</code> (attribute) \u2014 Maximum time (seconds) the coordinator waits for a subagent reply (default <code>30.0</code>).</li> </ul>"},{"location":"reference/api/agent/#coordination-tools","title":"Coordination Tools","text":"<p>Two tools are registered automatically during <code>setup()</code>:</p> Tool Purpose <code>send_to_agent(agent_id: str, command: str) -&gt; str</code> Sends a command to a registered subagent and waits for the reply within the coordination timeout. <code>list_subagents() -&gt; str</code> Returns the current list of subagents and their last known status."},{"location":"reference/api/agent/#behaviour","title":"Behaviour","text":"<ul> <li>All messages from or to managed subagents are forced into the shared <code>coordination_session</code>, giving the LLM full visibility of the organizational state.</li> <li>Custom routing ensures intermediate messages stay inside the organization, while termination markers (<code>&lt;TASK_COMPLETE&gt;</code>, <code>&lt;END&gt;</code>, <code>&lt;DONE&gt;</code>) trigger delivery to the original requester.</li> <li>Subagent status is tracked automatically (<code>unknown</code>, <code>sent_command</code>, <code>responded</code>, <code>timeout</code>), allowing the LLM to plan next steps.</li> </ul>"},{"location":"reference/api/agent/#example_2","title":"Example","text":"<pre><code>from spade_llm.agent import CoordinatorAgent\nfrom spade_llm.providers import LLMProvider\n\nsubagents = [\n    \"traffic-analyzer@xmpp.local\",\n    \"notification-service@xmpp.local\",\n]\n\ncoordinator = CoordinatorAgent(\n    jid=\"city-coordinator@xmpp.local\",\n    password=\"secret\",\n    subagent_ids=subagents,\n    provider=LLMProvider.create_openai(\n        api_key=\"sk-...\",\n        model=\"gpt-4o-mini\",\n    ),\n    coordination_session=\"city_ops\"\n)\nawait coordinator.start()\n</code></pre>"},{"location":"reference/api/agent/#agent-lifecycle","title":"Agent Lifecycle","text":""},{"location":"reference/api/agent/#starting-agents","title":"Starting Agents","text":"<pre><code>await agent.start()  # Initialize and connect\n</code></pre>"},{"location":"reference/api/agent/#stopping-agents","title":"Stopping Agents","text":"<pre><code>await agent.stop()  # Cleanup and disconnect\n</code></pre>"},{"location":"reference/api/agent/#running-with-spade","title":"Running with SPADE","text":"<pre><code>import spade\n\nasync def main():\n    agent = LLMAgent(...)\n    await agent.start()\n    # Agent runs until stopped\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/api/agent/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    await agent.start()\nexcept ConnectionError:\n    print(\"Failed to connect to XMPP server\")\nexcept ValueError:\n    print(\"Invalid configuration\")\n</code></pre>"},{"location":"reference/api/agent/#best-practices","title":"Best Practices","text":"<ul> <li>Always call <code>start()</code> before using agents</li> <li>Use <code>stop()</code> for proper cleanup</li> <li>Handle connection errors gracefully</li> <li>Set appropriate conversation limits</li> <li>Use callbacks for monitoring</li> </ul>"},{"location":"reference/api/behaviour/","title":"Behaviour API","text":"<p>API reference for SPADE_LLM behaviour classes.</p>"},{"location":"reference/api/behaviour/#llmbehaviour","title":"LLMBehaviour","text":"<p>Core behaviour that handles LLM interaction loop.</p>"},{"location":"reference/api/behaviour/#constructor","title":"Constructor","text":"<pre><code>LLMBehaviour(\n    llm_provider: LLMProvider,\n    reply_to: Optional[str] = None,\n    routing_function: Optional[RoutingFunction] = None,\n    context_manager: Optional[ContextManager] = None,\n    termination_markers: Optional[List[str]] = None,\n    max_interactions_per_conversation: Optional[int] = None,\n    on_conversation_end: Optional[Callable[[str, str], None]] = None,\n    tools: Optional[List[LLMTool]] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_provider</code> - LLM provider instance</li> <li><code>reply_to</code> - Fixed reply destination (optional)</li> <li><code>routing_function</code> - Custom routing function</li> <li><code>context_manager</code> - Context manager instance</li> <li><code>termination_markers</code> - Conversation end markers</li> <li><code>max_interactions_per_conversation</code> - Interaction limit</li> <li><code>on_conversation_end</code> - End callback</li> <li><code>tools</code> - Available tools</li> </ul>"},{"location":"reference/api/behaviour/#methods","title":"Methods","text":""},{"location":"reference/api/behaviour/#register_tooltool-llmtool","title":"register_tool(tool: LLMTool)","text":"<p>Register a tool with the behaviour.</p> <pre><code>tool = LLMTool(name=\"func\", description=\"desc\", parameters={}, func=my_func)\nbehaviour.register_tool(tool)\n</code></pre>"},{"location":"reference/api/behaviour/#get_tools-listllmtool","title":"get_tools() -&gt; List[LLMTool]","text":"<p>Get registered tools.</p> <pre><code>tools = behaviour.get_tools()\n</code></pre>"},{"location":"reference/api/behaviour/#reset_conversationconversation_id-str-bool","title":"reset_conversation(conversation_id: str) -&gt; bool","text":"<p>Reset conversation state.</p> <pre><code>success = behaviour.reset_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/behaviour/#get_conversation_stateconversation_id-str-optionaldictstr-any","title":"get_conversation_state(conversation_id: str) -&gt; Optional[Dict[str, Any]]","text":"<p>Get conversation state.</p> <pre><code>state = behaviour.get_conversation_state(\"user1_session\")\n</code></pre>"},{"location":"reference/api/behaviour/#processing-loop","title":"Processing Loop","text":"<p>The behaviour automatically:</p> <ol> <li>Receives XMPP messages</li> <li>Updates conversation context</li> <li>Calls LLM provider</li> <li>Executes requested tools</li> <li>Routes responses</li> </ol>"},{"location":"reference/api/behaviour/#conversation-states","title":"Conversation States","text":"<pre><code>class ConversationState:\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    ERROR = \"error\"\n    TIMEOUT = \"timeout\"\n    MAX_INTERACTIONS_REACHED = \"max_interactions_reached\"\n</code></pre>"},{"location":"reference/api/behaviour/#example","title":"Example","text":"<pre><code>from spade_llm.behaviour import LLMBehaviour\nfrom spade_llm.context import ContextManager\n\ncontext = ContextManager(system_prompt=\"You are helpful\")\nbehaviour = LLMBehaviour(\n    llm_provider=provider,\n    context_manager=context,\n    termination_markers=[\"DONE\", \"END\"],\n    max_interactions_per_conversation=10\n)\n\n# Used internally by LLMAgent\nagent.add_behaviour(behaviour)\n</code></pre>"},{"location":"reference/api/behaviour/#internal-architecture","title":"Internal Architecture","text":""},{"location":"reference/api/behaviour/#message-processing-flow","title":"Message Processing Flow","text":"<pre><code>async def run(self):\n    \"\"\"Main processing loop.\"\"\"\n    msg = await self.receive(timeout=10)\n    if not msg:\n        return\n\n    # Update context\n    self.context.add_message(msg, conversation_id)\n\n    # Process with LLM\n    await self._process_message_with_llm(msg, conversation_id)\n</code></pre>"},{"location":"reference/api/behaviour/#tool-execution-loop","title":"Tool Execution Loop","text":"<pre><code>async def _process_message_with_llm(self, msg, conversation_id):\n    \"\"\"Process message with tool execution.\"\"\"\n    max_iterations = 20\n    current_iteration = 0\n\n    while current_iteration &lt; max_iterations:\n        response = await self.provider.get_llm_response(self.context, self.tools)\n        tool_calls = response.get('tool_calls', [])\n\n        if not tool_calls:\n            # Final response\n            break\n\n        # Execute tools\n        for tool_call in tool_calls:\n            await self._execute_tool(tool_call)\n\n        current_iteration += 1\n</code></pre>"},{"location":"reference/api/behaviour/#conversation-lifecycle","title":"Conversation Lifecycle","text":"<pre><code>def _end_conversation(self, conversation_id: str, reason: str):\n    \"\"\"End conversation and cleanup.\"\"\"\n    self._active_conversations[conversation_id][\"state\"] = reason\n\n    if self.on_conversation_end:\n        self.on_conversation_end(conversation_id, reason)\n</code></pre>"},{"location":"reference/api/behaviour/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/api/behaviour/#custom-behaviour","title":"Custom Behaviour","text":"<pre><code>class CustomLLMBehaviour(LLMBehaviour):\n    async def run(self):\n        \"\"\"Custom processing logic.\"\"\"\n        # Pre-processing\n        await self.custom_preprocessing()\n\n        # Standard processing\n        await super().run()\n\n        # Post-processing\n        await self.custom_postprocessing()\n\n    async def custom_preprocessing(self):\n        \"\"\"Custom preprocessing.\"\"\"\n        pass\n\n    async def custom_postprocessing(self):\n        \"\"\"Custom postprocessing.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/behaviour/#direct-usage","title":"Direct Usage","text":"<pre><code># Rarely used directly - usually through LLMAgent\nfrom spade.agent import Agent\nfrom spade.template import Template\n\nclass MyAgent(Agent):\n    async def setup(self):\n        behaviour = LLMBehaviour(llm_provider=provider)\n        template = Template()\n        self.add_behaviour(behaviour, template)\n</code></pre>"},{"location":"reference/api/behaviour/#error-handling","title":"Error Handling","text":"<p>The behaviour handles various error conditions:</p> <ul> <li>Provider Errors: LLM service failures</li> <li>Tool Errors: Tool execution failures  </li> <li>Timeout Errors: Response timeouts</li> <li>Conversation Limits: Max interaction limits</li> </ul> <pre><code>try:\n    await behaviour._process_message_with_llm(msg, conv_id)\nexcept Exception as e:\n    logger.error(f\"Processing error: {e}\")\n    await behaviour._end_conversation(conv_id, ConversationState.ERROR)\n</code></pre>"},{"location":"reference/api/behaviour/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Tool Iteration Limit: Prevents infinite tool loops</li> <li>Conversation Cleanup: Removes completed conversations</li> <li>Message Deduplication: Prevents duplicate processing</li> <li>Context Management: Efficient memory usage</li> </ul>"},{"location":"reference/api/behaviour/#best-practices","title":"Best Practices","text":"<ul> <li>Let <code>LLMAgent</code> manage behaviour lifecycle</li> <li>Use appropriate termination markers</li> <li>Set reasonable interaction limits</li> <li>Handle conversation end events</li> <li>Monitor conversation states</li> </ul>"},{"location":"reference/api/context/","title":"Context API","text":"<p>API reference for conversation context management and strategies.</p>"},{"location":"reference/api/context/#contextmanager","title":"ContextManager","text":"<p>Manages conversation history and context for LLM interactions.</p>"},{"location":"reference/api/context/#constructor","title":"Constructor","text":"<pre><code>ContextManager(\n    max_tokens: int = 4096,\n    system_prompt: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>max_tokens</code> - Maximum context size in tokens</li> <li><code>system_prompt</code> - System instructions for LLM</li> </ul>"},{"location":"reference/api/context/#methods","title":"Methods","text":""},{"location":"reference/api/context/#add_message","title":"add_message()","text":"<pre><code>def add_message(self, message: Message, conversation_id: str) -&gt; None\n</code></pre> <p>Add SPADE message to conversation context.</p> <p>Example:</p> <pre><code>context = ContextManager(system_prompt=\"You are helpful\")\ncontext.add_message(spade_message, \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_message_dict","title":"add_message_dict()","text":"<pre><code>def add_message_dict(self, message_dict: ContextMessage, conversation_id: str) -&gt; None\n</code></pre> <p>Add message from dictionary format.</p> <p>Example:</p> <pre><code>user_msg = {\"role\": \"user\", \"content\": \"Hello!\"}\ncontext.add_message_dict(user_msg, \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_assistant_message","title":"add_assistant_message()","text":"<pre><code>def add_assistant_message(self, content: str, conversation_id: Optional[str] = None) -&gt; None\n</code></pre> <p>Add assistant response to context.</p> <p>Example:</p> <pre><code>context.add_assistant_message(\"Hello! How can I help?\", \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_tool_result","title":"add_tool_result()","text":"<pre><code>def add_tool_result(\n    self, \n    tool_name: str, \n    result: Any, \n    tool_call_id: str, \n    conversation_id: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Add tool execution result to context.</p> <p>Example:</p> <pre><code>context.add_tool_result(\n    tool_name=\"get_weather\",\n    result=\"22\u00b0C, sunny\",\n    tool_call_id=\"call_123\",\n    conversation_id=\"user1_session\"\n)\n</code></pre>"},{"location":"reference/api/context/#get_prompt","title":"get_prompt()","text":"<pre><code>def get_prompt(self, conversation_id: Optional[str] = None) -&gt; List[ContextMessage]\n</code></pre> <p>Get formatted prompt for LLM provider.</p> <p>Example:</p> <pre><code>prompt = context.get_prompt(\"user1_session\")\n# Returns list of messages formatted for LLM\n</code></pre>"},{"location":"reference/api/context/#get_conversation_history","title":"get_conversation_history()","text":"<pre><code>def get_conversation_history(self, conversation_id: Optional[str] = None) -&gt; List[ContextMessage]\n</code></pre> <p>Get raw conversation history.</p> <p>Example:</p> <pre><code>history = context.get_conversation_history(\"user1_session\")\nprint(f\"Conversation has {len(history)} messages\")\n</code></pre>"},{"location":"reference/api/context/#clear","title":"clear()","text":"<pre><code>def clear(self, conversation_id: Optional[str] = None) -&gt; None\n</code></pre> <p>Clear conversation messages.</p> <p>Example:</p> <pre><code># Clear specific conversation\ncontext.clear(\"user1_session\")\n\n# Clear all conversations\ncontext.clear(\"all\")\n</code></pre>"},{"location":"reference/api/context/#get_active_conversations","title":"get_active_conversations()","text":"<pre><code>def get_active_conversations(self) -&gt; List[str]\n</code></pre> <p>Get list of active conversation IDs.</p> <p>Example:</p> <pre><code>conversations = context.get_active_conversations()\nprint(f\"Active conversations: {conversations}\")\n</code></pre>"},{"location":"reference/api/context/#set_current_conversation","title":"set_current_conversation()","text":"<pre><code>def set_current_conversation(self, conversation_id: str) -&gt; bool\n</code></pre> <p>Set current conversation context.</p> <p>Example:</p> <pre><code>success = context.set_current_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#example-usage","title":"Example Usage","text":"<pre><code>from spade_llm.context import ContextManager\n\n# Create context manager\ncontext = ContextManager(\n    system_prompt=\"You are a helpful coding assistant\",\n    max_tokens=2000\n)\n\n# Add conversation messages\ncontext.add_message_dict(\n    {\"role\": \"user\", \"content\": \"Help me with Python\"}, \n    \"coding_session\"\n)\n\ncontext.add_assistant_message(\n    \"I'd be happy to help with Python!\", \n    \"coding_session\"\n)\n\n# Get formatted prompt\nprompt = context.get_prompt(\"coding_session\")\n# Use with LLM provider\n</code></pre>"},{"location":"reference/api/context/#coordinationcontextmanager","title":"CoordinationContextManager","text":"<p>Context manager tailored for coordination scenarios where a coordinator agent supervises several subagents.</p> <pre><code>CoordinationContextManager(\n    coordination_session: str,\n    subagent_ids: Set[str],\n    **kwargs\n)\n</code></pre> <p>Additional behaviour:</p> <ul> <li>Forces every message involving a registered subagent to use the shared <code>coordination_session</code> thread.</li> <li>Preserves standard handling for external participants (messages outside the organization keep their original thread).</li> <li>Provides <code>add_coordination_command(target_agent: str, command: str)</code> to record coordinator-issued instructions inside the shared context.</li> </ul> <p>Usage Example:</p> <pre><code>from spade_llm.agent.coordinator_agent import CoordinationContextManager\n\ncoordination_context = CoordinationContextManager(\n    coordination_session=\"city_ops\",\n    subagent_ids={\"traffic@xmpp.local\", \"alerts@xmpp.local\"}\n)\n\n# Attach to CoordinatorAgent via `_context_override` or constructor kwargs\n</code></pre>"},{"location":"reference/api/context/#context-management-strategies","title":"Context Management Strategies","text":""},{"location":"reference/api/context/#contextmanagement-abstract-base","title":"ContextManagement (Abstract Base)","text":"<p>Base class for all context management strategies.</p> <pre><code>from spade_llm.context.management import ContextManagement\n</code></pre>"},{"location":"reference/api/context/#apply_context_strategy","title":"apply_context_strategy()","text":"<pre><code>def apply_context_strategy(\n    self, \n    messages: List[ContextMessage], \n    system_prompt: Optional[str] = None\n) -&gt; List[ContextMessage]\n</code></pre> <p>Apply the context management strategy to messages.</p>"},{"location":"reference/api/context/#get_stats","title":"get_stats()","text":"<pre><code>def get_stats(self, total_messages: int) -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics about context management.</p>"},{"location":"reference/api/context/#nocontextmanagement","title":"NoContextManagement","text":"<p>Preserves all messages without any filtering.</p> <pre><code>from spade_llm.context import NoContextManagement\n\ncontext = NoContextManagement()\n</code></pre>"},{"location":"reference/api/context/#windowsizecontext","title":"WindowSizeContext","text":"<p>Basic sliding window context management.</p> <pre><code>from spade_llm.context import WindowSizeContext\n\ncontext = WindowSizeContext(max_messages=20)\n</code></pre> <p>Parameters: - <code>max_messages</code> (int): Maximum number of messages to keep</p>"},{"location":"reference/api/context/#smartwindowsizecontext","title":"SmartWindowSizeContext","text":"<p>Context management with message selection.</p> <pre><code>from spade_llm.context import SmartWindowSizeContext\n\ncontext = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n</code></pre> <p>Parameters: - <code>max_messages</code> (int): Maximum number of messages to keep - <code>preserve_initial</code> (int, optional): Number of initial messages to always preserve - <code>prioritize_tools</code> (bool, optional): Whether to prioritize tool result messages</p> <p>Example:</p> <pre><code># Smart context with tool prioritization\nsmart_context = SmartWindowSizeContext(\n    max_messages=25,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Get statistics\nstats = smart_context.get_stats(total_messages=50)\n# Returns: {\"strategy\": \"smart_window_size\", \"max_messages\": 25, ...}\n</code></pre>"},{"location":"reference/api/context/#message-types","title":"Message Types","text":""},{"location":"reference/api/context/#contextmessage-types","title":"ContextMessage Types","text":"<pre><code>from spade_llm.context._types import (\n    SystemMessage,\n    UserMessage, \n    AssistantMessage,\n    ToolResultMessage\n)\n</code></pre>"},{"location":"reference/api/context/#systemmessage","title":"SystemMessage","text":"<pre><code>{\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant\"\n}\n</code></pre>"},{"location":"reference/api/context/#usermessage","title":"UserMessage","text":"<pre><code>{\n    \"role\": \"user\",\n    \"content\": \"Hello, how are you?\",\n    \"name\": \"user@example.com\"  # Optional\n}\n</code></pre>"},{"location":"reference/api/context/#assistantmessage","title":"AssistantMessage","text":"<pre><code># Text response\n{\n    \"role\": \"assistant\",\n    \"content\": \"I'm doing well, thank you!\"\n}\n\n# With tool calls\n{\n    \"role\": \"assistant\", \n    \"content\": None,\n    \"tool_calls\": [\n        {\n            \"id\": \"call_123\",\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"arguments\": \"{\\\"city\\\": \\\"Madrid\\\"}\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"reference/api/context/#toolresultmessage","title":"ToolResultMessage","text":"<pre><code>{\n    \"role\": \"tool\",\n    \"content\": \"Weather in Madrid: 22\u00b0C, sunny\",\n    \"tool_call_id\": \"call_123\"\n}\n</code></pre>"},{"location":"reference/api/guardrails/","title":"Guardrails API","text":"<p>API reference for content filtering and safety controls.</p>"},{"location":"reference/api/guardrails/#base-classes","title":"Base Classes","text":""},{"location":"reference/api/guardrails/#guardrail","title":"Guardrail","text":"<p>Abstract base class for all guardrail implementations.</p> <pre><code>class Guardrail(ABC):\n    def __init__(self, name: str, enabled: bool = True, blocked_message: Optional[str] = None)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> - Unique identifier for the guardrail</li> <li><code>enabled</code> - Whether the guardrail is active (default: True)</li> <li><code>blocked_message</code> - Custom message when content is blocked</li> </ul> <p>Methods:</p> <pre><code>async def check(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult\n</code></pre> <p>Abstract method that must be implemented by all guardrails.</p> <pre><code>async def __call__(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult\n</code></pre> <p>Main execution method that handles enabled state and post-processing.</p>"},{"location":"reference/api/guardrails/#guardrailresult","title":"GuardrailResult","text":"<p>Result object returned by guardrail checks.</p> <pre><code>@dataclass\nclass GuardrailResult:\n    action: GuardrailAction\n    content: Optional[str] = None\n    reason: Optional[str] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    custom_message: Optional[str] = None\n</code></pre> <p>Fields:</p> <ul> <li><code>action</code> - Action to take (PASS, MODIFY, BLOCK, WARNING)</li> <li><code>content</code> - Modified content (if action is MODIFY)</li> <li><code>reason</code> - Explanation for the action taken</li> <li><code>metadata</code> - Additional information about the result</li> <li><code>custom_message</code> - Custom message to send when blocked</li> </ul>"},{"location":"reference/api/guardrails/#guardrailaction","title":"GuardrailAction","text":"<p>Enumeration of possible guardrail actions.</p> <pre><code>class GuardrailAction(Enum):\n    PASS = \"pass\"          # Allow without changes\n    MODIFY = \"modify\"      # Modify the content\n    BLOCK = \"block\"        # Block completely\n    WARNING = \"warning\"    # Allow with warning\n</code></pre>"},{"location":"reference/api/guardrails/#guardrail-types","title":"Guardrail Types","text":""},{"location":"reference/api/guardrails/#inputguardrail","title":"InputGuardrail","text":"<p>Base class for guardrails that process incoming messages.</p> <pre><code>class InputGuardrail(Guardrail):\n    def __init__(self, name: str, enabled: bool = True, blocked_message: Optional[str] = None)\n</code></pre> <p>Default blocked message: \"Your message was blocked by security filters.\"</p>"},{"location":"reference/api/guardrails/#outputguardrail","title":"OutputGuardrail","text":"<p>Base class for guardrails that process LLM responses.</p> <pre><code>class OutputGuardrail(Guardrail):\n    def __init__(self, name: str, enabled: bool = True, blocked_message: Optional[str] = None)\n</code></pre> <p>Default blocked message: \"I apologize, but I cannot provide that response.\"</p>"},{"location":"reference/api/guardrails/#implementations","title":"Implementations","text":""},{"location":"reference/api/guardrails/#keywordguardrail","title":"KeywordGuardrail","text":"<p>Filters content based on keyword matching.</p> <pre><code>KeywordGuardrail(\n    name: str,\n    blocked_keywords: List[str],\n    action: GuardrailAction = GuardrailAction.BLOCK,\n    replacement: str = \"[REDACTED]\",\n    case_sensitive: bool = False,\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>blocked_keywords</code> - List of keywords to filter</li> <li><code>action</code> - Action to take when keyword found (BLOCK or MODIFY)</li> <li><code>replacement</code> - Text to replace keywords with (if action is MODIFY)</li> <li><code>case_sensitive</code> - Whether matching is case sensitive</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Block harmful content\nsecurity_filter = KeywordGuardrail(\n    name=\"security\",\n    blocked_keywords=[\"hack\", \"exploit\", \"malware\"],\n    action=GuardrailAction.BLOCK\n)\n\n# Replace profanity\nprofanity_filter = KeywordGuardrail(\n    name=\"profanity\",\n    blocked_keywords=[\"damn\", \"hell\"],\n    action=GuardrailAction.MODIFY,\n    replacement=\"[CENSORED]\"\n)\n</code></pre>"},{"location":"reference/api/guardrails/#regexguardrail","title":"RegexGuardrail","text":"<p>Applies regex patterns for content detection and modification.</p> <pre><code>RegexGuardrail(\n    name: str,\n    patterns: Dict[str, Union[str, GuardrailAction]],\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>patterns</code> - Dictionary mapping regex patterns to replacement strings or actions</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import RegexGuardrail, GuardrailAction\n\n# Redact personal information\npii_filter = RegexGuardrail(\n    name=\"pii_protection\",\n    patterns={\n        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b': '[EMAIL]',\n        r'\\b\\d{3}-\\d{2}-\\d{4}\\b': '[SSN]',\n        r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b': GuardrailAction.BLOCK\n    },\n    blocked_message=\"Credit card information is not allowed.\"\n)\n</code></pre>"},{"location":"reference/api/guardrails/#llmguardrail","title":"LLMGuardrail","text":"<p>Uses a separate LLM model to validate content safety.</p> <pre><code>LLMGuardrail(\n    name: str,\n    provider: LLMProvider,\n    safety_prompt: Optional[str] = None,\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>provider</code> - LLM provider to use for safety validation</li> <li><code>safety_prompt</code> - Custom prompt for safety checking</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import LLMGuardrail\nfrom spade_llm.providers import LLMProvider\n\nsafety_provider = LLMProvider.create_openai(\n    api_key=\"your-key\",\n    model=\"gpt-3.5-turbo\"\n)\n\nai_safety = LLMGuardrail(\n    name=\"ai_safety_check\",\n    provider=safety_provider,\n    safety_prompt=\"\"\"\n    Analyze this text for harmful content.\n    Respond with JSON: {\"safe\": true/false, \"reason\": \"explanation\"}\n\n    Text: {content}\n    \"\"\"\n)\n</code></pre>"},{"location":"reference/api/guardrails/#customfunctionguardrail","title":"CustomFunctionGuardrail","text":"<p>Allows custom validation logic using user-defined functions.</p> <pre><code>CustomFunctionGuardrail(\n    name: str,\n    check_function: Callable[[str, Dict[str, Any]], GuardrailResult],\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>check_function</code> - Function that performs the validation</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import CustomFunctionGuardrail, GuardrailResult, GuardrailAction\n\ndef length_check(content: str, context: dict) -&gt; GuardrailResult:\n    if len(content) &gt; 1000:\n        return GuardrailResult(\n            action=GuardrailAction.BLOCK,\n            reason=\"Message too long\"\n        )\n    return GuardrailResult(action=GuardrailAction.PASS, content=content)\n\nlength_filter = CustomFunctionGuardrail(\n    name=\"length_limit\",\n    check_function=length_check\n)\n</code></pre>"},{"location":"reference/api/guardrails/#compositeguardrail","title":"CompositeGuardrail","text":"<p>Combines multiple guardrails into a processing pipeline.</p> <pre><code>CompositeGuardrail(\n    name: str,\n    guardrails: List[Guardrail],\n    stop_on_block: bool = True,\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>guardrails</code> - List of guardrails to apply in sequence</li> <li><code>stop_on_block</code> - Whether to stop processing on first block</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import CompositeGuardrail\n\nsecurity_pipeline = CompositeGuardrail(\n    name=\"security_pipeline\",\n    guardrails=[\n        KeywordGuardrail(\"keywords\", [\"hack\"], GuardrailAction.BLOCK),\n        RegexGuardrail(\"pii\", email_patterns),\n        LLMGuardrail(\"ai_safety\", safety_provider)\n    ],\n    stop_on_block=True\n)\n</code></pre>"},{"location":"reference/api/guardrails/#processing-functions","title":"Processing Functions","text":""},{"location":"reference/api/guardrails/#apply_input_guardrails","title":"apply_input_guardrails()","text":"<p>Processes input guardrails and returns filtered content.</p> <pre><code>async def apply_input_guardrails(\n    content: str,\n    message: Message,\n    guardrails: List[InputGuardrail],\n    on_trigger: Optional[Callable[[GuardrailResult], None]] = None,\n    send_reply: Optional[Callable[[Message], None]] = None\n) -&gt; Optional[str]\n</code></pre> <p>Parameters:</p> <ul> <li><code>content</code> - Input content to process</li> <li><code>message</code> - Original SPADE message</li> <li><code>guardrails</code> - List of input guardrails to apply</li> <li><code>on_trigger</code> - Callback for guardrail events</li> <li><code>send_reply</code> - Function to send blocking responses</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Processed content if passed guardrails</li> <li><code>None</code> - If content was blocked</li> </ul>"},{"location":"reference/api/guardrails/#apply_output_guardrails","title":"apply_output_guardrails()","text":"<p>Processes output guardrails and returns filtered content.</p> <pre><code>async def apply_output_guardrails(\n    content: str,\n    original_message: Message,\n    guardrails: List[OutputGuardrail],\n    on_trigger: Optional[Callable[[GuardrailResult], None]] = None\n) -&gt; str\n</code></pre> <p>Parameters:</p> <ul> <li><code>content</code> - LLM response content to process</li> <li><code>original_message</code> - Original input message</li> <li><code>guardrails</code> - List of output guardrails to apply</li> <li><code>on_trigger</code> - Callback for guardrail events</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Processed content (never None - blocked content returns safe message)</li> </ul>"},{"location":"reference/api/guardrails/#context-information","title":"Context Information","text":"<p>Guardrails receive context information for decision making:</p>"},{"location":"reference/api/guardrails/#input-guardrail-context","title":"Input Guardrail Context","text":"<pre><code>{\n    \"message\": Message,           # Original SPADE message\n    \"sender\": str,               # Message sender JID\n    \"conversation_id\": str       # Conversation identifier\n}\n</code></pre>"},{"location":"reference/api/guardrails/#output-guardrail-context","title":"Output Guardrail Context","text":"<pre><code>{\n    \"original_message\": Message, # Original input message\n    \"conversation_id\": str,      # Conversation identifier\n    \"llm_response\": str         # Original LLM response\n}\n</code></pre>"},{"location":"reference/api/guardrails/#agent-integration","title":"Agent Integration","text":""},{"location":"reference/api/guardrails/#adding-guardrails-to-agents","title":"Adding Guardrails to Agents","text":"<pre><code>from spade_llm import LLMAgent\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=[input_filter1, input_filter2],\n    output_guardrails=[output_filter1, output_filter2],\n    on_guardrail_trigger=callback_function\n)\n</code></pre>"},{"location":"reference/api/guardrails/#dynamic-guardrail-management","title":"Dynamic Guardrail Management","text":"<pre><code># Add guardrails at runtime\nagent.add_input_guardrail(new_filter)\nagent.add_output_guardrail(safety_check)\n\n# Control individual guardrails\nagent.input_guardrails[0].enabled = False  # Disable specific guardrail\n</code></pre>"},{"location":"reference/api/guardrails/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/guardrails/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>LLM-based guardrails are slower - use sparingly</li> <li>Keyword guardrails are fastest for simple filtering</li> <li>Regex guardrails offer good performance/flexibility balance</li> <li>Consider caching for expensive operations</li> </ul>"},{"location":"reference/api/guardrails/#security-guidelines","title":"Security Guidelines","text":"<ul> <li>Use multiple layers of protection</li> <li>Test thoroughly with diverse inputs</li> <li>Monitor performance impact</li> <li>Log all guardrail actions for auditing</li> <li>Fail safely - prefer allowing content over breaking functionality</li> </ul>"},{"location":"reference/api/guardrails/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await guardrail.check(content, context)\nexcept Exception as e:\n    # Guardrails fail safely - log error and allow content\n    logger.error(f\"Guardrail {guardrail.name} failed: {e}\")\n    result = GuardrailResult(action=GuardrailAction.PASS, content=content)\n</code></pre>"},{"location":"reference/api/human-interface/","title":"Human Interface API Reference","text":"<p>Complete API documentation for Human-in-the-Loop components.</p>"},{"location":"reference/api/human-interface/#overview","title":"Overview","text":"<p>The Human Interface API consists of three main components:</p> <ul> <li><code>HumanInTheLoopTool</code>: LLM tool for consulting human experts</li> <li><code>HumanInteractionBehaviour</code>: SPADE behaviour for handling individual consultations  </li> <li>Web Interface: Browser-based interface for human experts</li> </ul>"},{"location":"reference/api/human-interface/#humaninthelooptool","title":"HumanInTheLoopTool","text":"<p>LLM tool that enables agents to consult with human experts via XMPP messaging.</p>"},{"location":"reference/api/human-interface/#constructor","title":"Constructor","text":"<pre><code>HumanInTheLoopTool(\n    human_expert_jid: str,\n    timeout: float = 300.0,\n    name: str = \"ask_human_expert\", \n    description: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>human_expert_jid</code> (<code>str</code>): The XMPP JID of the human expert to consult</li> <li><code>timeout</code> (<code>float</code>, optional): Maximum time to wait for response in seconds. Default: 300.0</li> <li><code>name</code> (<code>str</code>, optional): Name of the tool for LLM reference. Default: \"ask_human_expert\"</li> <li><code>description</code> (<code>str</code>, optional): Description of when to use this tool. Auto-generated if not provided.</li> </ul> <p>Example: <pre><code>from spade_llm.tools import HumanInTheLoopTool\n\ntool = HumanInTheLoopTool(\n    human_expert_jid=\"expert@company.com\",\n    timeout=180.0,  # 3 minutes\n    name=\"ask_domain_expert\",\n    description=\"Consult domain expert for specialized knowledge\"\n)\n</code></pre></p>"},{"location":"reference/api/human-interface/#methods","title":"Methods","text":""},{"location":"reference/api/human-interface/#set_agentagent-agent","title":"<code>set_agent(agent: Agent)</code>","text":"<p>Binds the tool to an agent instance. Called automatically when tool is added to an agent.</p> <p>Parameters: - <code>agent</code> (<code>Agent</code>): The SPADE agent that will use this tool</p> <p>Note: This method is called internally by <code>LLMAgent._register_tool()</code> and should not be called manually.</p>"},{"location":"reference/api/human-interface/#_ask_humanquestion-str-context-optionalstr-none-str","title":"<code>_ask_human(question: str, context: Optional[str] = None) -&gt; str</code>","text":"<p>Internal method that executes the human consultation. Called by the LLM system.</p> <p>Parameters: - <code>question</code> (<code>str</code>): The question to ask the human expert - <code>context</code> (<code>str</code>, optional): Additional context to help the human understand</p> <p>Returns: - <code>str</code>: The human expert's response or an error message</p> <p>Raises: - <code>TimeoutError</code>: If human doesn't respond within the timeout period - <code>Exception</code>: For XMPP connection or other system errors</p>"},{"location":"reference/api/human-interface/#humaninteractionbehaviour","title":"HumanInteractionBehaviour","text":"<p>SPADE OneShotBehaviour that handles individual human consultations via XMPP.</p>"},{"location":"reference/api/human-interface/#constructor_1","title":"Constructor","text":"<pre><code>HumanInteractionBehaviour(\n    human_jid: str,\n    question: str,\n    context: Optional[str] = None,\n    timeout: float = 300.0\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>human_jid</code> (<code>str</code>): The XMPP JID of the human expert</li> <li><code>question</code> (<code>str</code>): The question to ask the human</li> <li><code>context</code> (<code>str</code>, optional): Additional context for the question</li> <li><code>timeout</code> (<code>float</code>, optional): Maximum wait time for response. Default: 300.0</li> </ul> <p>Example: <pre><code>from spade_llm.behaviour.human_interaction import HumanInteractionBehaviour\n\nbehaviour = HumanInteractionBehaviour(\n    human_jid=\"expert@company.com\",\n    question=\"What's our policy on remote work?\",\n    context=\"New employee asking about work arrangement options\",\n    timeout=240.0\n)\n</code></pre></p>"},{"location":"reference/api/human-interface/#attributes","title":"Attributes","text":""},{"location":"reference/api/human-interface/#query_id-str","title":"<code>query_id: str</code>","text":"<p>Unique 8-character identifier for this consultation, used for message correlation.</p>"},{"location":"reference/api/human-interface/#response-optionalstr","title":"<code>response: Optional[str]</code>","text":"<p>The human expert's response. Set after successful completion, <code>None</code> if no response received.</p>"},{"location":"reference/api/human-interface/#methods_1","title":"Methods","text":""},{"location":"reference/api/human-interface/#async-run","title":"<code>async run()</code>","text":"<p>Executes the behaviour: sends question to human and waits for response.</p> <p>Workflow: 1. Waits for agent XMPP connection 2. Formats and sends question with unique thread ID 3. Waits for response with timeout 4. Stores response in <code>self.response</code></p> <p>Note: This method is called automatically by SPADE and should not be invoked manually.</p>"},{"location":"reference/api/human-interface/#_format_question-str","title":"<code>_format_question() -&gt; str</code>","text":"<p>Formats the question for display to the human expert.</p> <p>Returns: - <code>str</code>: Formatted question with query ID, context, and response instructions</p> <p>Example output: <pre><code>[Query a1b2c3d4] What's our policy on remote work?\n\nContext: New employee asking about work arrangement options\n\n(Please reply to this message to provide your answer)\n</code></pre></p>"},{"location":"reference/api/human-interface/#web-interface","title":"Web Interface","text":"<p>The web interface consists of static files served by a simple HTTP server.</p>"},{"location":"reference/api/human-interface/#web-server","title":"Web Server","text":""},{"location":"reference/api/human-interface/#run_serverport8080-directorynone","title":"<code>run_server(port=8080, directory=None)</code>","text":"<p>Starts the HTTP server for the human expert interface.</p> <p>Parameters: - <code>port</code> (<code>int</code>, optional): Port to run server on. Default: 8080 - <code>directory</code> (<code>str</code>, optional): Directory to serve files from. Default: <code>web_client</code> folder</p> <p>Example: <pre><code>from spade_llm.human_interface.web_server import run_server\n\n# Start on default port\nrun_server()\n\n# Start on custom port\nrun_server(port=9000)\n</code></pre></p>"},{"location":"reference/api/human-interface/#starting-via-command-line","title":"Starting via Command Line","text":"<pre><code># Default configuration (port 8080)\npython -m spade_llm.human_interface.web_server\n\n# Custom port\npython -m spade_llm.human_interface.web_server 9000\n</code></pre>"},{"location":"reference/api/human-interface/#web-interface-features","title":"Web Interface Features","text":""},{"location":"reference/api/human-interface/#connection-management","title":"Connection Management","text":"<ul> <li>WebSocket connection to XMPP server</li> <li>Automatic reconnection on connection loss</li> <li>Visual connection status indicator</li> </ul>"},{"location":"reference/api/human-interface/#query-handling","title":"Query Handling","text":"<ul> <li>Real-time notifications for new queries</li> <li>Query filtering (show/hide answered)</li> <li>Response history tracking</li> <li>Thread-based correlation for proper message routing</li> </ul>"},{"location":"reference/api/human-interface/#user-interface-elements","title":"User Interface Elements","text":"<ul> <li>Connection form: XMPP credentials input</li> <li>Query list: Active and historical queries  </li> <li>Response interface: Text area and send button</li> <li>Status indicators: Connection and query states</li> </ul>"},{"location":"reference/api/human-interface/#integration-patterns","title":"Integration Patterns","text":""},{"location":"reference/api/human-interface/#agent-registration","title":"Agent Registration","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.tools import HumanInTheLoopTool\n\n# Create tool\nhuman_tool = HumanInTheLoopTool(\"expert@company.com\")\n\n# Method 1: Pass in constructor\nagent = LLMAgent(\n    jid=\"agent@company.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[human_tool]  # Automatic registration and binding\n)\n\n# Method 2: Add after creation\nagent = LLMAgent(jid=\"agent@company.com\", password=\"password\", provider=provider)\nagent.add_tool(human_tool)  # Manual registration\n</code></pre>"},{"location":"reference/api/human-interface/#error-handling-patterns","title":"Error Handling Patterns","text":"<pre><code># In tool usage\ntry:\n    response = await human_tool._ask_human(\"What's the weather?\")\n    if response.startswith(\"Timeout:\"):\n        # Handle timeout gracefully\n        return \"Human expert unavailable, using alternative approach\"\n    elif response.startswith(\"Error:\"):\n        # Handle system errors\n        return \"Consultation failed, proceeding with available information\"\n    else:\n        return f\"Expert says: {response}\"\nexcept Exception as e:\n    return f\"Unexpected error: {e}\"\n</code></pre>"},{"location":"reference/api/human-interface/#multiple-expert-configuration","title":"Multiple Expert Configuration","text":"<pre><code># Domain-specific experts\nsales_expert = HumanInTheLoopTool(\n    human_expert_jid=\"sales@company.com\",\n    name=\"ask_sales_expert\",\n    description=\"Consult sales team about pricing and customers\"\n)\n\ntech_expert = HumanInTheLoopTool(\n    human_expert_jid=\"tech@company.com\", \n    name=\"ask_tech_expert\",\n    description=\"Consult tech team about systems and architecture\"\n)\n\nagent = LLMAgent(\n    jid=\"agent@company.com\",\n    password=\"password\", \n    provider=provider,\n    tools=[sales_expert, tech_expert],\n    system_prompt=\"\"\"Choose the appropriate expert:\n    - Use ask_sales_expert for pricing, deals, customer questions\n    - Use ask_tech_expert for technical, system, architecture questions\"\"\"\n)\n</code></pre>"},{"location":"reference/api/human-interface/#message-protocol","title":"Message Protocol","text":""},{"location":"reference/api/human-interface/#xmpp-message-format","title":"XMPP Message Format","text":""},{"location":"reference/api/human-interface/#question-message-agent-human","title":"Question Message (Agent \u2192 Human)","text":"<pre><code>&lt;message to=\"expert@company.com\" type=\"chat\" thread=\"a1b2c3d4\"&gt;\n  &lt;body&gt;\n    [Query a1b2c3d4] What's our WiFi password?\n\n    Context: New employee needs network access\n\n    (Please reply to this message to provide your answer)\n  &lt;/body&gt;\n  &lt;metadata type=\"human_query\" query_id=\"a1b2c3d4\"/&gt;\n&lt;/message&gt;\n</code></pre>"},{"location":"reference/api/human-interface/#response-message-human-agent","title":"Response Message (Human \u2192 Agent)","text":"<pre><code>&lt;message to=\"agent@company.com\" type=\"chat\" thread=\"a1b2c3d4\"&gt;\n  &lt;body&gt;The WiFi password is \"CompanyWiFi2024\" - please don't share externally.&lt;/body&gt;\n&lt;/message&gt;\n</code></pre>"},{"location":"reference/api/human-interface/#message-correlation","title":"Message Correlation","text":"<p>Messages are correlated using XMPP thread IDs:</p> <ol> <li>Query ID generated: 8-character UUID segment</li> <li>Thread set: <code>msg.thread = query_id</code> </li> <li>Response inherits: Automatic thread inheritance in XMPP</li> <li>Behaviour filters: Only processes messages with matching thread</li> </ol>"},{"location":"reference/api/human-interface/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/api/human-interface/#environment-variables","title":"Environment Variables","text":"<pre><code># XMPP server configuration\nXMPP_SERVER=your-server.com\nXMPP_WEBSOCKET_PORT=7070\n\n# Expert credentials\nEXPERT_JID=expert@your-server.com  \nEXPERT_PASSWORD=secure-password\n\n# Web interface\nWEB_INTERFACE_PORT=8080\n</code></pre>"},{"location":"reference/api/human-interface/#system-prompt-examples","title":"System Prompt Examples","text":""},{"location":"reference/api/human-interface/#basic-configuration","title":"Basic Configuration","text":"<pre><code>system_prompt = \"\"\"You are an AI assistant with access to human experts.\n\nUse the ask_human_expert tool when you need:\n- Current information not in your training data\n- Human judgment or opinions\n- Company-specific information  \n- Clarification on ambiguous requests\n\nAlways explain whether information comes from human experts or your training.\"\"\"\n</code></pre>"},{"location":"reference/api/human-interface/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>system_prompt = \"\"\"You are an enterprise AI assistant with specialized expert access.\n\nExpert Consultation Rules:\n1. ask_sales_expert: pricing, deals, customer relationships, market intelligence\n2. ask_tech_expert: architecture, systems, security, technical feasibility  \n3. ask_legal_expert: compliance, contracts, regulatory questions\n4. ask_hr_expert: policies, procedures, employee-related questions\n\nOnly consult experts for:\n- Information requiring current knowledge (post-training cutoff)\n- Subjective decisions requiring human judgment\n- Company-specific policies or procedures\n- Sensitive matters requiring approval\n\nFor general knowledge questions, answer directly without consultation.\"\"\"\n</code></pre>"},{"location":"reference/api/human-interface/#security-considerations","title":"Security Considerations","text":""},{"location":"reference/api/human-interface/#access-control","title":"Access Control","text":"<ul> <li>XMPP authentication: All participants must authenticate to XMPP server</li> <li>JID validation: Tools validate expert JID format  </li> <li>Message encryption: Use XMPP TLS/SSL in production</li> <li>Web interface: No authentication by default - add auth layer for production</li> </ul>"},{"location":"reference/api/human-interface/#data-privacy","title":"Data Privacy","text":"<ul> <li>Message logging: XMPP servers may log messages - configure retention policies</li> <li>Browser storage: Web interface uses sessionStorage (cleared on close)</li> <li>Network traffic: Use encrypted WebSocket connections (<code>wss://</code>) in production</li> </ul>"},{"location":"reference/api/human-interface/#deployment-security","title":"Deployment Security","text":"<pre><code># Production configuration example\nhuman_tool = HumanInTheLoopTool(\n    human_expert_jid=os.getenv(\"EXPERT_JID\"),  # From environment\n    timeout=int(os.getenv(\"EXPERT_TIMEOUT\", \"300\")),\n    name=\"ask_expert\",\n    description=\"Consult verified human expert for sensitive information\"\n)\n\n# Agent with security enabled\nagent = LLMAgent(\n    jid=os.getenv(\"AGENT_JID\"),\n    password=os.getenv(\"AGENT_PASSWORD\"),\n    provider=provider,\n    tools=[human_tool],\n    verify_security=True  # Enable certificate verification\n)\n</code></pre>"},{"location":"reference/api/human-interface/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/api/human-interface/#timeout-strategy","title":"Timeout Strategy","text":"<pre><code># Tiered timeout approach\nurgent_tool = HumanInTheLoopTool(\"expert@company.com\", timeout=60.0)   # 1 min\nnormal_tool = HumanInTheLoopTool(\"expert@company.com\", timeout=300.0)  # 5 min  \nresearch_tool = HumanInTheLoopTool(\"expert@company.com\", timeout=1800.0) # 30 min\n</code></pre>"},{"location":"reference/api/human-interface/#connection-pooling","title":"Connection Pooling","text":"<pre><code># Reuse connections for multiple consultations\nagent = LLMAgent(...)  # Single agent instance\nawait agent.start()     # Establish XMPP connection once\n\n# Multiple consultations reuse the same agent connection\nfor question in questions:\n    response = await agent.process_message(question)\n</code></pre>"},{"location":"reference/api/human-interface/#expert-availability","title":"Expert Availability","text":"<pre><code># Fallback strategy\nsystem_prompt = \"\"\"When consulting experts:\n1. If timeout occurs, inform user and provide best-effort answer\n2. If expert unavailable, use available information sources\n3. Set user expectations about response times\n4. For urgent matters, escalate through alternative channels\"\"\"\n</code></pre>"},{"location":"reference/api/memory/","title":"Memory API","text":"<p>API reference for SPADE_LLM's dual memory system supporting both interaction memory and agent base memory.</p>"},{"location":"reference/api/memory/#agentinteractionmemory","title":"AgentInteractionMemory","text":"<p>Manages conversation-specific memory with JSON-based storage.</p>"},{"location":"reference/api/memory/#constructor","title":"Constructor","text":"<pre><code>AgentInteractionMemory(\n    agent_id: str,\n    memory_path: Optional[str] = None\n)\n</code></pre> <p>Parameters: - <code>agent_id</code> (str): Unique identifier for the agent - <code>memory_path</code> (str, optional): Custom memory storage directory path</p> <p>Example:</p> <pre><code>from spade_llm.memory import AgentInteractionMemory\n\n# Default storage location\nmemory = AgentInteractionMemory(\"agent@example.com\")\n\n# Custom storage location\nmemory = AgentInteractionMemory(\n    agent_id=\"agent@example.com\",\n    memory_path=\"/custom/memory/path\"\n)\n</code></pre>"},{"location":"reference/api/memory/#methods","title":"Methods","text":""},{"location":"reference/api/memory/#add_information","title":"add_information()","text":"<pre><code>def add_information(\n    self, \n    conversation_id: str, \n    information: str\n) -&gt; str\n</code></pre> <p>Add information to conversation memory.</p> <p>Parameters: - <code>conversation_id</code> (str): Unique conversation identifier - <code>information</code> (str): Information to store</p> <p>Returns: - <code>str</code>: Confirmation message</p> <p>Example:</p> <pre><code>memory.add_information(\n    \"user1_session\",\n    \"User prefers JSON responses over XML\"\n)\n</code></pre>"},{"location":"reference/api/memory/#get_information","title":"get_information()","text":"<pre><code>def get_information(\n    self, \n    conversation_id: str\n) -&gt; List[str]\n</code></pre> <p>Get stored information for a conversation.</p> <p>Parameters: - <code>conversation_id</code> (str): Conversation identifier</p> <p>Returns: - <code>List[str]</code>: List of stored information strings</p> <p>Example:</p> <pre><code>info_list = memory.get_information(\"user1_session\")\n# Returns: [\"User prefers JSON responses\", \"API token: abc123\"]\n</code></pre>"},{"location":"reference/api/memory/#get_context_summary","title":"get_context_summary()","text":"<pre><code>def get_context_summary(\n    self, \n    conversation_id: str\n) -&gt; Optional[str]\n</code></pre> <p>Get formatted summary of conversation memory.</p> <p>Parameters: - <code>conversation_id</code> (str): Conversation identifier</p> <p>Returns: - <code>Optional[str]</code>: Formatted summary string or None if no memory</p> <p>Example:</p> <pre><code>summary = memory.get_context_summary(\"user1_session\")\n# Returns: \"Previous interactions:\\n- User prefers JSON responses\\n- API token: abc123\"\n</code></pre>"},{"location":"reference/api/memory/#clear_conversation","title":"clear_conversation()","text":"<pre><code>def clear_conversation(\n    self, \n    conversation_id: str\n) -&gt; bool\n</code></pre> <p>Clear memory for a specific conversation.</p> <p>Parameters: - <code>conversation_id</code> (str): Conversation identifier</p> <p>Returns: - <code>bool</code>: True if successful, False otherwise</p> <p>Example:</p> <pre><code>success = memory.clear_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/memory/#get_all_conversations","title":"get_all_conversations()","text":"<pre><code>def get_all_conversations(self) -&gt; List[str]\n</code></pre> <p>Get list of all conversation IDs with stored memory.</p> <p>Returns: - <code>List[str]</code>: List of conversation IDs</p> <p>Example:</p> <pre><code>conversations = memory.get_all_conversations()\n# Returns: [\"user1_session\", \"user2_session\", \"api_session\"]\n</code></pre>"},{"location":"reference/api/memory/#agentbasememory","title":"AgentBaseMemory","text":"<p>Manages long-term agent memory with SQLite backend and categorized storage. Supports both persistent file-based and temporary in-memory storage modes.</p>"},{"location":"reference/api/memory/#constructor_1","title":"Constructor","text":"<pre><code>AgentBaseMemory(\n    agent_id: str,\n    memory_path: Optional[str] = None,\n    backend: Optional[MemoryBackend] = None\n)\n</code></pre> <p>Parameters: - <code>agent_id</code> (str): Unique identifier for the agent - <code>memory_path</code> (str, optional): Custom memory storage directory path or \":memory:\" for in-memory storage - <code>backend</code> (MemoryBackend, optional): Custom memory backend implementation</p> <p>Example:</p> <pre><code>from spade_llm.memory import AgentBaseMemory\n\n# Default SQLite backend (persistent)\nmemory = AgentBaseMemory(\"agent@example.com\")\n\n# Custom memory path (persistent)\nmemory = AgentBaseMemory(\n    agent_id=\"agent@example.com\",\n    memory_path=\"/custom/memory/path\"\n)\n\n# In-memory storage (temporary)\nmemory = AgentBaseMemory(\n    agent_id=\"agent@example.com\",\n    memory_path=\":memory:\"\n)\n</code></pre>"},{"location":"reference/api/memory/#async-methods","title":"Async Methods","text":""},{"location":"reference/api/memory/#initialize","title":"initialize()","text":"<pre><code>async def initialize(self) -&gt; None\n</code></pre> <p>Initialize the memory backend and create necessary database structures.</p> <p>Example:</p> <pre><code>await memory.initialize()\n</code></pre>"},{"location":"reference/api/memory/#store_memory","title":"store_memory()","text":"<pre><code>async def store_memory(\n    self,\n    content: str,\n    category: str,\n    context: Optional[str] = None,\n    confidence: float = 1.0\n) -&gt; str\n</code></pre> <p>Store information in long-term memory.</p> <p>Parameters: - <code>content</code> (str): Information content to store - <code>category</code> (str): Memory category (<code>fact</code>, <code>pattern</code>, <code>preference</code>, <code>capability</code>) - <code>context</code> (str, optional): Additional context information - <code>confidence</code> (float, optional): Confidence score (0.0 to 1.0), defaults to 1.0</p> <p>Returns: - <code>str</code>: Unique memory identifier</p> <p>Example:</p> <pre><code>memory_id = await memory.store_memory(\n    content=\"API endpoint requires authentication header\",\n    category=\"fact\",\n    context=\"API integration discussion\",\n    confidence=0.9\n)\n</code></pre>"},{"location":"reference/api/memory/#search_memories","title":"search_memories()","text":"<pre><code>async def search_memories(\n    self,\n    query: str,\n    limit: int = 10,\n    category: Optional[str] = None\n) -&gt; List[MemoryEntry]\n</code></pre> <p>Search through stored memories.</p> <p>Parameters: - <code>query</code> (str): Search query string - <code>limit</code> (int, optional): Maximum number of results, defaults to 10 - <code>category</code> (str, optional): Filter by memory category</p> <p>Returns: - <code>List[MemoryEntry]</code>: List of matching memory entries</p> <p>Example:</p> <pre><code>results = await memory.search_memories(\n    query=\"API authentication\",\n    limit=5,\n    category=\"fact\"\n)\n</code></pre>"},{"location":"reference/api/memory/#get_memories_by_category","title":"get_memories_by_category()","text":"<pre><code>async def get_memories_by_category(\n    self,\n    category: str,\n    limit: int = 10\n) -&gt; List[MemoryEntry]\n</code></pre> <p>Get memories by category.</p> <p>Parameters: - <code>category</code> (str): Memory category to retrieve - <code>limit</code> (int, optional): Maximum number of results, defaults to 10</p> <p>Returns: - <code>List[MemoryEntry]</code>: List of memory entries in category</p> <p>Example:</p> <pre><code>facts = await memory.get_memories_by_category(\"fact\", limit=20)\n</code></pre>"},{"location":"reference/api/memory/#get_recent_memories","title":"get_recent_memories()","text":"<pre><code>async def get_recent_memories(\n    self,\n    limit: int = 10\n) -&gt; List[MemoryEntry]\n</code></pre> <p>Get most recently accessed memories.</p> <p>Parameters: - <code>limit</code> (int, optional): Maximum number of results, defaults to 10</p> <p>Returns: - <code>List[MemoryEntry]</code>: List of recent memory entries</p> <p>Example:</p> <pre><code>recent = await memory.get_recent_memories(limit=5)\n</code></pre>"},{"location":"reference/api/memory/#update_memory","title":"update_memory()","text":"<pre><code>async def update_memory(\n    self,\n    memory_id: str,\n    content: Optional[str] = None,\n    category: Optional[str] = None,\n    context: Optional[str] = None,\n    confidence: Optional[float] = None\n) -&gt; bool\n</code></pre> <p>Update existing memory entry.</p> <p>Parameters: - <code>memory_id</code> (str): Memory identifier to update - <code>content</code> (str, optional): New content - <code>category</code> (str, optional): New category - <code>context</code> (str, optional): New context - <code>confidence</code> (float, optional): New confidence score</p> <p>Returns: - <code>bool</code>: True if successful, False otherwise</p> <p>Example:</p> <pre><code>success = await memory.update_memory(\n    memory_id=\"mem_123\",\n    content=\"Updated API endpoint information\",\n    confidence=0.95\n)\n</code></pre>"},{"location":"reference/api/memory/#delete_memory","title":"delete_memory()","text":"<pre><code>async def delete_memory(\n    self,\n    memory_id: str\n) -&gt; bool\n</code></pre> <p>Delete a memory entry.</p> <p>Parameters: - <code>memory_id</code> (str): Memory identifier to delete</p> <p>Returns: - <code>bool</code>: True if successful, False otherwise</p> <p>Example:</p> <pre><code>success = await memory.delete_memory(\"mem_123\")\n</code></pre>"},{"location":"reference/api/memory/#get_memory_stats","title":"get_memory_stats()","text":"<pre><code>async def get_memory_stats(self) -&gt; Dict[str, Any]\n</code></pre> <p>Get memory usage statistics.</p> <p>Returns: - <code>Dict[str, Any]</code>: Statistics dictionary</p> <p>Example:</p> <pre><code>stats = await memory.get_memory_stats()\n# Returns: {\n#     \"total_memories\": 150,\n#     \"categories\": {\"fact\": 50, \"pattern\": 30, \"preference\": 40, \"capability\": 30},\n#     \"avg_confidence\": 0.85,\n#     \"oldest_memory\": \"2025-01-01T00:00:00Z\",\n#     \"newest_memory\": \"2025-01-10T12:00:00Z\"\n# }\n</code></pre>"},{"location":"reference/api/memory/#cleanup","title":"cleanup()","text":"<pre><code>async def cleanup(self) -&gt; None\n</code></pre> <p>Clean up memory backend resources.</p> <p>Example:</p> <pre><code>await memory.cleanup()\n</code></pre>"},{"location":"reference/api/memory/#memoryentry","title":"MemoryEntry","text":"<p>Data structure representing a memory entry.</p>"},{"location":"reference/api/memory/#structure","title":"Structure","text":"<pre><code>@dataclass\nclass MemoryEntry:\n    id: str\n    agent_id: str\n    category: str\n    content: str\n    context: Optional[str] = None\n    confidence: float = 1.0\n    created_at: datetime\n    last_accessed: datetime\n    access_count: int = 0\n</code></pre> <p>Fields: - <code>id</code> (str): Unique memory identifier - <code>agent_id</code> (str): Agent that owns this memory - <code>category</code> (str): Memory category - <code>content</code> (str): Memory content - <code>context</code> (str, optional): Additional context - <code>confidence</code> (float): Confidence score (0.0 to 1.0) - <code>created_at</code> (datetime): Creation timestamp - <code>last_accessed</code> (datetime): Last access timestamp - <code>access_count</code> (int): Number of times accessed</p>"},{"location":"reference/api/memory/#example","title":"Example","text":"<pre><code>memory_entry = MemoryEntry(\n    id=\"mem_123\",\n    agent_id=\"agent@example.com\",\n    category=\"fact\",\n    content=\"API requires authentication header\",\n    context=\"API integration discussion\",\n    confidence=0.9,\n    created_at=datetime.now(),\n    last_accessed=datetime.now(),\n    access_count=3\n)\n</code></pre>"},{"location":"reference/api/memory/#memory-tools","title":"Memory Tools","text":""},{"location":"reference/api/memory/#interaction-memory-tools","title":"Interaction Memory Tools","text":""},{"location":"reference/api/memory/#remember_interaction_info","title":"remember_interaction_info","text":"<p>Tool function for storing conversation-specific information.</p> <pre><code>def remember_interaction_info(information: str) -&gt; str\n</code></pre> <p>Parameters: - <code>information</code> (str): Information to store in memory</p> <p>Returns: - <code>str</code>: Confirmation message</p> <p>LLM Tool Schema:</p> <pre><code>{\n    \"name\": \"remember_interaction_info\",\n    \"description\": \"Store important information for future reference in this conversation\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"information\": {\n                \"type\": \"string\",\n                \"description\": \"Important information to remember for this conversation\"\n            }\n        },\n        \"required\": [\"information\"]\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#agent-base-memory-tools","title":"Agent Base Memory Tools","text":""},{"location":"reference/api/memory/#store_memory_1","title":"store_memory","text":"<p>Tool function for storing information in long-term memory.</p> <pre><code>def store_memory(\n    content: str,\n    category: str,\n    context: Optional[str] = None\n) -&gt; str\n</code></pre> <p>Parameters: - <code>content</code> (str): Information content to store - <code>category</code> (str): Memory category (<code>fact</code>, <code>pattern</code>, <code>preference</code>, <code>capability</code>) - <code>context</code> (str, optional): Additional context information</p> <p>Returns: - <code>str</code>: Confirmation message with memory ID</p> <p>LLM Tool Schema:</p> <pre><code>{\n    \"name\": \"store_memory\",\n    \"description\": \"Store information in long-term memory with category classification\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"content\": {\n                \"type\": \"string\",\n                \"description\": \"Information content to store\"\n            },\n            \"category\": {\n                \"type\": \"string\",\n                \"enum\": [\"fact\", \"pattern\", \"preference\", \"capability\"],\n                \"description\": \"Memory category for organization\"\n            },\n            \"context\": {\n                \"type\": \"string\",\n                \"description\": \"Additional context information (optional)\"\n            }\n        },\n        \"required\": [\"content\", \"category\"]\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#search_memories_1","title":"search_memories","text":"<p>Tool function for searching through stored memories.</p> <pre><code>def search_memories(\n    query: str,\n    limit: Optional[int] = 10,\n    category: Optional[str] = None\n) -&gt; str\n</code></pre> <p>Parameters: - <code>query</code> (str): Search query string - <code>limit</code> (int, optional): Maximum number of results, defaults to 10 - <code>category</code> (str, optional): Filter by memory category</p> <p>Returns: - <code>str</code>: Formatted search results</p> <p>LLM Tool Schema:</p> <pre><code>{\n    \"name\": \"search_memories\",\n    \"description\": \"Search through stored memories for relevant information\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search query string\"\n            },\n            \"limit\": {\n                \"type\": \"integer\",\n                \"description\": \"Maximum number of results to return (default: 10)\"\n            },\n            \"category\": {\n                \"type\": \"string\",\n                \"enum\": [\"fact\", \"pattern\", \"preference\", \"capability\"],\n                \"description\": \"Filter by memory category (optional)\"\n            }\n        },\n        \"required\": [\"query\"]\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#list_memories","title":"list_memories","text":"<p>Tool function for listing memories by category.</p> <pre><code>def list_memories(\n    category: Optional[str] = None,\n    limit: Optional[int] = 10\n) -&gt; str\n</code></pre> <p>Parameters: - <code>category</code> (str, optional): Memory category to list, if None lists recent memories - <code>limit</code> (int, optional): Maximum number of results, defaults to 10</p> <p>Returns: - <code>str</code>: Formatted list of memories</p> <p>LLM Tool Schema:</p> <pre><code>{\n    \"name\": \"list_memories\",\n    \"description\": \"List memories by category or view recent memories\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"category\": {\n                \"type\": \"string\",\n                \"enum\": [\"fact\", \"pattern\", \"preference\", \"capability\"],\n                \"description\": \"Memory category to list (optional, lists recent if not specified)\"\n            },\n            \"limit\": {\n                \"type\": \"integer\",\n                \"description\": \"Maximum number of results to return (default: 10)\"\n            }\n        },\n        \"required\": []\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#memory-backends","title":"Memory Backends","text":""},{"location":"reference/api/memory/#memorybackend-abstract-base-class","title":"MemoryBackend (Abstract Base Class)","text":"<p>Abstract interface for memory storage backends.</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import List, Optional, Dict, Any\n\nclass MemoryBackend(ABC):\n    \"\"\"Abstract base class for memory storage backends\"\"\"\n\n    @abstractmethod\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the backend\"\"\"\n        pass\n\n    @abstractmethod\n    async def store_memory(self, entry: MemoryEntry) -&gt; str:\n        \"\"\"Store a memory entry\"\"\"\n        pass\n\n    @abstractmethod\n    async def search_memories(\n        self,\n        agent_id: str,\n        query: str,\n        limit: int = 10,\n        category: Optional[str] = None\n    ) -&gt; List[MemoryEntry]:\n        \"\"\"Search memories\"\"\"\n        pass\n\n    @abstractmethod\n    async def get_memories_by_category(\n        self,\n        agent_id: str,\n        category: str,\n        limit: int = 10\n    ) -&gt; List[MemoryEntry]:\n        \"\"\"Get memories by category\"\"\"\n        pass\n\n    @abstractmethod\n    async def get_recent_memories(\n        self,\n        agent_id: str,\n        limit: int = 10\n    ) -&gt; List[MemoryEntry]:\n        \"\"\"Get recent memories\"\"\"\n        pass\n\n    @abstractmethod\n    async def update_memory(\n        self,\n        memory_id: str,\n        **updates\n    ) -&gt; bool:\n        \"\"\"Update a memory entry\"\"\"\n        pass\n\n    @abstractmethod\n    async def delete_memory(self, memory_id: str) -&gt; bool:\n        \"\"\"Delete a memory entry\"\"\"\n        pass\n\n    @abstractmethod\n    async def get_memory_stats(self, agent_id: str) -&gt; Dict[str, Any]:\n        \"\"\"Get memory statistics\"\"\"\n        pass\n\n    @abstractmethod\n    async def cleanup(self) -&gt; None:\n        \"\"\"Clean up backend resources\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/memory/#sqlitebackend","title":"SqliteBackend","text":"<p>SQLite implementation of the memory backend.</p> <pre><code>from spade_llm.memory.backends.sqlite_backend import SqliteBackend\n\nbackend = SqliteBackend(database_path=\"/path/to/memory.db\")\nawait backend.initialize()\n</code></pre> <p>Features: - Async SQLite operations using <code>aiosqlite</code> - Full-text search capabilities - Optimized indexing for fast queries - ACID compliance for data integrity - Concurrent access support</p>"},{"location":"reference/api/memory/#storage-formats","title":"Storage Formats","text":""},{"location":"reference/api/memory/#interaction-memory-storage","title":"Interaction Memory Storage","text":"<p>File Path: <code>{memory_path}/{safe_agent_id}_interactions.json</code></p> <p>JSON Structure: <pre><code>{\n    \"agent_id\": \"agent@example.com\",\n    \"interactions\": {\n        \"conversation_id\": [\n            {\n                \"content\": \"User prefers JSON responses\",\n                \"timestamp\": \"2025-01-10T10:30:00.000Z\"\n            },\n            {\n                \"content\": \"API token: db_token_123\",\n                \"timestamp\": \"2025-01-10T10:35:00.000Z\"\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"reference/api/memory/#agent-base-memory-storage","title":"Agent Base Memory Storage","text":"<p>File Path: <code>{memory_path}/{safe_agent_id}_base_memory.db</code></p> <p>SQLite Schema: <pre><code>CREATE TABLE agent_memories (\n    id TEXT PRIMARY KEY,\n    agent_id TEXT NOT NULL,\n    category TEXT NOT NULL,\n    content TEXT NOT NULL,\n    context TEXT,\n    confidence REAL DEFAULT 1.0,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    access_count INTEGER DEFAULT 0\n);\n\nCREATE INDEX idx_agent_category ON agent_memories(agent_id, category);\nCREATE INDEX idx_content_search ON agent_memories(content);\nCREATE INDEX idx_created_at ON agent_memories(created_at);\n</code></pre></p>"},{"location":"reference/api/memory/#llmagent-integration","title":"LLMAgent Integration","text":""},{"location":"reference/api/memory/#configuration-parameters","title":"Configuration Parameters","text":"<pre><code>from spade_llm.agent import LLMAgent\n\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n\n    # Memory configuration\n    interaction_memory: Union[bool, Tuple[bool, str]] = False,\n    agent_base_memory: Union[bool, Tuple[bool, str]] = False,\n\n    # Memory path (alternative to tuple syntax)\n    memory_path: Optional[str] = None,\n\n    # System prompt with memory instructions\n    system_prompt: str = \"You have memory capabilities...\"\n)\n</code></pre>"},{"location":"reference/api/memory/#memory-path-configuration","title":"Memory Path Configuration","text":"<pre><code># Boolean flags (uses default or environment path)\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True,\n    agent_base_memory=True\n)\n\n# Tuple syntax with custom paths\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=(True, \"/custom/interaction/path\"),\n    agent_base_memory=(True, \"/custom/base/path\")\n)\n\n# Environment variable\nimport os\nos.environ['SPADE_LLM_MEMORY_PATH'] = \"/custom/memory/path\"\n</code></pre>"},{"location":"reference/api/memory/#tool-auto-registration","title":"Tool Auto-Registration","text":"<p>When memory is enabled, tools are automatically registered:</p> <pre><code># Interaction memory tools\nif interaction_memory:\n    agent.register_tool(remember_interaction_info)\n\n# Agent base memory tools\nif agent_base_memory:\n    agent.register_tool(store_memory)\n    agent.register_tool(search_memories)\n    agent.register_tool(list_memories)\n</code></pre>"},{"location":"reference/api/memory/#example-usage","title":"Example Usage","text":""},{"location":"reference/api/memory/#complete-integration-example","title":"Complete Integration Example","text":"<pre><code>import asyncio\nfrom spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\nfrom spade_llm.memory import AgentBaseMemory, AgentInteractionMemory\n\nasync def memory_integration_example():\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Create agent with both memory types\n    agent = LLMAgent(\n        jid=\"memory_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,\n        agent_base_memory=True,\n        system_prompt=\"\"\"You are an assistant with dual memory capabilities.\n\n        Use remember_interaction_info for conversation-specific information.\n        Use store_memory, search_memories, and list_memories for long-term learning.\n        \"\"\"\n    )\n\n    # Access memory instances directly\n    interaction_memory = agent.interaction_memory\n    base_memory = agent.agent_base_memory\n\n    # Direct memory operations\n    if interaction_memory:\n        interaction_memory.add_information(\"conv_1\", \"User prefers JSON\")\n        summary = interaction_memory.get_context_summary(\"conv_1\")\n        print(f\"Interaction memory: {summary}\")\n\n    if base_memory:\n        await base_memory.initialize()\n\n        # Store a fact\n        memory_id = await base_memory.store_memory(\n            content=\"API requires authentication\",\n            category=\"fact\",\n            context=\"API integration\"\n        )\n\n        # Search memories\n        results = await base_memory.search_memories(\"API\")\n        print(f\"Found {len(results)} relevant memories\")\n\n        # Get statistics\n        stats = await base_memory.get_memory_stats()\n        print(f\"Memory statistics: {stats}\")\n\n        await base_memory.cleanup()\n\n    await agent.start()\n    print(\"Agent started with memory capabilities\")\n\n    # Agent will automatically use memory during conversations\n    await asyncio.sleep(10)\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(memory_integration_example())\n</code></pre>"},{"location":"reference/api/memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/api/memory/#memory-usage","title":"Memory Usage","text":"<ul> <li>Interaction Memory: Minimal memory overhead, JSON-based storage</li> <li>Agent Base Memory: SQLite database, efficient indexing, query optimization</li> </ul>"},{"location":"reference/api/memory/#storage-limits","title":"Storage Limits","text":"<ul> <li>File System: Consider disk space for database growth</li> <li>Concurrent Access: SQLite handles multiple connections efficiently</li> <li>Query Performance: Indexed searches scale well with database size</li> </ul>"},{"location":"reference/api/memory/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use appropriate memory types for different use cases</li> <li>Limit search results to avoid performance issues</li> <li>Regular cleanup of old or unused memories</li> <li>Monitor database size and implement archiving strategies</li> <li>Use specific search queries for better performance</li> </ol>"},{"location":"reference/api/memory/#error-handling","title":"Error Handling","text":""},{"location":"reference/api/memory/#common-exceptions","title":"Common Exceptions","text":"<pre><code>from spade_llm.memory.exceptions import (\n    MemoryInitializationError,\n    MemoryStorageError,\n    MemorySearchError,\n    MemoryBackendError\n)\n\ntry:\n    await memory.initialize()\n    await memory.store_memory(\"content\", \"fact\")\n    results = await memory.search_memories(\"query\")\nexcept MemoryInitializationError as e:\n    print(f\"Failed to initialize memory: {e}\")\nexcept MemoryStorageError as e:\n    print(f\"Failed to store memory: {e}\")\nexcept MemorySearchError as e:\n    print(f\"Search failed: {e}\")\nexcept MemoryBackendError as e:\n    print(f\"Backend error: {e}\")\n</code></pre>"},{"location":"reference/api/providers/","title":"Providers API","text":"<p>API reference for LLM provider classes.</p>"},{"location":"reference/api/providers/#llmprovider","title":"LLMProvider","text":"<p>Unified interface for different LLM services.</p>"},{"location":"reference/api/providers/#class-methods","title":"Class Methods","text":""},{"location":"reference/api/providers/#create_openai","title":"create_openai()","text":"<pre><code>LLMProvider.create_openai(\n    api_key: str,\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.7,\n    timeout: Optional[float] = None,\n    max_tokens: Optional[int] = None\n) -&gt; LLMProvider\n</code></pre> <p>Create OpenAI provider.</p> <p>Parameters:</p> <ul> <li><code>api_key</code> - OpenAI API key</li> <li><code>model</code> - Model name (e.g., \"gpt-4o\", \"gpt-4o-mini\")</li> <li><code>temperature</code> - Sampling temperature (0.0-1.0)</li> <li><code>timeout</code> - Request timeout in seconds</li> <li><code>max_tokens</code> - Maximum tokens to generate</li> </ul> <p>Example:</p> <pre><code>provider = LLMProvider.create_openai(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7\n)\n</code></pre>"},{"location":"reference/api/providers/#create_ollama","title":"create_ollama()","text":"<pre><code>LLMProvider.create_ollama(\n    model: str = \"llama3.1:8b\",\n    base_url: str = \"http://localhost:11434/v1\",\n    temperature: float = 0.7,\n    timeout: float = 120.0\n) -&gt; LLMProvider\n</code></pre> <p>Create Ollama provider.</p> <p>Parameters:</p> <ul> <li><code>model</code> - Model name (e.g., \"llama3.1:8b\", \"mistral:7b\")</li> <li><code>base_url</code> - Ollama server URL</li> <li><code>temperature</code> - Sampling temperature</li> <li><code>timeout</code> - Request timeout (longer for local models)</li> </ul> <p>Example:</p> <pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    temperature=0.8,\n    timeout=180.0\n)\n</code></pre>"},{"location":"reference/api/providers/#create_lm_studio","title":"create_lm_studio()","text":"<pre><code>LLMProvider.create_lm_studio(\n    model: str = \"local-model\",\n    base_url: str = \"http://localhost:1234/v1\",\n    temperature: float = 0.7\n) -&gt; LLMProvider\n</code></pre> <p>Create LM Studio provider.</p> <p>Example:</p> <pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"Meta-Llama-3.1-8B-Instruct\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre>"},{"location":"reference/api/providers/#create_vllm","title":"create_vllm()","text":"<pre><code>LLMProvider.create_vllm(\n    model: str,\n    base_url: str = \"http://localhost:8000/v1\"\n) -&gt; LLMProvider\n</code></pre> <p>Create vLLM provider.</p> <p>Example:</p> <pre><code>provider = LLMProvider.create_vllm(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre>"},{"location":"reference/api/providers/#instance-methods","title":"Instance Methods","text":""},{"location":"reference/api/providers/#get_llm_response","title":"get_llm_response()","text":"<pre><code>async def get_llm_response(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Get complete response from LLM.</p> <p>Returns:</p> <pre><code>{\n    'text': Optional[str],      # Text response\n    'tool_calls': List[Dict]    # Tool calls requested\n}\n</code></pre> <p>Example:</p> <pre><code>response = await provider.get_llm_response(context, tools)\n\nif response['tool_calls']:\n    # Handle tool calls\n    for call in response['tool_calls']:\n        print(f\"Tool: {call['name']}, Args: {call['arguments']}\")\nelse:\n    # Handle text response\n    print(f\"Response: {response['text']}\")\n</code></pre>"},{"location":"reference/api/providers/#get_response-legacy","title":"get_response() (Legacy)","text":"<pre><code>async def get_response(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; Optional[str]\n</code></pre> <p>Get text response only.</p> <p>Example:</p> <pre><code>text_response = await provider.get_response(context)\n</code></pre>"},{"location":"reference/api/providers/#get_tool_calls-legacy","title":"get_tool_calls() (Legacy)","text":"<pre><code>async def get_tool_calls(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get tool calls only.</p>"},{"location":"reference/api/providers/#baseprovider","title":"BaseProvider","text":"<p>Abstract base class for custom providers.</p> <pre><code>from spade_llm.providers.base_provider import LLMProvider as BaseProvider\n\nclass CustomProvider(BaseProvider):\n    async def get_llm_response(self, context, tools=None):\n        \"\"\"Implement custom LLM integration.\"\"\"\n        # Your implementation\n        return {\n            'text': \"Response from custom provider\",\n            'tool_calls': []\n        }\n</code></pre>"},{"location":"reference/api/providers/#provider-configuration","title":"Provider Configuration","text":""},{"location":"reference/api/providers/#model-formats","title":"Model Formats","text":"<pre><code>class ModelFormat(Enum):\n    OPENAI = \"openai\"    # gpt-4, gpt-3.5-turbo\n    OLLAMA = \"ollama\"    # llama3.1:8b, mistral:7b  \n    CUSTOM = \"custom\"    # custom/model-name\n</code></pre>"},{"location":"reference/api/providers/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nOPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4o-mini\n\n# Ollama  \nOLLAMA_BASE_URL=http://localhost:11434/v1\nOLLAMA_MODEL=llama3.1:8b\n\n# LM Studio\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\nLM_STUDIO_MODEL=local-model\n</code></pre>"},{"location":"reference/api/providers/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>import os\n\ndef create_provider_from_env():\n    provider_type = os.getenv('LLM_PROVIDER', 'openai')\n\n    if provider_type == 'openai':\n        return LLMProvider.create_openai(\n            api_key=os.getenv('OPENAI_API_KEY'),\n            model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n        )\n    elif provider_type == 'ollama':\n        return LLMProvider.create_ollama(\n            model=os.getenv('OLLAMA_MODEL', 'llama3.1:8b')\n        )\n\nprovider = create_provider_from_env()\n</code></pre>"},{"location":"reference/api/providers/#tool-support","title":"Tool Support","text":""},{"location":"reference/api/providers/#openai-tools","title":"OpenAI Tools","text":"<p>Native tool calling support:</p> <pre><code># Tools automatically formatted for OpenAI\nresponse = await provider.get_llm_response(context, tools)\n</code></pre>"},{"location":"reference/api/providers/#ollama-tools","title":"Ollama Tools","text":"<p>Limited to compatible models:</p> <pre><code># Check model compatibility\ntool_compatible_models = [\n    \"llama3.1:8b\", \"llama3.1:70b\", \"mistral:7b\"\n]\n\nif model in tool_compatible_models:\n    # Use tools\n    response = await provider.get_llm_response(context, tools)\n</code></pre>"},{"location":"reference/api/providers/#error-handling","title":"Error Handling","text":"<pre><code>from openai import OpenAIError\n\ntry:\n    response = await provider.get_llm_response(context)\nexcept OpenAIError as e:\n    print(f\"OpenAI API error: {e}\")\nexcept ConnectionError as e:\n    print(f\"Connection error: {e}\")\nexcept TimeoutError as e:\n    print(f\"Request timeout: {e}\")\n</code></pre>"},{"location":"reference/api/providers/#provider-comparison","title":"Provider Comparison","text":"Feature OpenAI Ollama LM Studio vLLM Setup Easy Medium Easy Hard Quality Excellent Good Good Good Speed Fast Slow Slow Fast Cost Paid Free Free Free Privacy Low High High High Tools Full Limited Limited Limited"},{"location":"reference/api/providers/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/providers/#provider-selection","title":"Provider Selection","text":"<pre><code>def choose_provider(use_case: str):\n    \"\"\"Choose provider based on use case.\"\"\"\n    if use_case == \"development\":\n        return LLMProvider.create_ollama(model=\"llama3.1:1b\")  # Fast\n    elif use_case == \"production\":\n        return LLMProvider.create_openai(model=\"gpt-4o-mini\")   # Reliable\n    elif use_case == \"privacy\":\n        return LLMProvider.create_ollama(model=\"llama3.1:8b\")  # Local\n</code></pre>"},{"location":"reference/api/providers/#error-recovery","title":"Error Recovery","text":"<pre><code>async def robust_llm_call(providers: List[LLMProvider], context):\n    \"\"\"Try multiple providers with fallback.\"\"\"\n    for provider in providers:\n        try:\n            return await provider.get_llm_response(context)\n        except Exception as e:\n            print(f\"Provider failed: {e}\")\n            continue\n\n    raise Exception(\"All providers failed\")\n</code></pre>"},{"location":"reference/api/providers/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\nasync def timed_call(provider, context):\n    \"\"\"Monitor provider performance.\"\"\"\n    start = time.time()\n    try:\n        response = await provider.get_llm_response(context)\n        duration = time.time() - start\n        print(f\"Provider response time: {duration:.2f}s\")\n        return response\n    except Exception as e:\n        duration = time.time() - start\n        print(f\"Provider failed after {duration:.2f}s: {e}\")\n        raise\n</code></pre>"},{"location":"reference/api/routing/","title":"Routing API","text":"<p>API reference for message routing system.</p>"},{"location":"reference/api/routing/#routingfunction","title":"RoutingFunction","text":"<p>Type definition for routing functions.</p> <pre><code>RoutingFunction = Callable[[Message, str, Dict[str, Any]], Union[str, RoutingResponse]]\n</code></pre> <p>Parameters:</p> <ul> <li><code>msg</code> - Original SPADE message</li> <li><code>response</code> - LLM response text</li> <li><code>context</code> - Conversation context and metadata</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Single recipient JID</li> <li><code>List[str]</code> - Multiple recipient JIDs  </li> <li><code>RoutingResponse</code> - Advanced routing</li> <li><code>None</code> - Send to original sender</li> </ul>"},{"location":"reference/api/routing/#basic-routing-function","title":"Basic Routing Function","text":"<pre><code>def simple_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"technical\" in response.lower():\n        return \"tech-support@example.com\"\n    elif \"billing\" in response.lower():\n        return \"billing@example.com\"\n    else:\n        return str(msg.sender)  # Reply to sender\n</code></pre>"},{"location":"reference/api/routing/#context-information","title":"Context Information","text":"<pre><code>context = {\n    \"conversation_id\": \"user1_agent1\",\n    \"state\": {\n        \"state\": \"active\",\n        \"interaction_count\": 5,\n        \"start_time\": 1642678800.0,\n        \"last_activity\": 1642678900.0\n    }\n}\n</code></pre>"},{"location":"reference/api/routing/#routingresponse","title":"RoutingResponse","text":"<p>Advanced routing with transformations and metadata.</p>"},{"location":"reference/api/routing/#constructor","title":"Constructor","text":"<pre><code>@dataclass\nclass RoutingResponse:\n    recipients: Union[str, List[str]]\n    transform: Optional[Callable[[str], str]] = None\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre> <p>Parameters:</p> <ul> <li><code>recipients</code> - Destination JID(s)</li> <li><code>transform</code> - Function to modify response before sending</li> <li><code>metadata</code> - Additional message metadata</li> </ul>"},{"location":"reference/api/routing/#example","title":"Example","text":"<pre><code>from spade_llm.routing import RoutingResponse\n\ndef advanced_router(msg, response, context):\n    \"\"\"Advanced routing with transformation.\"\"\"\n\n    def add_signature(text):\n        return f\"{text}\\n\\n--\\nProcessed by AI Assistant\"\n\n    if \"urgent\" in response.lower():\n        return RoutingResponse(\n            recipients=\"emergency@example.com\",\n            transform=add_signature,\n            metadata={\n                \"priority\": \"high\",\n                \"category\": \"urgent\",\n                \"original_sender\": str(msg.sender)\n            }\n        )\n\n    return str(msg.sender)\n</code></pre>"},{"location":"reference/api/routing/#routing-patterns","title":"Routing Patterns","text":""},{"location":"reference/api/routing/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>def content_router(msg, response, context):\n    \"\"\"Route based on response keywords.\"\"\"\n    response_lower = response.lower()\n\n    routing_map = {\n        \"tech-support@example.com\": [\"error\", \"bug\", \"technical\", \"debug\"],\n        \"sales@example.com\": [\"price\", \"cost\", \"purchase\", \"buy\"],\n        \"billing@example.com\": [\"payment\", \"invoice\", \"billing\"],\n        \"urgent@example.com\": [\"urgent\", \"emergency\", \"critical\"]\n    }\n\n    for recipient, keywords in routing_map.items():\n        if any(keyword in response_lower for keyword in keywords):\n            return recipient\n\n    return \"general@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#sender-based-routing","title":"Sender-Based Routing","text":"<pre><code>def sender_router(msg, response, context):\n    \"\"\"Route based on message sender.\"\"\"\n    sender = str(msg.sender)\n    sender_domain = sender.split('@')[1]\n\n    # Internal vs external routing\n    if sender_domain == \"company.com\":\n        return \"internal-support@example.com\"\n    else:\n        return \"external-support@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#context-aware-routing","title":"Context-Aware Routing","text":"<pre><code>def context_router(msg, response, context):\n    \"\"\"Route based on conversation context.\"\"\"\n    state = context.get(\"state\", {})\n    interaction_count = state.get(\"interaction_count\", 0)\n\n    # Long conversations need escalation\n    if interaction_count &gt; 10:\n        return RoutingResponse(\n            recipients=\"escalation@example.com\",\n            metadata={\n                \"reason\": \"long_conversation\",\n                \"interaction_count\": interaction_count\n            }\n        )\n\n    return \"standard@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#multi-recipient-routing","title":"Multi-Recipient Routing","text":"<pre><code>def broadcast_router(msg, response, context):\n    \"\"\"Route to multiple recipients.\"\"\"\n    recipients = [\"primary@example.com\"]\n\n    # Add recipients based on content\n    if \"error\" in response.lower():\n        recipients.append(\"monitoring@example.com\")\n\n    if \"sales\" in response.lower():\n        recipients.append(\"sales-team@example.com\")\n\n    return RoutingResponse(\n        recipients=recipients,\n        metadata={\n            \"broadcast\": True,\n            \"primary\": \"primary@example.com\"\n        }\n    )\n</code></pre>"},{"location":"reference/api/routing/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/routing/#routing-design","title":"Routing Design","text":"<ul> <li>Keep logic simple - Complex routing is hard to debug</li> <li>Use meaningful destinations - Clear JID naming</li> <li>Handle edge cases - Provide fallback routing</li> <li>Document routing rules - Clear rule descriptions</li> <li>Test thoroughly - Test all routing paths</li> </ul>"},{"location":"reference/api/tools/","title":"Tools API","text":"<p>API reference for the SPADE_LLM tools system.</p>"},{"location":"reference/api/tools/#llmtool","title":"LLMTool","text":"<p>Core tool class for defining executable functions.</p>"},{"location":"reference/api/tools/#constructor","title":"Constructor","text":"<pre><code>LLMTool(\n    name: str,\n    description: str,\n    parameters: Dict[str, Any],\n    func: Callable[..., Any]\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> - Unique tool identifier</li> <li><code>description</code> - Tool description for LLM understanding</li> <li><code>parameters</code> - JSON Schema parameter definition</li> <li><code>func</code> - Python function to execute</li> </ul>"},{"location":"reference/api/tools/#methods","title":"Methods","text":""},{"location":"reference/api/tools/#execute","title":"execute()","text":"<pre><code>async def execute(self, **kwargs) -&gt; Any\n</code></pre> <p>Execute tool with provided arguments.</p> <p>Example:</p> <pre><code>result = await tool.execute(city=\"Madrid\", units=\"celsius\")\n</code></pre>"},{"location":"reference/api/tools/#to_dict","title":"to_dict()","text":"<pre><code>def to_dict(self) -&gt; Dict[str, Any]\n</code></pre> <p>Convert tool to dictionary representation.</p>"},{"location":"reference/api/tools/#to_openai_tool","title":"to_openai_tool()","text":"<pre><code>def to_openai_tool(self) -&gt; Dict[str, Any]\n</code></pre> <p>Convert to OpenAI tool format.</p>"},{"location":"reference/api/tools/#example","title":"Example","text":"<pre><code>from spade_llm import LLMTool\n\nasync def get_weather(city: str, units: str = \"celsius\") -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Weather in {city}: 22\u00b0C, sunny\"\n\nweather_tool = LLMTool(\n    name=\"get_weather\",\n    description=\"Get current weather information for a city\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the city\"\n            },\n            \"units\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"default\": \"celsius\"\n            }\n        },\n        \"required\": [\"city\"]\n    },\n    func=get_weather\n)\n</code></pre>"},{"location":"reference/api/tools/#parameter-schema","title":"Parameter Schema","text":"<p>Tools use JSON Schema for parameter validation:</p>"},{"location":"reference/api/tools/#basic-types","title":"Basic Types","text":"<pre><code># String parameter\n\"city\": {\n    \"type\": \"string\",\n    \"description\": \"City name\"\n}\n\n# Number parameter  \n\"temperature\": {\n    \"type\": \"number\",\n    \"minimum\": -100,\n    \"maximum\": 100\n}\n\n# Boolean parameter\n\"include_forecast\": {\n    \"type\": \"boolean\",\n    \"default\": False\n}\n\n# Array parameter\n\"cities\": {\n    \"type\": \"array\",\n    \"items\": {\"type\": \"string\"},\n    \"maxItems\": 10\n}\n</code></pre>"},{"location":"reference/api/tools/#complex-schema","title":"Complex Schema","text":"<pre><code>parameters = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"Search query\"\n        },\n        \"filters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"date_range\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"start\": {\"type\": \"string\", \"format\": \"date\"},\n                        \"end\": {\"type\": \"string\", \"format\": \"date\"}\n                    }\n                },\n                \"category\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"news\", \"blogs\", \"academic\"]\n                }\n            }\n        },\n        \"max_results\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 100,\n            \"default\": 10\n        }\n    },\n    \"required\": [\"query\"]\n}\n</code></pre>"},{"location":"reference/api/tools/#langchain-integration","title":"LangChain Integration","text":""},{"location":"reference/api/tools/#langchaintooladapter","title":"LangChainToolAdapter","text":"<pre><code>from spade_llm.tools import LangChainToolAdapter\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\n# Create LangChain tool\nsearch_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM\nsearch_tool = LangChainToolAdapter(search_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"reference/api/tools/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/tools/#tool-design","title":"Tool Design","text":"<ul> <li>Single Purpose: Each tool should do one thing well</li> <li>Clear Names: Use descriptive tool names</li> <li>Good Descriptions: Help LLM understand when to use tools</li> <li>Validate Inputs: Always validate and sanitize parameters</li> <li>Handle Errors: Return meaningful error messages</li> <li>Use Async: Enable concurrent execution</li> </ul>"}]}